<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Group Relative Policy Optimization (GRPO) Illustrated Breakdown &amp; Explanation | Ebrahim Pichka</title> <meta name="author" content="Ebrahim Pichka"> <meta name="description" content="A brief and simplified intro to GRPO, an efficient policy optimization method used for LLM reasoning training"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon_32.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://epichka.com/blog/2025/grpo/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Group Relative Policy Optimization (GRPO) Illustrated Breakdown & Explanation",
      "description": "A brief and simplified intro to GRPO, an efficient policy optimization method used for LLM reasoning training",
      "published": "January 29, 2025",
      "authors": [
        {
          "author": "Ebrahim Pichka",
          "authorURL": "",
          "affiliations": [
            {
              "name": "-",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Ebrahim Pichka</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Code</a> </li> <li class="nav-item "> <a class="nav-link" href="/resources/">Resources</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Group Relative Policy Optimization (GRPO) Illustrated Breakdown &amp; Explanation</h1> <p>A brief and simplified intro to GRPO, an efficient policy optimization method used for LLM reasoning training</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#ppo-vs-grpo">PPO vs GRPO</a></div> <div><a href="#grpo-a-closer-look">GRPO: A Closer Look</a></div> <ul> <li><a href="#llm-as-a-policy">LLM as a Policy</a></li> <li><a href="#sequential-token-generation">Sequential Token Generation</a></li> <li><a href="#reward-and-advantage-calculation">Reward and Advantage Calculation</a></li> <li><a href="#the-grpo-objective">The GRPO Objective</a></li> </ul> <div><a href="#conclusion">Conclusion</a></div> <div><a href="#references">References</a></div> </nav> </d-contents> <h1 id="introduction">Introduction</h1> <p>Reinforcement Learning (RL) has emerged as a powerful tool for enhancing Large Language Models (LLMs) after their initial training, particularly in reasoning-intensive tasks. DeepSeek’s recent breakthroughs with DeepSeek-Math [2] and DeepSeek-R1 [3] models have demonstrated the remarkable potential of RL in improving mathematical reasoning and problem-solving abilities of LLMs.</p> <p>These achievements were made possible through an innovative RL approach called Group Relative Policy Optimization (GRPO), which addresses the unique challenges of applying RL to language models. In this post, we’ll dive deep into how GRPO works and why it represents a significant advancement in LLM training.</p> <h1 id="ppo-vs-grpo">PPO vs GRPO</h1> <p><img src="\assets\img\post_content\2024-01-29-grpo-files\overview.png" alt="PPO vs GRPO Comparison" style="margin:auto; display:block;" class="img-fluid rounded z-depth-1"></p> <p>Proximal Policy Optimization (PPO) [1] has been the go-to algorithm for RL fine-tuning of language models. At its core, PPO is a policy gradient method that uses clipping to limit policy updates (gradients), preventing destructive large policy changes. The objective function for PPO can be written as:</p> \[J_{PPO}(\theta) = \mathbb{E}[s \sim P(S), a \sim \pi_{\theta_{old}}(A\mid s)] \left[\min\left(\frac{\pi_\theta(a\mid s)}{\pi_{\theta_{old}}(a\mid s)}A(s,a), \text{clip}\left(\frac{\pi_\theta(a\mid s)}{\pi_{\theta_{old}}(a\mid s)}, 1-\epsilon, 1+\epsilon\right)A(s,a)\right)\right]\] <p>Where GRPO, introduced in [2] builds upon PPO’s foundation but introduces several key innovations that make it more <em>efficient</em> and better suited for language models:</p> <ol> <li>Eliminates the need for a value network, hence <strong>less memory/compute</strong> usage</li> <li>Uses group sampling for more efficient stable <strong>advantage estimation</strong> </li> <li>Uses a more conservative update mechanism through further penalizing both the objective and the rewards</li> </ol> <h1 id="grpo-a-closer-look">GRPO: A Closer Look</h1> <p><img src="\assets\img\post_content\2024-01-29-grpo-files\grpo.png" alt="GRPO Detailed" style="margin:auto; display:block;" class="img-fluid rounded z-depth-1"></p> <h2 id="llm-as-a-policy">LLM as a Policy</h2> <p>In GRPO, the language model serves as the policy network (actor), taking a question \(q\) as input observation \(s\) and producing a sequence of tokens as actions. The policy distribution factors across tokens:</p> \[\pi_\theta(a\mid q) = \prod_{t=1}^N \pi_\theta(a_t \mid q, a_{ &lt; t})\] <p><strong>Note:</strong> In the original paper [2], they use \(o_t\) to denote the output token at timestep \(t\). Whereas we use \(a_t\) instead to conform with standard RL notation of <strong>action</strong>.</p> <h2 id="sequential-token-generation">Sequential Token Generation</h2> <p><img src="\assets\img\post_content\2024-01-29-grpo-files\steps.png" alt="Sequential Generation" style="margin:auto; display:block;" class="img-fluid rounded z-depth-1"></p> <p>The generation process is inherently sequential because of auto-regressive nature of transformers/LLMs:</p> <ol> <li>Each token is generated conditionally on previous tokens</li> <li>The policy network (LLM) maintains a running context</li> <li>Each token generation step can be viewed as an action \(a_t\) in the RL framework</li> </ol> <h2 id="reward-and-advantage-calculation">Reward and Advantage Calculation</h2> <p>For each generated sequence, GRPO computes per-token rewards as follows:</p> \[r_t = r_\phi(q,a_{\leq t}) - \beta \log\frac{\pi_\theta(a_t \mid q,a_{ &lt; t})}{\pi_{ref}(a_t \mid q, a_{ &lt; t})}\] <p>Instead of using a value network, GRPO estimates baseline advantages \(A\) by normalizing a group (batch) of <strong>rewards</strong> obtained from <strong>sampling multiple different outputs</strong> from the <strong>reference policy</strong> produced in response to the same question as input. :</p> \[\hat{A}_{i,t} = \tilde{r}_i = \frac{r_i - \text{mean}(r)}{\text{std}(r)}\] <h2 id="the-grpo-objective">The GRPO Objective</h2> <p>for each question 𝑞, GRPO samples a group of outputs {𝑜1, 𝑜2, · · · , 𝑜𝐺} from the old policy 𝜋𝜃𝑜𝑙𝑑 and then optimizes the policy model by maximizing the GRPO objective. The complete GRPO objective brings everything together:</p> \[J_{GRPO}(\theta) = \frac{1}{G}\sum_{i=1}^G \frac{1}{\mid a_i \mid}\sum_{t=1}^{\mid a_i \mid} \left\{\min\left[\frac{\pi_\theta(a_{i,t} \mid s,a_{i, &lt; t})}{\pi_{\theta_{old}}(a_{i, t}\mid s, a_{i, &lt; t})}\hat{A}_{i,t}, \text{clip}\left(\frac{\pi_\theta(a_{i,t}\mid s, a_{ i, &lt; t })}{\pi_{\theta_{old}}(a_{i, t} \mid s, a_{i, &lt; t})}, 1-\epsilon, 1+\epsilon\right)\hat{A}_{i,t}\right] - \beta D_{KL}[\pi_\theta \mid \mid \pi_{ref}]\right\}\] <p>This objective:</p> <ol> <li>Averages over both groups and sequence lengths</li> <li>Uses clipping for conservative updates</li> <li>Includes an <strong>estimate</strong> of the KL divergence as a penalty to prevent large deviations from the reference model</li> </ol> <p><img src="\assets\img\post_content\2024-01-29-grpo-files\obj.png" alt="GRPO Objective"></p> <h1 id="conclusion">Conclusion</h1> <p>GRPO represents a significant advancement in applying RL to language models. By eliminating the need for a value network and introducing group-relative advantage estimation, it provides a more efficient and stable training process. The success of DeepSeek-Math and DeepSeek-R1 demonstrates the practical benefits of this approach.</p> <p>The key innovations of GRPO - group sampling, relative advantage estimation, and the elimination of the value network - provide a blueprint for future developments in LLM training. As we continue to push the boundaries of what language models can achieve, techniques like GRPO will be crucial in unlocking their full potential.</p> <h1 id="references">References</h1> <p>[1] Schulman, John, et al. Proximal Policy Optimization Algorithms. arXiv:1707.06347, arXiv, 28 Aug. 2017. arXiv.org, https://doi.org/10.48550/arXiv.1707.06347.</p> <p>[2] Shao, Zhihong, et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv:2402.03300, arXiv, 27 Apr. 2024. arXiv.org, https://doi.org/10.48550/arXiv.2402.03300.</p> <p>[3] DeepSeek-AI, et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948, arXiv, 22 Jan. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2501.12948.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ebrahimpichka/ebrahimpichka.github.io","data-repo-id":"MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==","data-category":"Comments","data-category-id":"DIC_kwDOA5PmLc4CTBt6","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"top","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Ebrahim Pichka. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-JF4RVNMJBY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-JF4RVNMJBY");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>