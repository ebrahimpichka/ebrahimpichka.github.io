<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Automatic Differentiation (AutoDiff): A Brief Intro with Examples | Ebrahim Pichka</title> <meta name="author" content="Ebrahim Pichka"> <meta name="description" content="An introduction to the mechanics of AutoDiff, exploring its mathematical principles, implementation strategies, and applications in currently most-used frameworks."> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon_32.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://epichka.com/blog/2024/autodiff/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Automatic Differentiation (AutoDiff): A Brief Intro with Examples",
      "description": "An introduction to the mechanics of AutoDiff, exploring its mathematical principles, implementation strategies, and applications in currently most-used frameworks.",
      "published": "October 10, 2024",
      "authors": [
        {
          "author": "Ebrahim Pichka",
          "authorURL": "",
          "affiliations": [
            {
              "name": "-",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Ebrahim Pichka</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Code</a> </li> <li class="nav-item "> <a class="nav-link" href="/resources/">Resources</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Automatic Differentiation (AutoDiff): A Brief Intro with Examples</h1> <p>An introduction to the mechanics of AutoDiff, exploring its mathematical principles, implementation strategies, and applications in currently most-used frameworks.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#1-the-role-of-differentiation-in-modern-ml-optimization">1. The Role of Differentiation in Modern ML Optimization</a></div> <div><a href="#2-the-differentiation-triad">2. The Differentiation Triad</a></div> <ul> <li><a href="#2-1-symbolic-differentiation">2.1 Symbolic Differentiation</a></li> <li><a href="#2-2-numeric-differentiation">2.2 Numeric Differentiation</a></li> <li><a href="#2-3-automatic-differentiation">2.3 Automatic Differentiation</a></li> </ul> <div><a href="#3-autodiff-modes-forward-and-reverse">3. AutoDiff Modes: Forward and Reverse</a></div> <ul> <li><a href="#3-1-forward-mode">3.1 Forward Mode</a></li> <li><a href="#3-2-reverse-mode">3.2 Reverse Mode</a></li> </ul> <div><a href="#4-implementation-strategies-operator-overloading-vs-source-transformation">4. Implementation Strategies: Operator Overloading vs. Source Transformation</a></div> <ul> <li><a href="#4-1-operator-overloading">4.1 Operator Overloading</a></li> <li><a href="#4-2-source-transformation">4.2 Source Transformation</a></li> </ul> <div><a href="#5-autodiff-in-the-wild-pytorch-vs-jax">5. AutoDiff in the Wild: PyTorch vs. JAX</a></div> <ul> <li><a href="#5-1-pytorch">5.1 PyTorch</a></li> <li><a href="#5-2-jax">5.2 JAX</a></li> </ul> <div><a href="#6-some-advanced-topics-in-autodiff">6. Some Advanced Topics in AutoDiff</a></div> <ul> <li><a href="#6-1-higher-order-derivatives">6.1 Higher-Order Derivatives</a></li> <li><a href="#6-2-vector-jacobian-products-vjps-and-jacobian-vector-products-jvps">6.2 Vector-Jacobian Products (VJPs) and Jacobian-Vector Products (JVPs)</a></li> <li><a href="#6-3-ad-through-iterative-processes">6.3 AD through Iterative Processes</a></li> </ul> <div><a href="#7-the-impact-of-autodiff-on-deep-learning">7. The Impact of AutoDiff on Deep Learning</a></div> <div><a href="#8-conclusion-and-future-directions">8. Conclusion and Future Directions</a></div> <div><a href="#references">References</a></div> </nav> </d-contents> <p>An introduction to the mechanics of AutoDiff, exploring its mathematical principles, implementation strategies, and applications in currently most-used frameworks</p> <p><img src="https://cdn-images-1.medium.com/max/12032/0*2zoQV7HydfU2dV8c" alt="Photo by [Bozhin Karaivanov](https://unsplash.com/@bkaraivanov?utm_source=medium&amp;utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&amp;utm_medium=referral)" style="margin:auto; display:block;" class="img-fluid rounded z-depth-1"></p> <hr> <h1 id="1-the-role-of-differentiation-in-modern-ml-optimization">1. The Role of Differentiation in Modern ML Optimization</h1> <p>At the heart of machine learning lies the optimization of loss/objective functions. This optimization process heavily relies on computing gradients of these functions with respect to model parameters. As Baydin et al. (2018) elucidate in their comprehensive survey [1], these gradients guide the iterative updates in optimization algorithms such as stochastic gradient descent (SGD):</p> \[\theta_{t+1} = \theta_{t} - \alpha \nabla_{\theta} \mathbb{L}_{\theta_{t}}(x)\] <p>Where:</p> <ul> <li> <p>\(\theta_{t}\) represents the model parameters at step t</p> </li> <li> <p>\(\alpha\) is the learning rate</p> </li> <li> <p>\(\nabla_{\theta}\mathbb{L}_{\theta_{t}}(x)\) denotes the gradient of the loss function \(\mathbb{L}\) with respect to the parameters \(\theta\)</p> </li> </ul> <p>This simple update rule belies the complexity of computing gradients in deep neural networks with millions or even billions of parameters.</p> <hr> <h1 id="2-the-differentiation-triad">2. The Differentiation Triad</h1> <p>Differentiation can generally be performed in three main manners. <strong>Symbolic</strong>, <strong>Numeric</strong>, and <strong>Automatic</strong> Differentiation. We will now briefly discuss the differences between them.</p> <h2 id="21-symbolic-differentiation">2.1 Symbolic Differentiation</h2> <p><em>Symbolic</em> differentiation involves the manipulation of mathematical expressions to produce exact derivatives. If you have ever taken any introductory courses in calculus, this method must’ve been your first exposure to differentiation. While it provides precise results, it often leads to expression swell, making it impractical for the complex, nested functions typical in machine learning [1].</p> <p>Consider the function \(f(x) = x^4 + 3x^2 + 2x\). Symbolic differentiation would yield:</p> \[f'(x) = 4x^3 + 6x + 2\] <p>While this is manageable for simple functions with clear analytical clsoed forms, imagine the complexity for a neural network with thousands of nonlinearities and multiple skip connections, branches, heads!</p> <h2 id="22-numeric-differentiation">2.2 Numeric Differentiation</h2> <p>Numeric differentiation approximates derivatives using finite differences following thw formal definition of derivatives, namely:</p> \[f'(x) ≈ \frac{f(x + h) - f(x)}{h}\] <p>This method simply tries to compute an approximate value for \(f'\) by <strong>assigning a</strong> <strong>very small value to \(h\)</strong> and computing the change it causes in the output of \(f\). While straightforward to implement, it’s realy susceptible to truncation errors (for large h) and round-off errors (for small h) [2]. Moreover, its computational cost scales poorly with the number of input variables as each input \(x_i\) would require calling of the function separately.</p> <h2 id="23-automatic-differentiation">2.3 Automatic Differentiation</h2> <p>In contrast with the two previous methods, Automatic Differentiation, <strong><em>AutoDiff</em></strong> for short, strikes a balance between symbolic and numeric methods, computing exact derivatives (up to machine precision) efficiently by systematically applying <a href="https://tutorial.math.lamar.edu/classes/calci/chainrule.aspx" rel="external nofollow noopener" target="_blank"><strong>the chain rule</strong></a> to elementary operations and functions [1]. In short, the chain rule basically says that the <strong>derivative of a composite function</strong> is the <strong>product of the derivatives of its component</strong> functions. Mathematically, if we have two functions \(y = f(u)\) and \(u = f(x)\), we have:</p> \[\frac{dy}{dx} = \frac{dy}{du} \times \frac{du}{dx}\] <p>where:</p> <ul> <li> <p><em>dy/dx</em> is the derivative of <em>y</em> with respect to <em>x</em> (the overall derivative we’re trying to find — in case of deep learning models, <em>y</em> is usually the finall loss and <em>x</em> is the doels weights)</p> </li> <li> <p><em>dy/du</em> is the derivative of <em>y</em> with respect to an intermediate variable <em>u</em></p> </li> <li> <p><em>du/dx</em> is the derivative of the intermediate variable <em>u</em> with respect to <em>x</em></p> </li> </ul> <p>Leveraging the chain rule, along with some implementation details that we are going to discuss next, allows us to compute gradients in a very optimal manner.</p> <hr> <h1 id="3-autodiff-modes-forward-and-reverse">3. AutoDiff Modes: Forward and Reverse</h1> <p>AutoDiff can be practically done in two ways, namely <strong>Forward</strong> mode and <strong>Reverse</strong> mode differentiation, each having some computational advantages and disadvantages based on the use case.</p> <h2 id="31-forward-mode">3.1 Forward Mode</h2> <p>Forward mode — also known as left-to-right — AutoDiff computes <strong>directional</strong> derivatives alongside the function evaluation. It’s particularly efficient for functions with few inputs and many outputs [3].</p> <p>Mathematically, for a function \(y = f(x)\) where \(x \in \mathrm{R}^n\) and $$y \in \mathrm{R}^m$, forward mode computes the Jacobian-vector product on the side:</p> <p><img src="https://cdn-images-1.medium.com/max/2000/1*wZ21Lgor0RqG1dHOOrSVHQ.png" alt="" style="margin:auto; display:block;" class="img-fluid rounded z-depth-1"></p> <p>Where \(J\) is the Jacobian matrix and \(\dot{x}\) is the seed vector. For a detailed explanation of Jacobian-vector product see <a href="https://maximerobeyns.com/of_vjps_and_jvps" rel="external nofollow noopener" target="_blank">here</a>.</p> <p>Let’s implement a simple forward mode AD:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Dual</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">derivative</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>
        <span class="n">self</span><span class="p">.</span><span class="n">derivative</span> <span class="o">=</span> <span class="n">derivative</span> <span class="c1"># works as \dot{x}
</span>
    <span class="k">def</span> <span class="nf">__add__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="nc">Dual</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">+</span> <span class="n">other</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">derivative</span> <span class="o">+</span> <span class="n">other</span><span class="p">.</span><span class="n">derivative</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__mul__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="nc">Dual</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">*</span> <span class="n">other</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> 
                    <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">*</span> <span class="n">other</span><span class="p">.</span><span class="n">derivative</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">derivative</span> <span class="o">*</span> <span class="n">other</span><span class="p">.</span><span class="n">value</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__pow__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="k">return</span> <span class="nc">Dual</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">**</span> <span class="n">n</span><span class="p">,</span> 
                    <span class="n">n</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">**</span> <span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">derivative</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span>

<span class="n">x</span> <span class="o">=</span> <span class="nc">Dual</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>  <span class="c1"># x = 2, dx/dx = 1 (~&gt; \dot{x})
</span><span class="n">result</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">f(2) = </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">value</span><span class="si">}</span><span class="s">, f</span><span class="sh">'</span><span class="s">(2) = </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">derivative</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># &gt;&gt; Output: f(2) = 42.0, f'(2) = 58.0
</span></code></pre></div></div> <p>As demonstrated, Forward autodiff augments each intermediate variable during evaluation of a function <strong>with its derivative</strong>. It involves replacing individual floating point values flowing through a function with <strong>tuples of the original intermediate values also called primals paired with their derivatives</strong>.</p> <p>To compute the partial derivative of a function with respect to an input variable, we have to run <strong>a separate forward pass</strong> for each input variable of interest with corresponding seed set to 1. The forward mode autodiff produces one column of the corresponding <strong>Jacobian</strong> \(J\).</p> <p>For a two dimensional input \(x \in \mathrm{R}^2\), setting \(\dot{x}\) to [1, 0] yields the first column of \(J\) which is the partial derivative w.r.t \(x_1\) and setting it to [0, 1] results in the second column which is the partial derivative w.r.t \(x_2\).</p> <p>Ari Seff does a great job explaining it in his <a href="https://youtu.be/wG_nF1awSSY?t=305" rel="external nofollow noopener" target="_blank"><strong>AutoDiff video here</strong></a>.</p> <h2 id="32-reverse-mode">3.2 Reverse Mode</h2> <p>Reverse mode AutoDiff, which is the main AD method used in current major deep learning frameworks, computes gradients by propagating derivatives by <strong>going backward</strong> through the <a href="https://simple-english-machine-learning.readthedocs.io/en/latest/neural-networks/computational-graphs.html" rel="external nofollow noopener" target="_blank"><strong>computation graph (see [6])</strong></a> starting from the output and then applying the chain rule until it traverses the whole graph. It’s particularly efficient for functions with many inputs and few outputs, which is the typical case in neural networks [3].</p> <p>Reverse mode computes the vector-Jacobian product which is explained PyTorch’s introduction to AtuoDiff in the “<strong>Vector Calculus using autograd</strong>” section <a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html" rel="external nofollow noopener" target="_blank">here</a>.</p> <p>Here’s a simplified implementation of reverse mode AD:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Node</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>
        <span class="n">self</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="bp">None</span> <span class="c1"># this is defined as the forward mode is done based on the computation graph. 
</span>        <span class="n">self</span><span class="p">.</span><span class="n">_prev</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__add__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Node</span><span class="p">)</span> <span class="k">else</span> <span class="nc">Node</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nc">Node</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">+</span> <span class="n">other</span><span class="p">.</span><span class="n">value</span><span class="p">)</span>
        <span class="n">out</span><span class="p">.</span><span class="n">_prev</span> <span class="o">=</span> <span class="p">{</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">}</span>
        <span class="k">def</span> <span class="nf">_backward</span><span class="p">():</span>
            <span class="n">self</span><span class="p">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">out</span><span class="p">.</span><span class="n">grad</span>
            <span class="n">other</span><span class="p">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">out</span><span class="p">.</span><span class="n">grad</span>
        <span class="n">out</span><span class="p">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="n">_backward</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">__mul__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Node</span><span class="p">)</span> <span class="k">else</span> <span class="nc">Node</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nc">Node</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">*</span> <span class="n">other</span><span class="p">.</span><span class="n">value</span><span class="p">)</span>
        <span class="n">out</span><span class="p">.</span><span class="n">_prev</span> <span class="o">=</span> <span class="p">{</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">}</span>
        <span class="k">def</span> <span class="nf">_backward</span><span class="p">():</span>
            <span class="n">self</span><span class="p">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">other</span><span class="p">.</span><span class="n">value</span> <span class="o">*</span> <span class="n">out</span><span class="p">.</span><span class="n">grad</span>
            <span class="n">other</span><span class="p">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">*</span> <span class="n">out</span><span class="p">.</span><span class="n">grad</span>
        <span class="n">out</span><span class="p">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="n">_backward</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">__pow__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nc">Node</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">**</span> <span class="n">n</span><span class="p">)</span>
        <span class="n">out</span><span class="p">.</span><span class="n">_prev</span> <span class="o">=</span> <span class="p">{</span><span class="n">self</span><span class="p">}</span>

        <span class="k">def</span> <span class="nf">_backward</span><span class="p">():</span>
            <span class="n">self</span><span class="p">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">n</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">**</span> <span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">out</span><span class="p">.</span><span class="n">grad</span> 

        <span class="n">out</span><span class="p">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="n">_backward</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">node</span><span class="p">):</span>
    <span class="n">topo</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">visited</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">build_topo</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">v</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">visited</span><span class="p">:</span>
            <span class="n">visited</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">v</span><span class="p">.</span><span class="n">_prev</span><span class="p">:</span>
                <span class="nf">build_topo</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
            <span class="n">topo</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

    <span class="nf">build_topo</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
    <span class="n">node</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="n">topo</span><span class="p">):</span>
        <span class="n">node</span><span class="p">.</span><span class="nf">_backward</span><span class="p">()</span>


<span class="c1"># Example usage
</span><span class="n">x</span> <span class="o">=</span> <span class="nc">Node</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span>

<span class="nf">backward</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">f(2) = </span><span class="si">{</span><span class="n">y</span><span class="p">.</span><span class="n">value</span><span class="si">}</span><span class="s">, f</span><span class="sh">'</span><span class="s">(2) = </span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># &gt;&gt; Output: f(2) = 42.0, f'(2) = 58.0
</span></code></pre></div></div> <p>This implementation builds a computational graph and then traverses it backwards when the backward method is called on the output, to compute gradients, mimicking the process used in deep learning frameworks.</p> <p>The <strong>key difference</strong> in computational complexity between <strong>forward</strong> and <strong>reverse</strong> modes becomes apparent when we consider functions with many inputs (parameters) and few outputs (typically a single loss value in ML), making reverse mode the preferred choice for deep learning [1]. the reason is that, in forward mode, computing the gradient for each input element \(x_i\) requires a <strong>separate</strong> forward pass through the computational graph.</p> <hr> <h1 id="4-implementation-strategies-operator-overloading-vs-source-transformation">4. Implementation Strategies: Operator Overloading vs. Source Transformation</h1> <h2 id="41-operator-overloading">4.1 Operator Overloading</h2> <p>Operator overloading, as demonstrated in our previous examples, redefines mathematical operations to compute both the result and its derivative. It’s the approach used by PyTorch and many Python-based AD libraries [4].</p> <h2 id="42-source-transformation">4.2 Source Transformation</h2> <p>Source transformation analyzes and modifies the source code to insert derivative computations. While more complex to implement, it can lead to more optimized code, especially for <strong>static</strong> computational graphs [1]. Tools like Tapenade use this approach.</p> <p>Here’s a conceptual example of how source transformation might work:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Original function
</span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span>

<span class="c1"># Transformed function (conceptual, not actual code)
</span><span class="k">def</span> <span class="nf">f_and_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># Forward pass
</span>    <span class="n">t1</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">t2</span> <span class="o">=</span> <span class="n">t1</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">t3</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">t1</span>
    <span class="n">t4</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">t2</span> <span class="o">+</span> <span class="n">t3</span> <span class="o">+</span> <span class="n">t4</span>

    <span class="c1"># Backward pass
</span>    <span class="n">dy</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">dt4</span> <span class="o">=</span> <span class="n">dy</span>
    <span class="n">dt3</span> <span class="o">=</span> <span class="n">dy</span>
    <span class="n">dt2</span> <span class="o">=</span> <span class="n">dy</span>
    <span class="n">dt1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="n">dt3</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">t1</span> <span class="o">*</span> <span class="n">dt2</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">3</span> <span class="o">*</span> <span class="n">dy</span> <span class="o">+</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="n">dy</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">dy</span>

    <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">dx</span>
</code></pre></div></div> <p>This transformed version computes both the function value and its gradient in a single pass through the code. As you can see, it is not as flexible and scalable for large-scale purposes such as in deep learning.</p> <hr> <h1 id="5-autodiff-in-the-wild-pytorch-vs-jax">5. AutoDiff in the Wild: PyTorch vs. JAX</h1> <h2 id="51-pytorch">5.1 PyTorch</h2> <p>PyTorch uses a dynamic computational graph, built on-the-fly as operations are performed. This allows for flexibility in network architecture and easier debugging [5].</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">f(2) = </span><span class="si">{</span><span class="n">y</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">, f</span><span class="sh">'</span><span class="s">(2) = </span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># Output: f(2) = 42.0, f'(2) = 58.0
</span></code></pre></div></div> <p>PyTorch’s autograd engine records operations in a directed acyclic graph (DAG), where leaves are input tensors and roots are output tensors. During the backward pass, it computes gradients by traversing this graph [5].</p> <p>For very detailed explanation to get a sense of how PyTorch’s autograd works, i would extremely recommend the first to videos of Andrej Karpathy’s <a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ" rel="external nofollow noopener" target="_blank"><strong>Neural Networks: Zero to Hero</strong></a> playlist.</p> <h2 id="52-jax">5.2 JAX</h2> <p>JAX, developed by Google Research, on the the hand uses a static computational graph and leverages XLA (Accelerated Linear Algebra) for efficient compilation to achieve better performance. It provides function transformations like grad for automatic differentiation, vmap for vectorization, and jit for compilation [4].</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>

<span class="nd">@jax.jit</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="mf">2.0</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">f(2) = </span><span class="si">{</span><span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s">, f</span><span class="sh">'</span><span class="s">(2) = </span><span class="si">{</span><span class="nf">df</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># Output: f(2) = 42.0, f'(2) = 58.0
</span>
<span class="c1"># Vectorized computation
</span><span class="n">vdf</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="n">x_vec</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">f</span><span class="sh">'</span><span class="s">(x) for x=[1,2,3]: </span><span class="si">{</span><span class="nf">vdf</span><span class="p">(</span><span class="n">x_vec</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># Output: f'(x) for x=[1,2,3]: [10. 58. 154.]
</span></code></pre></div></div> <p>JAX’s approach allows for efficient compilation and execution on accelerators like GPUs and TPUs [4]. Check out the <a href="https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html" rel="external nofollow noopener" target="_blank"><strong>“The Autodiff Cookbook”</strong></a> from JAX developers for a more technical grasp of their implementations.</p> <p>Note the difference that PyTorch’s implementation requires that first a forward pass is done with an input, then as the backwards are computed, the gradients are accessible, whereas in JAX, the jax.grad transformation can be called on any defined function without the need to calling the function itself manually.</p> <hr> <h1 id="6-some-advanced-topics-in-autodiff">6. Some Advanced Topics in AutoDiff</h1> <h2 id="61-higher-order-derivatives">6.1 Higher-Order Derivatives</h2> <p>One thing o note is that AutoDiff isn’t limited to first-order derivatives. By applying AD to its own output, we can compute higher-order derivatives. This is crucial for optimization algorithms like Newton’s method that use second-order information (Hessians).</p> <p>In JAX particularly, computing higher-order derivatives is pretty straightforward:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>


<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span>

<span class="n">ddf</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">f</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="mf">2.0</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">f</span><span class="sh">''</span><span class="s">(2) = </span><span class="si">{</span><span class="nf">ddf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># Output: f''(2) = 102.0
</span></code></pre></div></div> <p>just call the grad function transformation twice on your function and you’re good to go.</p> <h2 id="62-vector-jacobian-products-vjps-and-jacobian-vector-products-jvps">6.2 Vector-Jacobian Products (VJPs) and Jacobian-Vector Products (JVPs)</h2> <p>VJPs and JVPs are the building blocks of reverse and forward mode AD, respectively. Understanding these operations is crucial for implementing efficient custom gradients.</p> <p>JAX provides explicit functions for these operations:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>


<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">x</span><span class="o">**</span><span class="mi">3</span><span class="p">])</span>

<span class="n">x</span> <span class="o">=</span> <span class="mf">2.0</span>
<span class="n">y</span><span class="p">,</span> <span class="n">vjp_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">vjp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">VJP: </span><span class="si">{</span><span class="nf">vjp_fn</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]))[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="n">primal</span><span class="p">,</span> <span class="n">jvp_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">jvp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">JVP: </span><span class="si">{</span><span class="n">jvp_fn</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Output:
# VJP: 16.0
# JVP: [4. 12.]
</span></code></pre></div></div> <h2 id="63-ad-through-iterative-processes">6.3 AD through Iterative Processes</h2> <p>Applying AD to iterative processes like optimization loops or recurrent neural networks requires careful handling to avoid excessive memory usage. Techniques like checkpointing and reversible computations are used to balance memory usage and computational cost [1].</p> <hr> <h1 id="7-the-impact-of-autodiff-on-deep-learning">7. The Impact of AutoDiff on Deep Learning</h1> <p>AutoDiff, particularly <strong>reverse</strong> mode AD, has been instrumental in the deep learning revolution. It allows efficient computation of gradients for millions of parameters with respect to a loss value. This efficiency enables the training of increasingly complex models, driving advancements in areas like natural language processing, computer vision, and reinforcement learning [2].</p> <p>Some key impacts to mention:</p> <ol> <li> <p><strong>Architectural Flexibility</strong>: AD allows researchers to easily experiment with novel network architectures without manually deriving gradients.</p> </li> <li> <p><strong>Computational Efficiency</strong>: Reverse mode AD makes it feasible to train very deep networks with millions or billions of parameters.</p> </li> <li> <p><strong>Higher-Order Optimization</strong>: Easy access to higher-order derivatives enables more sophisticated optimization techniques.</p> </li> <li> <p><strong>Custom Differentiable Operations</strong>: Researchers can define custom differentiable operations, expanding the range of possible model architectures.</p> </li> </ol> <hr> <h1 id="8-conclusion-and-future-directions">8. Conclusion and Future Directions</h1> <p>Automatic Differentiation has become an indispensable tool in machine learning, enabling the training of increasingly complex models. As we push the boundaries of AI, several exciting directions for AD research emerge:</p> <ol> <li> <p><strong>AD for Probabilistic Programming</strong>: Extending AD to handle probabilistic computations and enable more flexible Bayesian inference.</p> </li> <li> <p><strong>Differentiable Programming</strong>: Moving beyond traditional neural networks to make entire programs differentiable.</p> </li> <li> <p><strong>Hardware-Specific Optimizations</strong>: Tailoring AD algorithms for specialized AI hardware.</p> </li> <li> <p><strong>AD for Sparse and Structured Computations</strong>: Developing efficient AD techniques for sparse or structured problems common in scientific computing.</p> </li> </ol> <p>As these areas develop, we can expect AutoDiff to continue playing a crucial role in advancing the field of machine learning and artificial intelligence.</p> <hr> <h1 id="references">References</h1> <p>[1] Baydin, A. G., Pearlmutter, B. A., Radul, A. A., &amp; Siskind, J. M. (2018). Automatic differentiation in machine learning: a survey. Journal of Machine Learning Research, 18, 1–43.</p> <p>[2] Grosse, R. (2019). Automatic Differentiation. CSC421/2516 Lecture Notes, University of Toronto.</p> <p>[3] Andmholm. (2023). What is Automatic Differentiation. Hugging Face Blog.</p> <p>[4] JAX Team. (2024). Automatic Differentiation and the JAX Ecosystem. JAX Documentation.</p> <p>[5] PyTorch Team. (2024). Autograd: Automatic Differentiation. PyTorch Tutorials.</p> <p>[6] <a href="https://simple-english-machine-learning.readthedocs.io/en/latest/neural-networks/computational-graphs.html" rel="external nofollow noopener" target="_blank">https://simple-english-machine-learning.readthedocs.io/en/latest/neural-networks/computational-graphs.html</a></p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ebrahimpichka/ebrahimpichka.github.io","data-repo-id":"MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==","data-category":"Comments","data-category-id":"DIC_kwDOA5PmLc4CTBt6","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"top","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2026 Ebrahim Pichka. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-JF4RVNMJBY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-JF4RVNMJBY");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>