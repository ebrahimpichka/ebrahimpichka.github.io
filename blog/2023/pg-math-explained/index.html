<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Policy Gradient Algorithm’s Mathematics Explained with PyTorch Implementation | Ebrahim Pichka</title> <meta name="author" content="Ebrahim Pichka"> <meta name="description" content="A step-by-step explanation of the vanilla policy gradient algorithm and its implementation."> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon_32.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://epichka.com/blog/2023/pg-math-explained/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Ebrahim Pichka</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Code</a> </li> <li class="nav-item "> <a class="nav-link" href="/resources/">Resources</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Policy Gradient Algorithm’s Mathematics Explained with PyTorch Implementation</h1> <p class="post-meta">May 23, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/rl"> <i class="fas fa-hashtag fa-sm"></i> RL</a>   <a href="/blog/tag/policy-gradient"> <i class="fas fa-hashtag fa-sm"></i> policy-gradient</a>   <a href="/blog/tag/reinforce"> <i class="fas fa-hashtag fa-sm"></i> REINFORCE</a>     ·   <a href="/blog/category/reinforcement-learning"> <i class="fas fa-tag fa-sm"></i> reinforcement-learning</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2048/0*VgbxFJr_l6SmL1H-.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>Table of Content</strong></p> <ul> <li><strong><a href="#introduction">Introduction</a></strong></li> <li> <strong><a href="#policy-gradient-method">Policy Gradient Method</a></strong> <ul> <li><a href="#derivation">Derivation</a></li> <li><a href="#optimization">Optimization</a></li> <li><a href="#the-algorithm">The Algorithm</a></li> </ul> </li> <li> <strong><a href="#pyTorch-implementation">PyTorch Implementation</a></strong> <ul> <li><a href="#networks">Networks</a></li> <li><a href="#training-loop-main-algorithm">Training Loop (Main algorithm)</a></li> <li><a href="#training-Results">Training Results</a></li> </ul> </li> <li><strong><a href="#conclusion">Conclusion</a></strong></li> <li><strong><a href="#references">References</a></strong></li> </ul> <hr> <h2 id="introduction">Introduction</h2> <p>Reinforcement Learning (RL) is a subdomain of AI that aims to enable machines to learn and improve their behavior by interacting with an environment and receiving feedback in the form of reward signals. This environment is mathematically formulated as a Markov Decision Process (MDP) where at each timestep, the agent is known to be at a certain state <strong>(s ∈ <em>S</em>)</strong> where it is able to take action <strong>(a ∈ <em>A</em>)</strong>. This action results in a transition from state <strong>s</strong> to a new state <strong>(s’ ∈ <em>S)</em></strong> with a certain probability from a dynamics function <strong><em>P(s, a, s’)</em></strong> and receiving a scalar reward <strong>r</strong> from a reward function <strong><em>R(s, a, s’)</em></strong>. That said, MDPs can be shown by a tuple of sets <strong>(<em>S, A, P, R, γ</em>)</strong> in which γ ∈ (0, 1] is a discount factor for future steps’ rewards.</p> <p>RL algorithms can be generally categorized into two groups i.e., value-based and policy-based methods. Value-based methods aim at estimating the expected return of the states and selecting an action in that state which results in the highest expected value, which is rather an indirect way of behaving optimally in an MDP environment. In contrast, policy-based methods try to learn and optimize a policy function, which is basically a mapping from states to actions. The Policy Gradient (PG) method <strong>[1][2]</strong> is a popular policy-based approach in RL, which **directly **optimizes the policy function by changing its parameters using gradient ascent.</p> <p>PG has some advantages over value-based methods, especially when dealing with environments with continuous action spaces or high stochasticity. PG can also handle non-differentiable policies, making it suitable for complex scenarios. However, PG can suffer from some issues such as high variance in the gradient estimates, which can lead to slower convergence and/or instability.</p> <p>In this post, we will lay out the Policy Gradient method in detail, while examining its strengths and limitations. We will discuss the intuition behind PG, how it works, and also provide a code implementation of the algorithm. I will try to cover various modifications and extensions that have been proposed to improve its performance in further posts.</p> <hr> <h2 id="policy-gradient-method">Policy Gradient Method</h2> <p>As explained above, Policy Gradient (PG) methods are algorithms that aim to learn the optimal policy function directly in a Markov Decision Processes setting (*S, A, P, R, *γ). In PG, the **policy **π is represented by a parametric function (e.g., a neural network), so we can control its outputs by changing its parameters. The **policy **π maps state to actions (or probability distributions over actions).</p> <p>The goal of PG is to achieve a policy that maximizes the <strong>expected cumulative rewards</strong> over a trajectory of states and actions (A.K.A. the return). Let us go through how it achieves to do so.</p> <h3 id="derivation">Derivation</h3> <p>So, considering we have a <strong>policy function π</strong>, parameterized by a parameter set θ. Given a state as input, The <strong>policy</strong> outputs a probability distribution over actions at each state. Knowing this, first, we need to come up with an <strong>objective/goal</strong> so that we would want to optimize our policy function’s parameters with regard to this goal.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2036/1*sSnx2pL_3_VGpc3ZZXbL2Q.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Note that this policy function could be approximated with any function approximation technique. However, in the case of <strong>Deep RL</strong>, we consider this function to be approximated by a <strong>Neural Network</strong> (with parameter set θ) that takes states (observations) as input and outputs the distribution over actions. It could be a discrete distribution for discrete actions or a continuous distribution for continuous actions.</p> <p>In general, the agent’s goal would be to obtain the maximum cumulative reward over a <strong>trajectory of interactions</strong> (state-action sequences). So, let <strong><em>τ</em></strong> denote such trajectory i.e., a state-action sequence <em>s₀, a₀, s₁, a₁, …, sₕ, aₕ</em>, And <strong>R(<em>τ</em>)</strong> denote the cumulative reward over this trajectory a.k.a <strong>the return</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*0X4fXdoylb1r-IUWIyWpjA.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>It is obvious that this trajectory <strong><em>τ,</em></strong> is a random variable because of its stochasticity. This makes <strong>R(<em>τ</em>)</strong> to be a stochastic function as well. As we can not maximize this stochastic function directly, we would want to maximize its expectation, meaning to maximize it on average case, while taking actions with policy π: <strong>E[ R(<em>τ</em>); π ]</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*pcz-xb2fsky67jbs92UtOw.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>And as a refresher on probabilities, this is how to calculate a discrete expectation of a function of a random variable:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2240/1*AYQ2TmEAwFop75ErmlP4ow.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>So the final objective function that we would want to maximize is rewritten as follows. Also, we will now call this objective function, <strong>the Utility Function (U)</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*x8fp_A1URSh8yP6Qma1fyQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>where <strong>P(<em>τ</em>; θ)</strong>is the probability of trajectory <em>τ</em> taking place over policy parameters θ.</p> <p>Note that this utility function is written as a function of <strong>θ</strong>, a set of policy parameters, because <strong>it is only the policy that controls the path of the trajectories</strong> since the environment dynamic is fixed (and usually unknown), and we would want to optimize this utility by changing and optimizing <strong>θ</strong>. Also, note that the reward series <strong>R</strong> over the trajectory is not dependent on the policy. Hence <strong>it is not dependent on the parameters θ</strong> because it is based on the environment and is only an experienced scalar value.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*NMBuSdL3haA9ZgTiORZeJw.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="optimization">Optimization</h3> <p>Now that we have the Utility function <strong>U(θ)</strong>, we would want to optimize it using a stochastic gradient-based optimization algorithm, namely <strong>Stochastic Gradient Ascent:</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*GnbR_fRQh3ofJZlvVQ_7vg.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>To be able to perform the parameter update, we have to compute the gradient of the Utility over <strong>θ</strong> (<strong>∇U</strong>). By attempting to do so, we get:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*8-oK4kmbJJ0EARps3EZHiw.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Note that the gradient over the summation is equal to the sum of gradients. Now we continue by applying a trick by multiplying this expression by P(<em>τ</em> ; θ)/P(<em>τ</em>; θ), which equals <strong>1</strong>. Remember from calculus that <strong>∇f(x)/f(x) = ∇log(f(x))</strong>, so we get:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*rWKTFBQYFiw8cXqfb6-S7A.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We do this trick to create a general form of expectation <strong>‘ΣP(x)f(x)’</strong> in the gradient’s formula, which we can later replace and estimate the <strong>expectation</strong> with <strong>the average over samples</strong> of real interaction data.</p> <p>As we cannot sum over ALL possible trajectories, in a stochastic setting, we estimate this expression (∇U(θ)), by generating “<strong>m</strong>” number of random trajectories using policy parameters θ and averaging the above expectation over these <strong>m</strong> trajectories to estimate the gradient ∇U(θ):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*pNUZAWxPw9BBN789Y-9fhQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>So now we have a way of estimating the gradient with trajectory samples, but there is an issue here. this gradient causes parameters θ to change in a direction that the probability of trajectories with higher return <em>*R(</em>τ<em>)</em> ***increases the most. However, this has a disadvantage, and that is the direction of change in the trajectory probability is highly dependent on how reward function R is designed. For example, if all transitions result in a positive reward, all trajectory probabilities would be increased and no trajectory would be penalized, and vice versa for cases with negative R.</p> <p>To address this issue, it is proposed to subtract a <strong>baseline value (b)</strong> [1] from the return (i.e., change ‘<strong>R(<em>τ</em>)</strong>’ to ‘<strong>R(<em>τ</em>)-b</strong>’) where this baseline <strong>b</strong> should estimate how the return would be on average. This helps returns to be centralized around zero which makes trajectory probabilities that performed better than average are increased and those that didn’t are decreased. There are multiple proposed ways of calculating baseline b, among which using a neural network is one that we will use later.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*Sq5fzKKYBVfHuf5QmrxYPw.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Further, by decomposing the trajectory probability P(<em>τ</em>) into Temporal timesteps and writing it as the <strong>product of the probabilities of all ‘H’ timesteps</strong>, and also by knowing that the environment dynamics <strong>does NOT depend on the parameters θ</strong> so its gradient over θ would be 1, we can rewrite the Trajectory’s gradient w.r.t. Temporal timesteps and solely based on the policy function <strong>π</strong>:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*QtJ9V0uA-_u5Qw_nilItVw.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Also, we can decompose the return <strong>R(<em>τ</em>)</strong> as a sum of individual timestep rewards Σrₜ. And at each timestep t, we can <strong>disregard</strong> the rewards from <strong>previous timesteps</strong> 0, … , t-1 since they are <strong>not affected by the current action aₜ</strong> (Removing terms that don’t depend on current action can lower variance). Together with incorporating the baseline, we would have the following Temporal format:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2050/1*JQ7oFxDVOJTl09NSD8bcyw.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>So the <strong>gradient estimate ‘g’</strong> will be:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*O85g5ZKpI3vY2QAKFjcWrA.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Finally, as a proper choice for estimation of the baseline b(sₜ), we can use expected return from state sₜ onward, which is also known as the <strong>Value function</strong> of this state <strong>V(sₜ)</strong>. We will use another neural network with parameter set <strong><em>ϕ</em></strong> to approximate this value function with a bellman target using either Monte Carlo or Temporal difference learning from interaction data. the final form would be as follows:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*053-3GF2kvsC6Xkz_SoRMQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Moreover, it is good to know that the difference between <strong>R(sₜ)</strong> and the baseline <strong>b(sₜ)</strong> is called the <strong>Advantage function A(sₜ)</strong>. In some implementations, as the <strong>R(sₜ)</strong> equivalences the state-action value, also known as the <strong>Q function Q(sₜ)</strong>, the advantage is written as <strong>A(sₜ) = Q(sₜ)-V(sₜ)</strong> where both Q and V can be approximated with neural networks, and maybe even with shared weights.</p> <p>It is worth mentioning that when we incorporate value estimation in policy gradient methods, it is also called <strong>the Actor-Critic method</strong>, while the <strong>actor</strong> is the policy network and the <strong>critic</strong> is the value estimator network. These types of methods are highly studied nowadays due to their good performance in various benchmarks and tasks.</p> <h3 id="the-algorithm">The Algorithm</h3> <p>Based on the derived <strong>gradient estimator g</strong>, the <strong>Value function</strong> approximation network, and the <strong>gradient ascent update rule</strong>, the Overall algorithm to train an agent with the Vanilla (simple) Policy Gradient algorithm is as follows:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*bsYAtBUIumT2yoU_bu-2jQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Note that in this algorithm, first, a number of state-action sequences (trajectories) are done then the updates are carried away. The policy network here is updated in an on-policy or online manner, whereas the value functions can be updated off-policy (offline) from batch-sampled data using gradient descent.</p> <hr> <h2 id="pytorch-implementation">PyTorch Implementation</h2> <p>To implement VPG, we need the following components:</p> <ul> <li> <strong>Policy Network</strong> with probabilistic outputs to sample from. (with Softmax output for discrete action space, or parameter estimations such as <strong>μ, σ</strong> output for Gaussian dist. for continuous action spaces)</li> <li> <strong>Value Network</strong> for Advantage estimation.</li> <li> <strong>Environment</strong> class with <a href="https://gymnasium.farama.org/" rel="external nofollow noopener" target="_blank">gym</a> interface for the agent to interact with.</li> <li><strong>Training loop</strong></li> </ul> <h3 id="networks">Networks</h3> <p>First of all, we define **Policy **and **Value Networks **as PyTorch Module classes. We are using simple Multi-layer perceptron networks for this toy task.</p> <p><strong>Importing Dependencies:</strong></p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># import dependencies
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="n">torch.distributions</span> <span class="kn">import</span> <span class="n">Categorical</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">gym</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">deque</span>
</code></pre></div></div> <p><strong>Policy Net:</strong></p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PolicyNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">PolicyNet</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rewards</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">saved_actions</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">aprob</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Softmax for categorical probabilities
</span>        <span class="k">return</span> <span class="n">aprob</span>
</code></pre></div></div> <p><strong>Value Net:</strong></p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ValueNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ValueNet</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">V</span>
</code></pre></div></div> <h3 id="training-loop-main-algorithm">Training Loop (Main algorithm)</h3> <p>We will be using a simple Cartpole environment from the <a href="https://gymnasium.farama.org/" rel="external nofollow noopener" target="_blank"><strong>gym</strong></a>library. You can read more about this environment and its state and action spaces <strong><a href="https://gymnasium.farama.org/environments/classic_control/cart_pole/" rel="external nofollow noopener" target="_blank">here</a></strong>.</p> <p>The <strong>full algorithm</strong> is as follows:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## Vanilla Policy Gradient
</span>
<span class="c1"># create environment
</span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">"</span><span class="s">CartPole-v1</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># sample toy environment
</span>
<span class="c1"># instantiate the policy and value networks
</span><span class="n">policy</span> <span class="o">=</span> <span class="nc">PolicyNet</span><span class="p">(</span><span class="n">state_dim</span><span class="o">=</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_actions</span><span class="o">=</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span> <span class="n">n_hidden</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="n">value</span> <span class="o">=</span> <span class="nc">ValueNet</span><span class="p">(</span><span class="n">state_dim</span><span class="o">=</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_hidden</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

<span class="c1"># instantiate an optimizer
</span><span class="n">policy_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">policy</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">2e-7</span><span class="p">)</span>
<span class="n">value_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">value</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">)</span>

<span class="c1"># initialize gamma and stats
</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span>
<span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">returns_deq</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">memory_buffer_deq</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">2000</span><span class="p">)</span>

<span class="k">for</span> <span class="n">n_ep</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">actions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">states</span>  <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># reset environment
</span>    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="c1"># recieve action probabilities from policy function
</span>        <span class="n">probs</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">state</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">float</span><span class="p">())</span>

        <span class="c1"># sample an action from the policy distribution
</span>        <span class="n">policy_prob_dist</span> <span class="o">=</span> <span class="nc">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">policy_prob_dist</span><span class="p">.</span><span class="nf">sample</span><span class="p">()</span>

        <span class="c1"># take that action in the environment
</span>        <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>

        <span class="c1"># store state, action and reward
</span>        <span class="n">states</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">actions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">rewards</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>

        <span class="n">memory_buffer_deq</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">new_state</span><span class="p">))</span>

        <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>

    <span class="c1">### UPDATE POLICY NET ###
</span>    <span class="n">rewards</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
    <span class="c1"># calculate rewards-to-go
</span>    <span class="n">R</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span><span class="o">*</span><span class="p">(</span><span class="n">gamma</span><span class="o">**</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">)))))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">))])</span>

    <span class="c1"># cast states and actions to tensors
</span>    <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">states</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
    <span class="n">actions</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>

    <span class="c1"># calculate baseline V(s)
</span>    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">baseline</span> <span class="o">=</span> <span class="nf">value</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>

    <span class="c1"># calculate utility func
</span>    <span class="n">probs</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
    <span class="n">sampler</span> <span class="o">=</span> <span class="nc">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="o">-</span> <span class="n">sampler</span><span class="p">.</span><span class="nf">log_prob</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>   <span class="c1"># "-" is because we are doing gradient ascent
</span>    <span class="n">utility</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">log_probs</span> <span class="o">*</span> <span class="p">(</span><span class="n">R</span><span class="o">-</span><span class="n">baseline</span><span class="p">))</span> <span class="c1"># loss that when differentiated with autograd gives the gradient of J(θ)
</span>    
    <span class="c1"># update policy weights
</span>    <span class="n">policy_optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">utility</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">policy_optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
    
    <span class="c1">########################
</span>    <span class="c1">### UPDATE VALUE NET ###
</span>
    <span class="c1"># getting batch experience data 
</span>    <span class="n">batch_experience</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">memory_buffer_deq</span><span class="p">),</span> <span class="nf">min</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">memory_buffer_deq</span><span class="p">)))</span>
    <span class="n">state_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">exp</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">exp</span> <span class="ow">in</span> <span class="n">batch_experience</span><span class="p">])</span>
    <span class="n">reward_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">exp</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">exp</span> <span class="ow">in</span> <span class="n">batch_experience</span><span class="p">]).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">new_state_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">exp</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">exp</span> <span class="ow">in</span> <span class="n">batch_experience</span><span class="p">])</span>


    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">reward_batch</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="nf">value</span><span class="p">(</span><span class="n">new_state_batch</span><span class="p">)</span>
    <span class="n">current_state_value</span> <span class="o">=</span> <span class="nf">value</span><span class="p">(</span><span class="n">new_state_batch</span><span class="p">)</span>

    <span class="n">value_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">current_state_value</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="c1"># update value weights
</span>    <span class="n">value_optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">value_loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">value_optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

    <span class="c1">########################
</span>
    <span class="c1"># calculate average return and print it out
</span>    <span class="n">returns_deq</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Episode: {:6d}</span><span class="se">\t</span><span class="s">Avg. Return: {:6.2f}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">n_ep</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">returns_deq</span><span class="p">)))</span>

<span class="c1"># close environment
</span><span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</code></pre></div></div> <h3 id="training-results">Training Results</h3> <p>After training the agent with the VPG for <strong>4000 episodes</strong>, we get the following results:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*rQwO-I_CqpDB9_A43qUJ3A.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>It is worth mentioning that vanilla PG suffers from two major limitations:</p> <ol> <li>It is highly sensitive to hyperparameters configuration such as gamma, learning rate, memory size, etc.</li> <li>It is highly prone to overshooting in the policy parameter space, which makes the learning so noisy and fragile. Sometimes the agent might take a step in the parameter space into a very suboptimal area where it would not be able to recover again.</li> </ol> <p>The second issue is addressed in some further variants of the PG algorithm, which I will try to cover in future posts.</p> <p>An illustration of a fully trained agent controlling the Cartpole environment is shown here:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/0*E2-PXki32kc2N3rh.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The full code of this implementation is available at <strong><a href="https://github.com/ebrahimpichka/vanilla-pg" rel="external nofollow noopener" target="_blank">this GitHub repository</a>.</strong></p> <hr> <h2 id="conclusion"><strong>Conclusion</strong></h2> <p>In conclusion, we have explored the Policy Gradient (PG) algorithm, a powerful approach in Reinforcement Learning (RL) that directly learns the optimal policy. Throughout this blog post, we have provided a step-by-step explanation of the PG algorithm and its implementation. We started by understanding the fundamental concept of RL and the difference between value-based and policy-based methods. We then delved into the details of PG, highlighting its objective of maximizing the expected cumulative reward by updating the policy parameters using gradient ascent. We discussed the Vanilla PG algorithm as a common implementation of PG, where we compute the gradient of the log-probability of actions and update the policy using policy gradients.</p> <p>Additionally, we explored the Actor-Critic method, which combines a policy network and a value function to improve convergence. While PG offers advantages in handling continuous action spaces and non-differentiable policies, it may suffer from high variance. Nevertheless, techniques like baselines and variance reduction methods can be employed to address these challenges. By grasping the intricacies of PG, you are now equipped with a valuable tool to tackle RL problems and design intelligent systems that learn and adapt through interactions with their environments.</p> <hr> <h2 id="references">References</h2> <p>[1] — Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning 8:229–256.</p> <p>[2] — Sutton, R. S., McAllester, D. A., Singh, S. P., Mansour, Y., et al. Policy gradient methods for reinforcement learning with function approximation. In Proc. Advances in Neural Information Processing Systems (NIPS), volume 99, pp. 1057–1063. Citeseer, 1999.</p> <p>[3] — course lectures from UC Berkeley: Deep Reinforcement Learning Bootcamp</p> <p>[4] — <a href="http://spinningup.openai.com/en/latest/algorithms/vpg.html" rel="external nofollow noopener" target="_blank">spinningup.openai.com/en/latest/algorithms/vpg.html</a></p> <p>[5] — Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction (second edition), The MIT Press <strong><a href="http://incompleteideas.net/book/RLbook2020.pdf" rel="external nofollow noopener" target="_blank">[PDF]</a></strong></p> <p><br></p> <font color="gray" face="Times New Roman" size="1"> Author: <strong>Ebrahim Pichka</strong> </font> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/autodiff/">Automatic Differentiation (AutoDiff): A Brief Intro with Examples</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/ortools-mathopt/">Solve Optimization Problems on Google Cloud Platform using Google’s OR API and OR-tools MathOpt</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/qkv-transformer/">What are Query, Key, and Value in the Transformer Architecture and Why Are They Used?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/gat-paper-explained/">Graph Attention Networks Paper Explained With Illustration and PyTorch Implementation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/rl-resources/">Resources to Learn Reinforcement Learning</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Ebrahim Pichka. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-JF4RVNMJBY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-JF4RVNMJBY");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>