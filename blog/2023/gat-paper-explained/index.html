<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Graph Attention Networks Paper Explained With Illustration and PyTorch Implementation | Ebrahim Pichka</title> <meta name="author" content="Ebrahim Pichka"> <meta name="description" content="A detailed and illustrated walkthrough of the “Graph Attention Networks” paper by Veličković et al. with the PyTorch implementation of the proposed model."> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon_32.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://epichka.com/blog/2023/gat-paper-explained/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Ebrahim Pichka</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">GitHub Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Graph Attention Networks Paper Explained With Illustration and PyTorch Implementation</h1> <p class="post-meta">July 26, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/gnn"> <i class="fas fa-hashtag fa-sm"></i> GNN</a>   <a href="/blog/tag/gat"> <i class="fas fa-hashtag fa-sm"></i> GAT</a>     ·   <a href="/blog/category/graph-representation-learning"> <i class="fas fa-tag fa-sm"></i> graph-representation-learning</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/4240/1*JeY2ChpCHoH84dyJ-Ugu3Q.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="introduction">Introduction</h2> <p>Graph neural networks (GNNs) are a powerful class of neural networks that operate on graph-structured data. They learn node representations (embeddings) by aggregating information from a node’s local neighborhood. This concept is known as <strong><em>‘message passing’</em></strong> in the graph representation learning literature.</p> <p>Messages (embeddings) are passed between nodes in the graph through multiple layers of the GNN. Each node **aggregates **the messages from its **neighbors **to **update **its representation. This process is repeated across layers, allowing nodes to obtain representations that encode richer information about the graph. Some of the important variants of GNNs can are GraphSAGE [2], Graph Convolution Network [3], etc. You can explore more GNN variants <a href="https://paperswithcode.com/methods/category/graph-models" rel="external nofollow noopener" target="_blank">here</a>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*sJ01stdUwds-YN4-5SYiyQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>Graph Attention Networks (GAT)</strong>[1] are a special class of GNNs that were proposed to improve upon this message-passing scheme. They introduced a learnable <strong>attention mechanism</strong> that enables a node to decide which neighbor nodes are more important when aggregating messages from their local neighborhood by assigning a weight between each source and target node instead of aggregating information from all neighbors with equal weights.</p> <p>Empirically, Graph Attention Networks have been shown to outperform many other GNN models on tasks such as node classification, link prediction, and graph classification. They demonstrated state-of-the-art performance on several benchmark graph datasets.</p> <p>In this post, we will walk through the crucial part of the original “Graph Attention Networks” paper by Veličković et al. [1], explain these parts, and simultaneously implement the notions proposed in the paper using PyTorch framework to better grasp the intuition of the GAT method.</p> <p>You can also access the full code used in this post, containing the training and validation code in <a href="https://github.com/ebrahimpichka/GAT-pt" rel="external nofollow noopener" target="_blank">this GitHub repository</a></p> <h2 id="going-through-the-paper">Going Through the Paper</h2> <h3 id="section-1--introduction">Section 1 — <em>Introduction</em> </h3> <p>After broadly reviewing the existing methods in the graph representation learning literature in Section 1, “<em>Introduction</em>”, the Graph Attention Network (GAT) is introduced. The authors mention:</p> <ol> <li> <p>An overall view of the incorporated attention mechanism.</p> </li> <li> <p>Three properties of GATs, namely efficient computation, general applicability to all nodes, and usability in <strong>inductive learning</strong>.</p> </li> <li> <p>Benchmarks and Datasets on which they evaluated the GAT’s performance.</p> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/3394/1*fsN-_tEJoW-3llxs34xxRQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Then After comparing their approach to some existing methods and mentioning the general similarities and differences between them, they move forward to the next section of the paper.</p> <h3 id="section-2--gat-architecture">Section 2 — GAT Architecture</h3> <p>In this section, which accounts for the main part of the paper, the Graph Attention Network architecture is laid out in detail. To move forward with the explanation, assume the proposed architecture performs on a graph with <strong><em>N nodes (V = {vᵢ}; i=1,…,N)</em></strong> and each node is represented with a <strong>vector hᵢ</strong> of <strong>F elements</strong>, With any arbitrary setting of edges existing between nodes.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*HTBen2imL_rH-j0unv-LpQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The authors first start by characterizing a single <strong>Graph Attention Layer</strong>, and how it operates, which becomes the building blocks of a Graph Attention Network. In general, a single GAT layer is supposed to take a graph with its given node embeddings (representations) as input, propagate information to local neighbor nodes, and output an updated representation of nodes.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*bZu6PYombELP47kEkF5pEg.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>As highlighted above, to do so, first, they state that all the input node feature vectors (<strong><em>hᵢ</em></strong>) to the GA-layer are linearly transformed (i.e. multiplied by <strong>a weight matrix <em>W</em></strong>), in PyTorch, it is generally done as follows:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2288/1*iUfoc0v7-Nf5wMuv25RzfQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># in_features -&gt; F and out_feature -&gt; F'
</span><span class="n">in_features</span> <span class="o">=</span> <span class="bp">...</span>
<span class="n">out_feature</span> <span class="o">=</span> <span class="bp">...</span>

<span class="c1"># instanciate the learnable weight matrix W (FxF')
</span><span class="n">W</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_feature</span><span class="p">)))</span>

<span class="c1">#  Initialize the weight matrix W
</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_normal_</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>

<span class="c1"># multiply W and h (h is input features of all the nodes -&gt; NxF matrix)
</span><span class="n">h_transformed</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mm</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
</code></pre></div></div> <p>Now having in mind that we obtained a transformed version of our input node features (embeddings), we jump a few steps forward to observe and understand what is our final objective in a GAT layer.</p> <p>As described in the paper, at the end of a graph attention layer, <strong>for each node <em>i</em></strong>, we need to obtain a new feature vector that is more structure- and context-aware from its neighborhood.</p> <p>This is done by calculating a <em>*weighted sum **of neighboring node features followed by a non-linear activation function *σ</em>. This weighted sum is also known as the ‘Aggregation Step’ in the general GNN layer operations, according to Graph ML literature.</p> <p>These <strong>weights <em>αᵢⱼ</em></strong> ∈ [0, 1] are <strong>learned **and computed by an attention mechanism that **denotes the importance</strong> of the <strong>neighbor <em>j</em></strong> features for <em>*node *i **</em>during message passing and aggregation.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2204/1*1VOm2GtHtIFHk9N-mc2dEg.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2576/1*nMAaVyiVh_awOiu7iocyUA.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Now let’s see how these attention <strong>weights <em>αᵢⱼ</em></strong> are computed for each pair of node <em>i</em> and its neighbor <em>j</em>:</p> <p>In short, attention <strong>weights <em>αᵢⱼ</em></strong> are calculated as below</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*2tU3NKNVke4ytwVOCocgyQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>where the <strong><em>eᵢⱼ</em></strong> are <strong><em>attention scores</em></strong> and the Softmax function is applied so that all the weights are in the [0, 1] interval and sum to 1.</p> <p>The attention <strong>scores <em>eᵢⱼ</em></strong> are now calculated between <strong>each node <em>i</em></strong> and its <strong>neighbors <em>j</em> ∈ <em>Nᵢ</em></strong> through the attention function <strong><em>a</em>(…)</strong> as such:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/3478/1*0-A9rQ5r7zOZKHxjq0cZGA.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Where <strong>||</strong> denotes the <strong>concatenation</strong> of two transformed node embeddings, and <strong>a</strong> is a vector of <strong>learnable</strong> parameters (i.e., <strong>attention parameters</strong>) of the size <em>2 x F’</em> (twice the size of transformed embeddings).</p> <p>And the <strong>(aᵀ)</strong> is the <strong>transpose</strong> of the vector <strong>a</strong>, resulting in the whole expression <strong>aᵀ [Whᵢ || Whⱼ]</strong> being the <strong>dot (inner) product</strong> between “a” and the concatenation of transformed embeddings.</p> <p>The whole operation is illustrated below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/3640/1*MY09NqbYWf-AmemC5YlmCA.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>In PyTorch, to achieve these scores, we take a slightly different approach. Because it is more efficient to compute <strong><em>eᵢⱼ</em></strong> between <strong>all pairs of nodes</strong> and then select only those which represent existing edges between nodes. To calculate all <strong><em>eᵢⱼ</em></strong>:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># instanciate the learnable attention parameter vector `a`
</span><span class="n">a</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">out_feature</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

<span class="c1"># Initialize the parameter vector `a`
</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_normal_</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="c1"># we obtained `h_transformed` in the previous code snippet
</span>
<span class="c1"># calculating the dot product of all node embeddings
# and first half the attention vector parameters (corresponding to neighbor messages)
</span><span class="n">source_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">h_transformed</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">a</span><span class="p">[:</span><span class="n">out_feature</span><span class="p">,</span> <span class="p">:])</span>

<span class="c1"># calculating the dot product of all node embeddings
# and second half the attention vector parameters (corresponding to target node)
</span><span class="n">target_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">h_transformed</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">a</span><span class="p">[</span><span class="n">out_feature</span><span class="p">:,</span> <span class="p">:])</span>

<span class="c1"># broadcast add 
</span><span class="n">e</span> <span class="o">=</span> <span class="n">source_scores</span> <span class="o">+</span> <span class="n">target_scores</span><span class="p">.</span><span class="n">T</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">leakyrelu</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</code></pre></div></div> <p>The last part of the code snippet (<em># broadcast add</em>) adds all the one-to-one source and target scores, resulting in an <em>N</em>x<em>N</em> matrix containing all the <strong><em>eᵢⱼ</em></strong> scores. (illustrated below)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2348/1*vnNQAJRpMuF9wRZB9bWnig.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>So far, it’s like we assumed the graph is fully-connected and calculated the attention scores between all possible pair of nodes. To address this, after the LeakyReLU activation is applied to the attention scores, the attention scores are masked based on existing edges in the graph, meaning we only keep the scores that correspond to existing edges.</p> <p>It can be done by assigning a <strong>large negative score</strong> (to approximate -∞) to elements in the scores matrix between nodes with non-existing edges so their corresponding attention weights <strong>become zero after softmax</strong>.</p> <p>We can achieve this by using the <strong>adjacency matrix</strong> of the graph. The adjacency matrix is an NxN matrix with 1 at row <em>i</em> and column <em>j</em> if there is an edge between node <em>i</em> and <em>j</em> and 0 elsewhere. So we create the mask by assigning -∞ to zero elements of the adjacency matrix and assigning 0 elsewhere. And then, we add the mask to our **scores **matrix. and apply the softmax function across its rows.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">connectivity_mask</span> <span class="o">=</span> <span class="o">-</span><span class="mf">9e16</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
<span class="c1"># adj_mat is the N by N adjacency matrix
</span><span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">adj_mat</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">connectivity_mask</span><span class="p">)</span> <span class="c1"># masked attention scores
</span>        
<span class="c1"># attention coefficients are computed as a softmax over the rows
# for each column j in the attention score matrix e
</span><span class="n">attention</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p>Finally, according to the paper, after obtaining the attention scores and masking them with the existing edges, we get the attention <em>*weights *αᵢⱼ **</em>by performing softmax over the rows of the scores matrix.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/3012/1*bQhdgq1YCn5ctD-BcJoUPg.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/6924/1*2VuuJFxLxdGKYTAqRf6D1w.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>And as discussed before, we calculate the weighted sum of the node embeddings:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># final node embeddings are computed as a weighted average of the features of its neighbors
</span><span class="n">h_prime</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">h_transformed</span><span class="p">)</span>
</code></pre></div></div> <p>Finally, the paper introduces the notion of **multi-head **attention, where the whole discussed operations are done through multiple parallel streams of operations, where the final result heads are either averaged or concatenated.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/3154/1*myPSu2-HL2ycDkoLYDUEng.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The multi-head attention and aggregation process is illustrated below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*UfgWhR9FVvwesBvL6Tp_ng.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>To wrap up the implementation in a cleaner modular form (as a <strong>PyTorch module</strong>) and to **incorporate the multi-head **attention functionality, the whole Graph Attention Layer implementation is done as follows:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="c1">################################
###  GAT LAYER DEFINITION    ###
################################
</span>
<span class="k">class</span> <span class="nc">GraphAttentionLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">concat</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.4</span><span class="p">,</span>
                    <span class="n">leaky_relu_slope</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">GraphAttentionLayer</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span> <span class="c1"># Number of attention heads
</span>        <span class="n">self</span><span class="p">.</span><span class="n">concat</span> <span class="o">=</span> <span class="n">concat</span> <span class="c1"># wether to concatenate the final attention heads
</span>        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span> <span class="c1"># Dropout rate
</span>
        <span class="k">if</span> <span class="n">concat</span><span class="p">:</span> <span class="c1"># concatenating the attention heads
</span>            <span class="n">self</span><span class="p">.</span><span class="n">out_features</span> <span class="o">=</span> <span class="n">out_features</span> <span class="c1"># Number of output features per node
</span>            <span class="k">assert</span> <span class="n">out_features</span> <span class="o">%</span> <span class="n">n_heads</span> <span class="o">==</span> <span class="mi">0</span> <span class="c1"># Ensure that out_features is a multiple of n_heads
</span>            <span class="n">self</span><span class="p">.</span><span class="n">n_hidden</span> <span class="o">=</span> <span class="n">out_features</span> <span class="o">//</span> <span class="n">n_heads</span>
        <span class="k">else</span><span class="p">:</span> <span class="c1"># averaging output over the attention heads (Used in the main paper)
</span>            <span class="n">self</span><span class="p">.</span><span class="n">n_hidden</span> <span class="o">=</span> <span class="n">out_features</span>

        <span class="c1">#  A shared linear transformation, parametrized by a weight matrix W is applied to every node
</span>        <span class="c1">#  Initialize the weight matrix W 
</span>        <span class="n">self</span><span class="p">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_hidden</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">)))</span>

        <span class="c1"># Initialize the attention weights a
</span>        <span class="n">self</span><span class="p">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_heads</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">n_hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

        <span class="n">self</span><span class="p">.</span><span class="n">leakyrelu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LeakyReLU</span><span class="p">(</span><span class="n">leaky_relu_slope</span><span class="p">)</span> <span class="c1"># LeakyReLU activation function
</span>        <span class="n">self</span><span class="p">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># softmax activation function to the attention coefficients
</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">reset_parameters</span><span class="p">()</span> <span class="c1"># Reset the parameters
</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>

        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_normal_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_normal_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">a</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_attention_scores</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">h_transformed</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
        
        <span class="n">source_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">h_transformed</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">a</span><span class="p">[:,</span> <span class="p">:</span><span class="n">self</span><span class="p">.</span><span class="n">n_hidden</span><span class="p">,</span> <span class="p">:])</span>
        <span class="n">target_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">h_transformed</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">a</span><span class="p">[:,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_hidden</span><span class="p">:,</span> <span class="p">:])</span>

        <span class="c1"># broadcast add 
</span>        <span class="c1"># (n_heads, n_nodes, 1) + (n_heads, 1, n_nodes) = (n_heads, n_nodes, n_nodes)
</span>        <span class="n">e</span> <span class="o">=</span> <span class="n">source_scores</span> <span class="o">+</span> <span class="n">target_scores</span><span class="p">.</span><span class="n">mT</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">leakyrelu</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span>  <span class="n">h</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">adj_mat</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>

        <span class="n">n_nodes</span> <span class="o">=</span> <span class="n">h</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Apply linear transformation to node feature -&gt; W h
</span>        <span class="c1"># output shape (n_nodes, n_hidden * n_heads)
</span>        <span class="n">h_transformed</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mm</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="p">)</span>
        <span class="n">h_transformed</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">h_transformed</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>

        <span class="c1"># splitting the heads by reshaping the tensor and putting heads dim first
</span>        <span class="c1"># output shape (n_heads, n_nodes, n_hidden)
</span>        <span class="n">h_transformed</span> <span class="o">=</span> <span class="n">h_transformed</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">n_nodes</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_hidden</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># getting the attention scores
</span>        <span class="c1"># output shape (n_heads, n_nodes, n_nodes)
</span>        <span class="n">e</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_get_attention_scores</span><span class="p">(</span><span class="n">h_transformed</span><span class="p">)</span>

        <span class="c1"># Set the attention score for non-existent edges to -9e15 (MASKING NON-EXISTENT EDGES)
</span>        <span class="n">connectivity_mask</span> <span class="o">=</span> <span class="o">-</span><span class="mf">9e16</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
        <span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">adj_mat</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">connectivity_mask</span><span class="p">)</span> <span class="c1"># masked attention scores
</span>        
        <span class="c1"># attention coefficients are computed as a softmax over the rows
</span>        <span class="c1"># for each column j in the attention score matrix e
</span>        <span class="n">attention</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>

        <span class="c1"># final node embeddings are computed as a weighted average of the features of its neighbors
</span>        <span class="n">h_prime</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">h_transformed</span><span class="p">)</span>

        <span class="c1"># concatenating/averaging the attention heads
</span>        <span class="c1"># output shape (n_nodes, out_features)
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">concat</span><span class="p">:</span>
            <span class="n">h_prime</span> <span class="o">=</span> <span class="n">h_prime</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="n">n_nodes</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">out_features</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">h_prime</span> <span class="o">=</span> <span class="n">h_prime</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">h_prime</span>
</code></pre></div></div> <p>Next, the authors do a comparison between GATs and some of the other existing GNN methodologies/architectures. They argue that:</p> <ol> <li> <p>GATs are computationally more efficient than some existing methods due to being able to compute attention weights and perform the local aggregation <strong>in parallel</strong>.</p> </li> <li> <p>GATs can assign **different importance **to neighbors of a node while aggregating messages which can enable a leap in model capacity and increase interpretability.</p> </li> <li> <p>GAT does consider the complete neighborhood of nodes (does not require sampling from neighbors) and it does not assume any ordering within nodes.</p> </li> <li> <p>GAT can be reformulated as a particular instance of MoNet (Monti et al., 2016) by setting the pseudo-coordinate function to be u(<em>x, y</em>) = f(<em>x</em>)<strong>||</strong>f(<em>y</em>), where f(<em>x</em>) represents (potentially MLP-transformed) features of node <em>x</em> and <strong>||</strong> is concatenation; and the weight function to be wj(<em>u</em>) = softmax(MLP(<em>u</em>))</p> </li> </ol> <h3 id="section-3--evaluation">Section 3 — Evaluation</h3> <p>In the third section of the paper, first, the authors describe the benchmarks, datasets, and tasks on which the GAT is evaluated. Then they present the results of their evaluation of the model.</p> <h4 id="transductive-learning-vs-inductive-learning">Transductive learning vs. Inductive learning</h4> <p>Datasets used as benchmarks in this paper are differentiated into two types of tasks: <strong>Transductive and Inductive.</strong></p> <ul> <li> <p><strong>Inductive learning:</strong> It is a type of supervised learning task in which a model is trained only on a set of labeled training examples and the trained model is evaluated and tested on examples that were completely unobserved during training. It is the type of learning which is known as common supervised learning.</p> </li> <li> <p><strong>Transductive learning:</strong> In this type of task, all the data, including training, validation, and test instances, are used during training. But in each phase, only the corresponding set of labels is accessed by the model. Meaning during training, the model is only trained using the <strong>loss</strong> that is resulted from the training instances and labels, but the test and validation features are used for the message passing. It is mostly because of the structural and contextual information existing in the examples.</p> </li> </ul> <h4 id="datasets">Datasets</h4> <p>In the paper, four benchmark datasets are used to evaluate GATs, three of which correspond to transductive learning, and one other is used as an inductive learning task.</p> <p>The transductive learning datasets, namely <strong>Cora</strong>, <strong>Citeseer</strong>, and <strong>Pubmed</strong> (Sen et al., 2008) datasets are all <strong>citation graphs</strong> in which nodes are published documents and edges (connections) are citations among them, and the node features are elements of a bag-of-words representation of a document. The inductive learning dataset is a <strong>protein-protein interaction (PPI)</strong> dataset containing graphs are different <strong>human tissues</strong> (Zitnik &amp; Leskovec, 2017). Datasets are described more below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/3124/1*Hqw73vD_hs8jF3Lil1JvvA.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="setup--results">Setup &amp; Results</h4> <p>For the three transductive tasks, the setting used for training is as follows:</p> <p>They use 2 GAT layers</p> <ul> <li>first layer uses: <ul> <li> <strong>K = 8</strong> attention heads</li> <li> <strong>F’ = 8</strong> output feature dim per head</li> <li> <strong>ELU</strong> activation</li> </ul> </li> <li>and for the second layer [<strong>Cora &amp; Citeseer | Pubmed</strong>] <ul> <li> <strong>[1 | 8] attention head</strong> with <strong>C number of classes</strong> output dim</li> <li> <strong>Softmax</strong> activation for classification probability output</li> </ul> </li> <li>and for the overall network <ul> <li> <strong>Dropout</strong> with <strong>p = 0.6</strong> </li> <li> <strong>L2</strong> regularization with <strong>λ = [0.0005 | 0.001]</strong> </li> </ul> </li> </ul> <p>For the three transductive tasks, the setting used for training is:</p> <ul> <li>Three layers — <ul> <li>Layer 1 &amp; 2: <strong>K = 4</strong> | <strong>F’ = 256</strong> | <strong>ELU</strong> </li> <li>Layer 3: <strong>K = 6</strong> | <strong>F’ = C classes</strong> | <strong>Sigmoid (multi-label)</strong> </li> <li>with <strong>no regularization and dropout</strong> </li> </ul> </li> </ul> <p>The first setting’s implementation in PyTorch is done below using the layer we defined earlier:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GAT</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span>
        <span class="n">in_features</span><span class="p">,</span>
        <span class="n">n_hidden</span><span class="p">,</span>
        <span class="n">n_heads</span><span class="p">,</span>
        <span class="n">num_classes</span><span class="p">,</span>
        <span class="n">concat</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span>
        <span class="n">leaky_relu_slope</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>

        <span class="nf">super</span><span class="p">(</span><span class="n">GAT</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="c1"># Define the Graph Attention layers
</span>        <span class="n">self</span><span class="p">.</span><span class="n">gat1</span> <span class="o">=</span> <span class="nc">GraphAttentionLayer</span><span class="p">(</span>
            <span class="n">in_features</span><span class="o">=</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="n">n_heads</span><span class="p">,</span>
            <span class="n">concat</span><span class="o">=</span><span class="n">concat</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">leaky_relu_slope</span><span class="o">=</span><span class="n">leaky_relu_slope</span>
            <span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">gat2</span> <span class="o">=</span> <span class="nc">GraphAttentionLayer</span><span class="p">(</span>
            <span class="n">in_features</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">concat</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">leaky_relu_slope</span><span class="o">=</span><span class="n">leaky_relu_slope</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span> <span class="p">,</span> <span class="n">adj_mat</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>


        <span class="c1"># Apply the first Graph Attention layer
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">gat1</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">adj_mat</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">elu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># Apply ELU activation function to the output of the first layer
</span>
        <span class="c1"># Apply the second Graph Attention layer
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">gat2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">adj_mat</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Apply softmax activation function
</span></code></pre></div></div> <p>After testing, the authors report the following performance for the four benchmarks showing the comparable results of GATs compared to existing GNN methods.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2814/1*1Z8MPnC6oTL4vEhTodnUng.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2974/1*ZoSLk01leYeh6TlGWTbjOg.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="conclusion">Conclusion</h2> <p>To conclude, in this blog post, I tried to take a detailed and easy-to-follow approach in explaining the “Graph Attention Networks” paper by Veličković et al. by using illustrations to help readers understand the main ideas behind these networks and why they are important for working with complex graph-structured data (e.g., social networks or molecules). Additionally, the post includes a practical implementation of the model using PyTorch, a popular programming framework. By going through the blog post and trying out the code, I hope readers can gain a solid understanding of how GATs work and how they can be applied in real-world scenarios. I hope this post has been helpful and encouraging to explore this exciting area of research further.</p> <p>Plus, you can access the full code used in this post, containing the training and validation code in <a href="https://github.com/ebrahimpichka/GAT-pt" rel="external nofollow noopener" target="_blank">this GitHub repository</a>.</p> <p>I’d be happy to hear any thoughts or any suggestions/changes on the post.</p> <h2 id="references">References</h2> <p>[1] — <strong>Graph Attention Networks (2017)</strong>, <em>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio</em>. <a href="https://arxiv.org/abs/1710.10903v3" rel="external nofollow noopener" target="_blank">arXiv:1710.10903v3</a></p> <p>[2] — <strong>Inductive Representation Learning on Large Graphs</strong> <strong>(2017)</strong>, <em>William L. Hamilton, Rex Ying, Jure Leskovec</em>. <a href="https://arxiv.org/abs/1706.02216v4" rel="external nofollow noopener" target="_blank">arXiv:1706.02216v4</a></p> <p>[3] — <strong>Semi-Supervised Classification with Graph Convolutional Networks (2016),</strong> <em>Thomas N. Kipf, Max Welling</em>. <a href="https://arxiv.org/abs/1609.02907v4" rel="external nofollow noopener" target="_blank">arXiv:1609.02907v4</a></p> <p><br></p> <font color="gray" face="Times New Roman" size="1"> Author: <strong>Ebrahim Pichka</strong> </font> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/qkv-transformer/">What are Query, Key, and Value in the Transformer Architecture and Why Are They Used?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/pg-math-explained/">Policy Gradient Algorithm’s Mathematics Explained with PyTorch Implementation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/rl-resources/">Resources to Learn Reinforcement Learning</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Ebrahim Pichka. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-JF4RVNMJBY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-JF4RVNMJBY");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>