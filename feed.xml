<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://epichka.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://epichka.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-07-28T04:14:03+00:00</updated><id>https://epichka.com/feed.xml</id><title type="html">blank</title><subtitle>Ebrahim Pichka&apos;s personal webiste. </subtitle><entry><title type="html">Graph Attention Networks Paper Explained With Illustration and PyTorch Implementation</title><link href="https://epichka.com/blog/2023/gat-paper-explained/" rel="alternate" type="text/html" title="Graph Attention Networks Paper Explained With Illustration and PyTorch Implementation"/><published>2023-07-26T14:57:00+00:00</published><updated>2023-07-26T14:57:00+00:00</updated><id>https://epichka.com/blog/2023/gat-paper-explained</id><content type="html" xml:base="https://epichka.com/blog/2023/gat-paper-explained/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/4240/1*JeY2ChpCHoH84dyJ-Ugu3Q.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="introduction">Introduction</h2> <p>Graph neural networks (GNNs) are a powerful class of neural networks that operate on graph-structured data. They learn node representations (embeddings) by aggregating information from a node’s local neighborhood. This concept is known as <strong><em>‘message passing’</em></strong> in the graph representation learning literature.</p> <p>Messages (embeddings) are passed between nodes in the graph through multiple layers of the GNN. Each node **aggregates **the messages from its **neighbors **to **update **its representation. This process is repeated across layers, allowing nodes to obtain representations that encode richer information about the graph. Some of the important variants of GNNs can are GraphSAGE [2], Graph Convolution Network [3], etc. You can explore more GNN variants <a href="https://paperswithcode.com/methods/category/graph-models">here</a>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*sJ01stdUwds-YN4-5SYiyQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Graph Attention Networks (GAT)</strong>[1] are a special class of GNNs that were proposed to improve upon this message-passing scheme. They introduced a learnable <strong>attention mechanism</strong> that enables a node to decide which neighbor nodes are more important when aggregating messages from their local neighborhood by assigning a weight between each source and target node instead of aggregating information from all neighbors with equal weights.</p> <p>Empirically, Graph Attention Networks have been shown to outperform many other GNN models on tasks such as node classification, link prediction, and graph classification. They demonstrated state-of-the-art performance on several benchmark graph datasets.</p> <p>In this post, we will walk through the crucial part of the original “Graph Attention Networks” paper by Veličković et al. [1], explain these parts, and simultaneously implement the notions proposed in the paper using PyTorch framework to better grasp the intuition of the GAT method.</p> <p>You can also access the full code used in this post, containing the training and validation code in <a href="https://github.com/ebrahimpichka/GAT-pt">this GitHub repository</a></p> <h2 id="going-through-the-paper">Going Through the Paper</h2> <h3 id="section-1--introduction">Section 1 — <em>Introduction</em></h3> <p>After broadly reviewing the existing methods in the graph representation learning literature in Section 1, “<em>Introduction</em>”, the Graph Attention Network (GAT) is introduced. The authors mention:</p> <ol> <li> <p>An overall view of the incorporated attention mechanism.</p> </li> <li> <p>Three properties of GATs, namely efficient computation, general applicability to all nodes, and usability in <strong>inductive learning</strong>.</p> </li> <li> <p>Benchmarks and Datasets on which they evaluated the GAT’s performance.</p> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/3394/1*fsN-_tEJoW-3llxs34xxRQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Then After comparing their approach to some existing methods and mentioning the general similarities and differences between them, they move forward to the next section of the paper.</p> <h3 id="section-2--gat-architecture">Section 2 — GAT Architecture</h3> <p>In this section, which accounts for the main part of the paper, the Graph Attention Network architecture is laid out in detail. To move forward with the explanation, assume the proposed architecture performs on a graph with <strong><em>N nodes (V = {vᵢ}; i=1,…,N)</em></strong> and each node is represented with a <strong>vector hᵢ</strong> of <strong>F elements</strong>, With any arbitrary setting of edges existing between nodes.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*HTBen2imL_rH-j0unv-LpQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The authors first start by characterizing a single <strong>Graph Attention Layer</strong>, and how it operates, which becomes the building blocks of a Graph Attention Network. In general, a single GAT layer is supposed to take a graph with its given node embeddings (representations) as input, propagate information to local neighbor nodes, and output an updated representation of nodes.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*bZu6PYombELP47kEkF5pEg.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>As highlighted above, to do so, first, they state that all the input node feature vectors (<strong><em>hᵢ</em></strong>) to the GA-layer are linearly transformed (i.e. multiplied by <strong>a weight matrix <em>W</em></strong>), in PyTorch, it is generally done as follows:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2288/1*iUfoc0v7-Nf5wMuv25RzfQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># in_features -&gt; F and out_feature -&gt; F'
</span><span class="n">in_features</span> <span class="o">=</span> <span class="bp">...</span>
<span class="n">out_feature</span> <span class="o">=</span> <span class="bp">...</span>

<span class="c1"># instanciate the learnable weight matrix W (FxF')
</span><span class="n">W</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_feature</span><span class="p">)))</span>

<span class="c1">#  Initialize the weight matrix W
</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_normal_</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>

<span class="c1"># multiply W and h (h is input features of all the nodes -&gt; NxF matrix)
</span><span class="n">h_transformed</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mm</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
</code></pre></div></div> <p>Now having in mind that we obtained a transformed version of our input node features (embeddings), we jump a few steps forward to observe and understand what is our final objective in a GAT layer.</p> <p>As described in the paper, at the end of a graph attention layer, <strong>for each node <em>i</em></strong>, we need to obtain a new feature vector that is more structure- and context-aware from its neighborhood.</p> <p>This is done by calculating a <em>*weighted sum **of neighboring node features followed by a non-linear activation function *σ</em>. This weighted sum is also known as the ‘Aggregation Step’ in the general GNN layer operations, according to Graph ML literature.</p> <p>These <strong>weights <em>αᵢⱼ</em></strong> ∈ [0, 1] are <strong>learned **and computed by an attention mechanism that **denotes the importance</strong> of the <strong>neighbor <em>j</em></strong> features for <em>*node *i **</em>during message passing and aggregation.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2204/1*1VOm2GtHtIFHk9N-mc2dEg.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2576/1*nMAaVyiVh_awOiu7iocyUA.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Now let’s see how these attention <strong>weights <em>αᵢⱼ</em></strong> are computed for each pair of node <em>i</em> and its neighbor <em>j</em>:</p> <p>In short, attention <strong>weights <em>αᵢⱼ</em></strong> are calculated as below</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*2tU3NKNVke4ytwVOCocgyQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>where the <strong><em>eᵢⱼ</em></strong> are <strong><em>attention scores</em></strong> and the Softmax function is applied so that all the weights are in the [0, 1] interval and sum to 1.</p> <p>The attention <strong>scores <em>eᵢⱼ</em></strong> are now calculated between <strong>each node <em>i</em></strong> and its <strong>neighbors <em>j</em> ∈ <em>Nᵢ</em></strong> through the attention function <strong><em>a</em>(…)</strong> as such:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/3478/1*0-A9rQ5r7zOZKHxjq0cZGA.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Where <strong>||</strong> denotes the <strong>concatenation</strong> of two transformed node embeddings, and <strong>a</strong> is a vector of <strong>learnable</strong> parameters (i.e., <strong>attention parameters</strong>) of the size <em>2 x F’</em> (twice the size of transformed embeddings).</p> <p>And the <strong>(aᵀ)</strong> is the <strong>transpose</strong> of the vector <strong>a</strong>, resulting in the whole expression <strong>aᵀ [Whᵢ || Whⱼ]</strong> being the <strong>dot (inner) product</strong> between “a” and the concatenation of transformed embeddings.</p> <p>The whole operation is illustrated below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/3640/1*MY09NqbYWf-AmemC5YlmCA.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In PyTorch, to achieve these scores, we take a slightly different approach. Because it is more efficient to compute <strong><em>eᵢⱼ</em></strong> between <strong>all pairs of nodes</strong> and then select only those which represent existing edges between nodes. To calculate all <strong><em>eᵢⱼ</em></strong>:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># instanciate the learnable attention parameter vector `a`
</span><span class="n">a</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">out_feature</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

<span class="c1"># Initialize the parameter vector `a`
</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_normal_</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="c1"># we obtained `h_transformed` in the previous code snippet
</span>
<span class="c1"># calculating the dot product of all node embeddings
# and first half the attention vector parameters (corresponding to neighbor messages)
</span><span class="n">source_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">h_transformed</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">a</span><span class="p">[:</span><span class="n">out_feature</span><span class="p">,</span> <span class="p">:])</span>

<span class="c1"># calculating the dot product of all node embeddings
# and second half the attention vector parameters (corresponding to target node)
</span><span class="n">target_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">h_transformed</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">a</span><span class="p">[</span><span class="n">out_feature</span><span class="p">:,</span> <span class="p">:])</span>

<span class="c1"># broadcast add 
</span><span class="n">e</span> <span class="o">=</span> <span class="n">source_scores</span> <span class="o">+</span> <span class="n">target_scores</span><span class="p">.</span><span class="n">T</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">leakyrelu</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</code></pre></div></div> <p>The last part of the code snippet (<em># broadcast add</em>) adds all the one-to-one source and target scores, resulting in an <em>N</em>x<em>N</em> matrix containing all the <strong><em>eᵢⱼ</em></strong> scores. (illustrated below)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2348/1*vnNQAJRpMuF9wRZB9bWnig.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>So far, it’s like we assumed the graph is fully-connected and calculated the attention scores between all possible pair of nodes. To address this, after the LeakyReLU activation is applied to the attention scores, the attention scores are masked based on existing edges in the graph, meaning we only keep the scores that correspond to existing edges.</p> <p>It can be done by assigning a <strong>large negative score</strong> (to approximate -∞) to elements in the scores matrix between nodes with non-existing edges so their corresponding attention weights <strong>become zero after softmax</strong>.</p> <p>We can achieve this by using the <strong>adjacency matrix</strong> of the graph. The adjacency matrix is an NxN matrix with 1 at row <em>i</em> and column <em>j</em> if there is an edge between node <em>i</em> and <em>j</em> and 0 elsewhere. So we create the mask by assigning -∞ to zero elements of the adjacency matrix and assigning 0 elsewhere. And then, we add the mask to our **scores **matrix. and apply the softmax function across its rows.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">connectivity_mask</span> <span class="o">=</span> <span class="o">-</span><span class="mf">9e16</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
<span class="c1"># adj_mat is the N by N adjacency matrix
</span><span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">adj_mat</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">connectivity_mask</span><span class="p">)</span> <span class="c1"># masked attention scores
</span>        
<span class="c1"># attention coefficients are computed as a softmax over the rows
# for each column j in the attention score matrix e
</span><span class="n">attention</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p>Finally, according to the paper, after obtaining the attention scores and masking them with the existing edges, we get the attention <em>*weights *αᵢⱼ **</em>by performing softmax over the rows of the scores matrix.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/3012/1*bQhdgq1YCn5ctD-BcJoUPg.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/6924/1*2VuuJFxLxdGKYTAqRf6D1w.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>And as discussed before, we calculate the weighted sum of the node embeddings:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># final node embeddings are computed as a weighted average of the features of its neighbors
</span><span class="n">h_prime</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">h_transformed</span><span class="p">)</span>
</code></pre></div></div> <p>Finally, the paper introduces the notion of **multi-head **attention, where the whole discussed operations are done through multiple parallel streams of operations, where the final result heads are either averaged or concatenated.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/3154/1*myPSu2-HL2ycDkoLYDUEng.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The multi-head attention and aggregation process is illustrated below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*UfgWhR9FVvwesBvL6Tp_ng.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>To wrap up the implementation in a cleaner modular form (as a <strong>PyTorch module</strong>) and to **incorporate the multi-head **attention functionality, the whole Graph Attention Layer implementation is done as follows:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="c1">################################
###  GAT LAYER DEFINITION    ###
################################
</span>
<span class="k">class</span> <span class="nc">GraphAttentionLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">concat</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.4</span><span class="p">,</span>
                    <span class="n">leaky_relu_slope</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">GraphAttentionLayer</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span> <span class="c1"># Number of attention heads
</span>        <span class="n">self</span><span class="p">.</span><span class="n">concat</span> <span class="o">=</span> <span class="n">concat</span> <span class="c1"># wether to concatenate the final attention heads
</span>        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span> <span class="c1"># Dropout rate
</span>
        <span class="k">if</span> <span class="n">concat</span><span class="p">:</span> <span class="c1"># concatenating the attention heads
</span>            <span class="n">self</span><span class="p">.</span><span class="n">out_features</span> <span class="o">=</span> <span class="n">out_features</span> <span class="c1"># Number of output features per node
</span>            <span class="k">assert</span> <span class="n">out_features</span> <span class="o">%</span> <span class="n">n_heads</span> <span class="o">==</span> <span class="mi">0</span> <span class="c1"># Ensure that out_features is a multiple of n_heads
</span>            <span class="n">self</span><span class="p">.</span><span class="n">n_hidden</span> <span class="o">=</span> <span class="n">out_features</span> <span class="o">//</span> <span class="n">n_heads</span>
        <span class="k">else</span><span class="p">:</span> <span class="c1"># averaging output over the attention heads (Used in the main paper)
</span>            <span class="n">self</span><span class="p">.</span><span class="n">n_hidden</span> <span class="o">=</span> <span class="n">out_features</span>

        <span class="c1">#  A shared linear transformation, parametrized by a weight matrix W is applied to every node
</span>        <span class="c1">#  Initialize the weight matrix W 
</span>        <span class="n">self</span><span class="p">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_hidden</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">)))</span>

        <span class="c1"># Initialize the attention weights a
</span>        <span class="n">self</span><span class="p">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_heads</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">n_hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

        <span class="n">self</span><span class="p">.</span><span class="n">leakyrelu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LeakyReLU</span><span class="p">(</span><span class="n">leaky_relu_slope</span><span class="p">)</span> <span class="c1"># LeakyReLU activation function
</span>        <span class="n">self</span><span class="p">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># softmax activation function to the attention coefficients
</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">reset_parameters</span><span class="p">()</span> <span class="c1"># Reset the parameters
</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>

        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_normal_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_normal_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">a</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_attention_scores</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">h_transformed</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
        
        <span class="n">source_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">h_transformed</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">a</span><span class="p">[:,</span> <span class="p">:</span><span class="n">self</span><span class="p">.</span><span class="n">n_hidden</span><span class="p">,</span> <span class="p">:])</span>
        <span class="n">target_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">h_transformed</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">a</span><span class="p">[:,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_hidden</span><span class="p">:,</span> <span class="p">:])</span>

        <span class="c1"># broadcast add 
</span>        <span class="c1"># (n_heads, n_nodes, 1) + (n_heads, 1, n_nodes) = (n_heads, n_nodes, n_nodes)
</span>        <span class="n">e</span> <span class="o">=</span> <span class="n">source_scores</span> <span class="o">+</span> <span class="n">target_scores</span><span class="p">.</span><span class="n">mT</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">leakyrelu</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span>  <span class="n">h</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">adj_mat</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>

        <span class="n">n_nodes</span> <span class="o">=</span> <span class="n">h</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Apply linear transformation to node feature -&gt; W h
</span>        <span class="c1"># output shape (n_nodes, n_hidden * n_heads)
</span>        <span class="n">h_transformed</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mm</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="p">)</span>
        <span class="n">h_transformed</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">h_transformed</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>

        <span class="c1"># splitting the heads by reshaping the tensor and putting heads dim first
</span>        <span class="c1"># output shape (n_heads, n_nodes, n_hidden)
</span>        <span class="n">h_transformed</span> <span class="o">=</span> <span class="n">h_transformed</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">n_nodes</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_hidden</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># getting the attention scores
</span>        <span class="c1"># output shape (n_heads, n_nodes, n_nodes)
</span>        <span class="n">e</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_get_attention_scores</span><span class="p">(</span><span class="n">h_transformed</span><span class="p">)</span>

        <span class="c1"># Set the attention score for non-existent edges to -9e15 (MASKING NON-EXISTENT EDGES)
</span>        <span class="n">connectivity_mask</span> <span class="o">=</span> <span class="o">-</span><span class="mf">9e16</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
        <span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">adj_mat</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">connectivity_mask</span><span class="p">)</span> <span class="c1"># masked attention scores
</span>        
        <span class="c1"># attention coefficients are computed as a softmax over the rows
</span>        <span class="c1"># for each column j in the attention score matrix e
</span>        <span class="n">attention</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>

        <span class="c1"># final node embeddings are computed as a weighted average of the features of its neighbors
</span>        <span class="n">h_prime</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">h_transformed</span><span class="p">)</span>

        <span class="c1"># concatenating/averaging the attention heads
</span>        <span class="c1"># output shape (n_nodes, out_features)
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">concat</span><span class="p">:</span>
            <span class="n">h_prime</span> <span class="o">=</span> <span class="n">h_prime</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="n">n_nodes</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">out_features</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">h_prime</span> <span class="o">=</span> <span class="n">h_prime</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">h_prime</span>
</code></pre></div></div> <p>Next, the authors do a comparison between GATs and some of the other existing GNN methodologies/architectures. They argue that:</p> <ol> <li> <p>GATs are computationally more efficient than some existing methods due to being able to compute attention weights and perform the local aggregation <strong>in parallel</strong>.</p> </li> <li> <p>GATs can assign **different importance **to neighbors of a node while aggregating messages which can enable a leap in model capacity and increase interpretability.</p> </li> <li> <p>GAT does consider the complete neighborhood of nodes (does not require sampling from neighbors) and it does not assume any ordering within nodes.</p> </li> <li> <p>GAT can be reformulated as a particular instance of MoNet (Monti et al., 2016) by setting the pseudo-coordinate function to be u(<em>x, y</em>) = f(<em>x</em>)<strong>||</strong>f(<em>y</em>), where f(<em>x</em>) represents (potentially MLP-transformed) features of node <em>x</em> and <strong>||</strong> is concatenation; and the weight function to be wj(<em>u</em>) = softmax(MLP(<em>u</em>))</p> </li> </ol> <h3 id="section-3--evaluation">Section 3 — Evaluation</h3> <p>In the third section of the paper, first, the authors describe the benchmarks, datasets, and tasks on which the GAT is evaluated. Then they present the results of their evaluation of the model.</p> <h4 id="transductive-learning-vs-inductive-learning">Transductive learning vs. Inductive learning</h4> <p>Datasets used as benchmarks in this paper are differentiated into two types of tasks: <strong>Transductive and Inductive.</strong></p> <ul> <li> <p><strong>Inductive learning:</strong> It is a type of supervised learning task in which a model is trained only on a set of labeled training examples and the trained model is evaluated and tested on examples that were completely unobserved during training. It is the type of learning which is known as common supervised learning.</p> </li> <li> <p><strong>Transductive learning:</strong> In this type of task, all the data, including training, validation, and test instances, are used during training. But in each phase, only the corresponding set of labels is accessed by the model. Meaning during training, the model is only trained using the <strong>loss</strong> that is resulted from the training instances and labels, but the test and validation features are used for the message passing. It is mostly because of the structural and contextual information existing in the examples.</p> </li> </ul> <h4 id="datasets">Datasets</h4> <p>In the paper, four benchmark datasets are used to evaluate GATs, three of which correspond to transductive learning, and one other is used as an inductive learning task.</p> <p>The transductive learning datasets, namely <strong>Cora</strong>, <strong>Citeseer</strong>, and <strong>Pubmed</strong> (Sen et al., 2008) datasets are all <strong>citation graphs</strong> in which nodes are published documents and edges (connections) are citations among them, and the node features are elements of a bag-of-words representation of a document. The inductive learning dataset is a <strong>protein-protein interaction (PPI)</strong> dataset containing graphs are different <strong>human tissues</strong> (Zitnik &amp; Leskovec, 2017). Datasets are described more below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/3124/1*Hqw73vD_hs8jF3Lil1JvvA.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="setup--results">Setup &amp; Results</h4> <p>For the three transductive tasks, the setting used for training is as follows:</p> <p>They use 2 GAT layers</p> <ul> <li>first layer uses: <ul> <li><strong>K = 8</strong> attention heads</li> <li><strong>F’ = 8</strong> output feature dim per head</li> <li><strong>ELU</strong> activation</li> </ul> </li> <li>and for the second layer [<strong>Cora &amp; Citeseer | Pubmed</strong>] <ul> <li><strong>[1 | 8] attention head</strong> with <strong>C number of classes</strong> output dim</li> <li><strong>Softmax</strong> activation for classification probability output</li> </ul> </li> <li>and for the overall network <ul> <li><strong>Dropout</strong> with <strong>p = 0.6</strong></li> <li><strong>L2</strong> regularization with <strong>λ = [0.0005 | 0.001]</strong></li> </ul> </li> </ul> <p>For the three transductive tasks, the setting used for training is:</p> <ul> <li>Three layers — <ul> <li>Layer 1 &amp; 2: <strong>K = 4</strong> | <strong>F’ = 256</strong> | <strong>ELU</strong></li> <li>Layer 3: <strong>K = 6</strong> | <strong>F’ = C classes</strong> | <strong>Sigmoid (multi-label)</strong></li> <li>with <strong>no regularization and dropout</strong></li> </ul> </li> </ul> <p>The first setting’s implementation in PyTorch is done below using the layer we defined earlier:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GAT</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span>
        <span class="n">in_features</span><span class="p">,</span>
        <span class="n">n_hidden</span><span class="p">,</span>
        <span class="n">n_heads</span><span class="p">,</span>
        <span class="n">num_classes</span><span class="p">,</span>
        <span class="n">concat</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span>
        <span class="n">leaky_relu_slope</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>

        <span class="nf">super</span><span class="p">(</span><span class="n">GAT</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="c1"># Define the Graph Attention layers
</span>        <span class="n">self</span><span class="p">.</span><span class="n">gat1</span> <span class="o">=</span> <span class="nc">GraphAttentionLayer</span><span class="p">(</span>
            <span class="n">in_features</span><span class="o">=</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="n">n_heads</span><span class="p">,</span>
            <span class="n">concat</span><span class="o">=</span><span class="n">concat</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">leaky_relu_slope</span><span class="o">=</span><span class="n">leaky_relu_slope</span>
            <span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">gat2</span> <span class="o">=</span> <span class="nc">GraphAttentionLayer</span><span class="p">(</span>
            <span class="n">in_features</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">concat</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">leaky_relu_slope</span><span class="o">=</span><span class="n">leaky_relu_slope</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span> <span class="p">,</span> <span class="n">adj_mat</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>


        <span class="c1"># Apply the first Graph Attention layer
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">gat1</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">adj_mat</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">elu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># Apply ELU activation function to the output of the first layer
</span>
        <span class="c1"># Apply the second Graph Attention layer
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">gat2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">adj_mat</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Apply softmax activation function
</span></code></pre></div></div> <p>After testing, the authors report the following performance for the four benchmarks showing the comparable results of GATs compared to existing GNN methods.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2814/1*1Z8MPnC6oTL4vEhTodnUng.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2974/1*ZoSLk01leYeh6TlGWTbjOg.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="conclusion">Conclusion</h2> <p>To conclude, in this blog post, I tried to take a detailed and easy-to-follow approach in explaining the “Graph Attention Networks” paper by Veličković et al. by using illustrations to help readers understand the main ideas behind these networks and why they are important for working with complex graph-structured data (e.g., social networks or molecules). Additionally, the post includes a practical implementation of the model using PyTorch, a popular programming framework. By going through the blog post and trying out the code, I hope readers can gain a solid understanding of how GATs work and how they can be applied in real-world scenarios. I hope this post has been helpful and encouraging to explore this exciting area of research further.</p> <p>Plus, you can access the full code used in this post, containing the training and validation code in <a href="https://github.com/ebrahimpichka/GAT-pt">this GitHub repository</a>.</p> <p>I’d be happy to hear any thoughts or any suggestions/changes on the post.</p> <h2 id="references">References</h2> <p>[1] — <strong>Graph Attention Networks (2017)</strong>, <em>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio</em>. <a href="https://arxiv.org/abs/1710.10903v3">arXiv:1710.10903v3</a></p> <p>[2] — <strong>Inductive Representation Learning on Large Graphs</strong> <strong>(2017)</strong>, <em>William L. Hamilton, Rex Ying, Jure Leskovec</em>. <a href="https://arxiv.org/abs/1706.02216v4">arXiv:1706.02216v4</a></p> <p>[3] — <strong>Semi-Supervised Classification with Graph Convolutional Networks (2016),</strong> <em>Thomas N. Kipf, Max Welling</em>. <a href="https://arxiv.org/abs/1609.02907v4">arXiv:1609.02907v4</a></p>]]></content><author><name></name></author><category term="graph-representation-learning"/><category term="GNN"/><category term="GAT"/><summary type="html"><![CDATA[A detailed and illustrated walkthrough of the “Graph Attention Networks” paper by Veličković et al. with the PyTorch implementation of the proposed model.]]></summary></entry><entry><title type="html">Policy Gradient Algorithm’s Mathematics Explained with PyTorch Implementation</title><link href="https://epichka.com/blog/2023/pg-math-explained/" rel="alternate" type="text/html" title="Policy Gradient Algorithm’s Mathematics Explained with PyTorch Implementation"/><published>2023-05-23T12:00:00+00:00</published><updated>2023-05-23T12:00:00+00:00</updated><id>https://epichka.com/blog/2023/pg-math-explained</id><content type="html" xml:base="https://epichka.com/blog/2023/pg-math-explained/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2048/0*VgbxFJr_l6SmL1H-.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Table of Content</strong></p> <ul> <li><strong><a href="#introduction">Introduction</a></strong></li> <li><strong><a href="#policy-gradient-method">Policy Gradient Method</a></strong> <ul> <li><a href="#derivation">Derivation</a></li> <li><a href="#optimization">Optimization</a></li> <li><a href="#the-algorithm">The Algorithm</a></li> </ul> </li> <li><strong><a href="#pyTorch-implementation">PyTorch Implementation</a></strong> <ul> <li><a href="#networks">Networks</a></li> <li><a href="#training-loop-main-algorithm">Training Loop (Main algorithm)</a></li> <li><a href="#training-Results">Training Results</a></li> </ul> </li> <li><strong><a href="#conclusion">Conclusion</a></strong></li> <li><strong><a href="#references">References</a></strong></li> </ul> <hr/> <h2 id="introduction">Introduction</h2> <p>Reinforcement Learning (RL) is a subdomain of AI that aims to enable machines to learn and improve their behavior by interacting with an environment and receiving feedback in the form of reward signals. This environment is mathematically formulated as a Markov Decision Process (MDP) where at each timestep, the agent is known to be at a certain state <strong>(s ∈ <em>S</em>)</strong> where it is able to take action <strong>(a ∈ <em>A</em>)</strong>. This action results in a transition from state <strong>s</strong> to a new state <strong>(s’ ∈ <em>S)</em></strong> with a certain probability from a dynamics function <strong><em>P(s, a, s’)</em></strong> and receiving a scalar reward <strong>r</strong> from a reward function <strong><em>R(s, a, s’)</em></strong>. That said, MDPs can be shown by a tuple of sets <strong>(<em>S, A, P, R, γ</em>)</strong> in which γ ∈ (0, 1] is a discount factor for future steps’ rewards.</p> <p>RL algorithms can be generally categorized into two groups i.e., value-based and policy-based methods. Value-based methods aim at estimating the expected return of the states and selecting an action in that state which results in the highest expected value, which is rather an indirect way of behaving optimally in an MDP environment. In contrast, policy-based methods try to learn and optimize a policy function, which is basically a mapping from states to actions. The Policy Gradient (PG) method <strong>[1][2]</strong> is a popular policy-based approach in RL, which **directly **optimizes the policy function by changing its parameters using gradient ascent.</p> <p>PG has some advantages over value-based methods, especially when dealing with environments with continuous action spaces or high stochasticity. PG can also handle non-differentiable policies, making it suitable for complex scenarios. However, PG can suffer from some issues such as high variance in the gradient estimates, which can lead to slower convergence and/or instability.</p> <p>In this post, we will lay out the Policy Gradient method in detail, while examining its strengths and limitations. We will discuss the intuition behind PG, how it works, and also provide a code implementation of the algorithm. I will try to cover various modifications and extensions that have been proposed to improve its performance in further posts.</p> <hr/> <h2 id="policy-gradient-method">Policy Gradient Method</h2> <p>As explained above, Policy Gradient (PG) methods are algorithms that aim to learn the optimal policy function directly in a Markov Decision Processes setting (*S, A, P, R, *γ). In PG, the **policy **π is represented by a parametric function (e.g., a neural network), so we can control its outputs by changing its parameters. The **policy **π maps state to actions (or probability distributions over actions).</p> <p>The goal of PG is to achieve a policy that maximizes the <strong>expected cumulative rewards</strong> over a trajectory of states and actions (A.K.A. the return). Let us go through how it achieves to do so.</p> <h3 id="derivation">Derivation</h3> <p>So, considering we have a <strong>policy function π</strong>, parameterized by a parameter set θ. Given a state as input, The <strong>policy</strong> outputs a probability distribution over actions at each state. Knowing this, first, we need to come up with an <strong>objective/goal</strong> so that we would want to optimize our policy function’s parameters with regard to this goal.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2036/1*sSnx2pL_3_VGpc3ZZXbL2Q.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Note that this policy function could be approximated with any function approximation technique. However, in the case of <strong>Deep RL</strong>, we consider this function to be approximated by a <strong>Neural Network</strong> (with parameter set θ) that takes states (observations) as input and outputs the distribution over actions. It could be a discrete distribution for discrete actions or a continuous distribution for continuous actions.</p> <p>In general, the agent’s goal would be to obtain the maximum cumulative reward over a <strong>trajectory of interactions</strong> (state-action sequences). So, let <strong><em>τ</em></strong> denote such trajectory i.e., a state-action sequence <em>s₀, a₀, s₁, a₁, …, sₕ, aₕ</em>, And <strong>R(<em>τ</em>)</strong> denote the cumulative reward over this trajectory a.k.a <strong>the return</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*0X4fXdoylb1r-IUWIyWpjA.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>It is obvious that this trajectory <strong><em>τ,</em></strong> is a random variable because of its stochasticity. This makes <strong>R(<em>τ</em>)</strong> to be a stochastic function as well. As we can not maximize this stochastic function directly, we would want to maximize its expectation, meaning to maximize it on average case, while taking actions with policy π: <strong>E[ R(<em>τ</em>); π ]</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*pcz-xb2fsky67jbs92UtOw.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>And as a refresher on probabilities, this is how to calculate a discrete expectation of a function of a random variable:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2240/1*AYQ2TmEAwFop75ErmlP4ow.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>So the final objective function that we would want to maximize is rewritten as follows. Also, we will now call this objective function, <strong>the Utility Function (U)</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*x8fp_A1URSh8yP6Qma1fyQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>where <strong>P(<em>τ</em>; θ)</strong>is the probability of trajectory <em>τ</em> taking place over policy parameters θ.</p> <p>Note that this utility function is written as a function of <strong>θ</strong>, a set of policy parameters, because <strong>it is only the policy that controls the path of the trajectories</strong> since the environment dynamic is fixed (and usually unknown), and we would want to optimize this utility by changing and optimizing <strong>θ</strong>. Also, note that the reward series <strong>R</strong> over the trajectory is not dependent on the policy. Hence <strong>it is not dependent on the parameters θ</strong> because it is based on the environment and is only an experienced scalar value.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*NMBuSdL3haA9ZgTiORZeJw.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="optimization">Optimization</h3> <p>Now that we have the Utility function <strong>U(θ)</strong>, we would want to optimize it using a stochastic gradient-based optimization algorithm, namely <strong>Stochastic Gradient Ascent:</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*GnbR_fRQh3ofJZlvVQ_7vg.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>To be able to perform the parameter update, we have to compute the gradient of the Utility over <strong>θ</strong> (<strong>∇U</strong>). By attempting to do so, we get:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*8-oK4kmbJJ0EARps3EZHiw.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Note that the gradient over the summation is equal to the sum of gradients. Now we continue by applying a trick by multiplying this expression by P(<em>τ</em> ; θ)/P(<em>τ</em>; θ), which equals <strong>1</strong>. Remember from calculus that <strong>∇f(x)/f(x) = ∇log(f(x))</strong>, so we get:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*rWKTFBQYFiw8cXqfb6-S7A.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We do this trick to create a general form of expectation <strong>‘ΣP(x)f(x)’</strong> in the gradient’s formula, which we can later replace and estimate the <strong>expectation</strong> with <strong>the average over samples</strong> of real interaction data.</p> <p>As we cannot sum over ALL possible trajectories, in a stochastic setting, we estimate this expression (∇U(θ)), by generating “<strong>m</strong>” number of random trajectories using policy parameters θ and averaging the above expectation over these <strong>m</strong> trajectories to estimate the gradient ∇U(θ):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*pNUZAWxPw9BBN789Y-9fhQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>So now we have a way of estimating the gradient with trajectory samples, but there is an issue here. this gradient causes parameters θ to change in a direction that the probability of trajectories with higher return <em>*R(</em>τ<em>)</em> ***increases the most. However, this has a disadvantage, and that is the direction of change in the trajectory probability is highly dependent on how reward function R is designed. For example, if all transitions result in a positive reward, all trajectory probabilities would be increased and no trajectory would be penalized, and vice versa for cases with negative R.</p> <p>To address this issue, it is proposed to subtract a <strong>baseline value (b)</strong> [1] from the return (i.e., change ‘<strong>R(<em>τ</em>)</strong>’ to ‘<strong>R(<em>τ</em>)-b</strong>’) where this baseline <strong>b</strong> should estimate how the return would be on average. This helps returns to be centralized around zero which makes trajectory probabilities that performed better than average are increased and those that didn’t are decreased. There are multiple proposed ways of calculating baseline b, among which using a neural network is one that we will use later.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*Sq5fzKKYBVfHuf5QmrxYPw.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Further, by decomposing the trajectory probability P(<em>τ</em>) into Temporal timesteps and writing it as the <strong>product of the probabilities of all ‘H’ timesteps</strong>, and also by knowing that the environment dynamics <strong>does NOT depend on the parameters θ</strong> so its gradient over θ would be 1, we can rewrite the Trajectory’s gradient w.r.t. Temporal timesteps and solely based on the policy function <strong>π</strong>:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*QtJ9V0uA-_u5Qw_nilItVw.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Also, we can decompose the return <strong>R(<em>τ</em>)</strong> as a sum of individual timestep rewards Σrₜ. And at each timestep t, we can <strong>disregard</strong> the rewards from <strong>previous timesteps</strong> 0, … , t-1 since they are <strong>not affected by the current action aₜ</strong> (Removing terms that don’t depend on current action can lower variance). Together with incorporating the baseline, we would have the following Temporal format:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2050/1*JQ7oFxDVOJTl09NSD8bcyw.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>So the <strong>gradient estimate ‘g’</strong> will be:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*O85g5ZKpI3vY2QAKFjcWrA.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Finally, as a proper choice for estimation of the baseline b(sₜ), we can use expected return from state sₜ onward, which is also known as the <strong>Value function</strong> of this state <strong>V(sₜ)</strong>. We will use another neural network with parameter set <strong><em>ϕ</em></strong> to approximate this value function with a bellman target using either Monte Carlo or Temporal difference learning from interaction data. the final form would be as follows:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*053-3GF2kvsC6Xkz_SoRMQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Moreover, it is good to know that the difference between <strong>R(sₜ)</strong> and the baseline <strong>b(sₜ)</strong> is called the <strong>Advantage function A(sₜ)</strong>. In some implementations, as the <strong>R(sₜ)</strong> equivalences the state-action value, also known as the <strong>Q function Q(sₜ)</strong>, the advantage is written as <strong>A(sₜ) = Q(sₜ)-V(sₜ)</strong> where both Q and V can be approximated with neural networks, and maybe even with shared weights.</p> <p>It is worth mentioning that when we incorporate value estimation in policy gradient methods, it is also called <strong>the Actor-Critic method</strong>, while the <strong>actor</strong> is the policy network and the <strong>critic</strong> is the value estimator network. These types of methods are highly studied nowadays due to their good performance in various benchmarks and tasks.</p> <h3 id="the-algorithm">The Algorithm</h3> <p>Based on the derived <strong>gradient estimator g</strong>, the <strong>Value function</strong> approximation network, and the <strong>gradient ascent update rule</strong>, the Overall algorithm to train an agent with the Vanilla (simple) Policy Gradient algorithm is as follows:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*bsYAtBUIumT2yoU_bu-2jQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Note that in this algorithm, first, a number of state-action sequences (trajectories) are done then the updates are carried away. The policy network here is updated in an on-policy or online manner, whereas the value functions can be updated off-policy (offline) from batch-sampled data using gradient descent.</p> <hr/> <h2 id="pytorch-implementation">PyTorch Implementation</h2> <p>To implement VPG, we need the following components:</p> <ul> <li><strong>Policy Network</strong> with probabilistic outputs to sample from. (with Softmax output for discrete action space, or parameter estimations such as <strong>μ, σ</strong> output for Gaussian dist. for continuous action spaces)</li> <li><strong>Value Network</strong> for Advantage estimation.</li> <li><strong>Environment</strong> class with <a href="https://gymnasium.farama.org/">gym</a> interface for the agent to interact with.</li> <li><strong>Training loop</strong></li> </ul> <h3 id="networks">Networks</h3> <p>First of all, we define **Policy **and **Value Networks **as PyTorch Module classes. We are using simple Multi-layer perceptron networks for this toy task.</p> <p><strong>Importing Dependencies:</strong></p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># import dependencies
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="n">torch.distributions</span> <span class="kn">import</span> <span class="n">Categorical</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">gym</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">deque</span>
</code></pre></div></div> <p><strong>Policy Net:</strong></p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PolicyNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">PolicyNet</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rewards</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">saved_actions</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">aprob</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Softmax for categorical probabilities
</span>        <span class="k">return</span> <span class="n">aprob</span>
</code></pre></div></div> <p><strong>Value Net:</strong></p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ValueNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ValueNet</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">V</span>
</code></pre></div></div> <h3 id="training-loop-main-algorithm">Training Loop (Main algorithm)</h3> <p>We will be using a simple Cartpole environment from the <a href="https://gymnasium.farama.org/"><strong>gym</strong></a>library. You can read more about this environment and its state and action spaces <strong><a href="https://gymnasium.farama.org/environments/classic_control/cart_pole/">here</a></strong>.</p> <p>The <strong>full algorithm</strong> is as follows:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## Vanilla Policy Gradient
</span>
<span class="c1"># create environment
</span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">"</span><span class="s">CartPole-v1</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># sample toy environment
</span>
<span class="c1"># instantiate the policy and value networks
</span><span class="n">policy</span> <span class="o">=</span> <span class="nc">PolicyNet</span><span class="p">(</span><span class="n">state_dim</span><span class="o">=</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_actions</span><span class="o">=</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span> <span class="n">n_hidden</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="n">value</span> <span class="o">=</span> <span class="nc">ValueNet</span><span class="p">(</span><span class="n">state_dim</span><span class="o">=</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_hidden</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

<span class="c1"># instantiate an optimizer
</span><span class="n">policy_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">policy</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">2e-7</span><span class="p">)</span>
<span class="n">value_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">value</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">)</span>

<span class="c1"># initialize gamma and stats
</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span>
<span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">returns_deq</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">memory_buffer_deq</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">2000</span><span class="p">)</span>

<span class="k">for</span> <span class="n">n_ep</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">actions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">states</span>  <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># reset environment
</span>    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="c1"># recieve action probabilities from policy function
</span>        <span class="n">probs</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">state</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">float</span><span class="p">())</span>

        <span class="c1"># sample an action from the policy distribution
</span>        <span class="n">policy_prob_dist</span> <span class="o">=</span> <span class="nc">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">policy_prob_dist</span><span class="p">.</span><span class="nf">sample</span><span class="p">()</span>

        <span class="c1"># take that action in the environment
</span>        <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>

        <span class="c1"># store state, action and reward
</span>        <span class="n">states</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">actions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">rewards</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>

        <span class="n">memory_buffer_deq</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">new_state</span><span class="p">))</span>

        <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>

    <span class="c1">### UPDATE POLICY NET ###
</span>    <span class="n">rewards</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
    <span class="c1"># calculate rewards-to-go
</span>    <span class="n">R</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span><span class="o">*</span><span class="p">(</span><span class="n">gamma</span><span class="o">**</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">)))))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">))])</span>

    <span class="c1"># cast states and actions to tensors
</span>    <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">states</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
    <span class="n">actions</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>

    <span class="c1"># calculate baseline V(s)
</span>    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">baseline</span> <span class="o">=</span> <span class="nf">value</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>

    <span class="c1"># calculate utility func
</span>    <span class="n">probs</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
    <span class="n">sampler</span> <span class="o">=</span> <span class="nc">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="o">-</span> <span class="n">sampler</span><span class="p">.</span><span class="nf">log_prob</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>   <span class="c1"># "-" is because we are doing gradient ascent
</span>    <span class="n">utility</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">log_probs</span> <span class="o">*</span> <span class="p">(</span><span class="n">R</span><span class="o">-</span><span class="n">baseline</span><span class="p">))</span> <span class="c1"># loss that when differentiated with autograd gives the gradient of J(θ)
</span>    
    <span class="c1"># update policy weights
</span>    <span class="n">policy_optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">utility</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">policy_optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
    
    <span class="c1">########################
</span>    <span class="c1">### UPDATE VALUE NET ###
</span>
    <span class="c1"># getting batch experience data 
</span>    <span class="n">batch_experience</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">memory_buffer_deq</span><span class="p">),</span> <span class="nf">min</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">memory_buffer_deq</span><span class="p">)))</span>
    <span class="n">state_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">exp</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">exp</span> <span class="ow">in</span> <span class="n">batch_experience</span><span class="p">])</span>
    <span class="n">reward_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">exp</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">exp</span> <span class="ow">in</span> <span class="n">batch_experience</span><span class="p">]).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">new_state_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">exp</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">exp</span> <span class="ow">in</span> <span class="n">batch_experience</span><span class="p">])</span>


    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">reward_batch</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="nf">value</span><span class="p">(</span><span class="n">new_state_batch</span><span class="p">)</span>
    <span class="n">current_state_value</span> <span class="o">=</span> <span class="nf">value</span><span class="p">(</span><span class="n">new_state_batch</span><span class="p">)</span>

    <span class="n">value_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">current_state_value</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="c1"># update value weights
</span>    <span class="n">value_optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">value_loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">value_optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

    <span class="c1">########################
</span>
    <span class="c1"># calculate average return and print it out
</span>    <span class="n">returns_deq</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Episode: {:6d}</span><span class="se">\t</span><span class="s">Avg. Return: {:6.2f}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">n_ep</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">returns_deq</span><span class="p">)))</span>

<span class="c1"># close environment
</span><span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</code></pre></div></div> <h3 id="training-results">Training Results</h3> <p>After training the agent with the VPG for <strong>4000 episodes</strong>, we get the following results:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*rQwO-I_CqpDB9_A43qUJ3A.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>It is worth mentioning that vanilla PG suffers from two major limitations:</p> <ol> <li>It is highly sensitive to hyperparameters configuration such as gamma, learning rate, memory size, etc.</li> <li>It is highly prone to overshooting in the policy parameter space, which makes the learning so noisy and fragile. Sometimes the agent might take a step in the parameter space into a very suboptimal area where it would not be able to recover again.</li> </ol> <p>The second issue is addressed in some further variants of the PG algorithm, which I will try to cover in future posts.</p> <p>An illustration of a fully trained agent controlling the Cartpole environment is shown here:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/0*E2-PXki32kc2N3rh.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The full code of this implementation is available at <strong><a href="https://github.com/ebrahimpichka/vanilla-pg">this GitHub repository</a>.</strong></p> <hr/> <h2 id="conclusion"><strong>Conclusion</strong></h2> <p>In conclusion, we have explored the Policy Gradient (PG) algorithm, a powerful approach in Reinforcement Learning (RL) that directly learns the optimal policy. Throughout this blog post, we have provided a step-by-step explanation of the PG algorithm and its implementation. We started by understanding the fundamental concept of RL and the difference between value-based and policy-based methods. We then delved into the details of PG, highlighting its objective of maximizing the expected cumulative reward by updating the policy parameters using gradient ascent. We discussed the Vanilla PG algorithm as a common implementation of PG, where we compute the gradient of the log-probability of actions and update the policy using policy gradients.</p> <p>Additionally, we explored the Actor-Critic method, which combines a policy network and a value function to improve convergence. While PG offers advantages in handling continuous action spaces and non-differentiable policies, it may suffer from high variance. Nevertheless, techniques like baselines and variance reduction methods can be employed to address these challenges. By grasping the intricacies of PG, you are now equipped with a valuable tool to tackle RL problems and design intelligent systems that learn and adapt through interactions with their environments.</p> <hr/> <h2 id="references">References</h2> <p>[1] — Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning 8:229–256.</p> <p>[2] — Sutton, R. S., McAllester, D. A., Singh, S. P., Mansour, Y., et al. Policy gradient methods for reinforcement learning with function approximation. In Proc. Advances in Neural Information Processing Systems (NIPS), volume 99, pp. 1057–1063. Citeseer, 1999.</p> <p>[3] — course lectures from UC Berkeley: Deep Reinforcement Learning Bootcamp</p> <p>[4] — <a href="http://spinningup.openai.com/en/latest/algorithms/vpg.html">spinningup.openai.com/en/latest/algorithms/vpg.html</a></p> <p>[5] — Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction (second edition), The MIT Press <strong><a href="http://incompleteideas.net/book/RLbook2020.pdf">[PDF]</a></strong></p>]]></content><author><name></name></author><category term="reinforcement-learning"/><category term="RL"/><category term="policy-gradient"/><category term="REINFORCE"/><summary type="html"><![CDATA[A step-by-step explanation of the vanilla policy gradient algorithm and its implementation.]]></summary></entry><entry><title type="html">The Best Resources to Learn Reinforcement Learning</title><link href="https://epichka.com/blog/2022/rl-resources/" rel="alternate" type="text/html" title="The Best Resources to Learn Reinforcement Learning"/><published>2022-01-12T12:00:00+00:00</published><updated>2022-01-12T12:00:00+00:00</updated><id>https://epichka.com/blog/2022/rl-resources</id><content type="html" xml:base="https://epichka.com/blog/2022/rl-resources/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2048/1*fC-GHp0h_8OSwYUIHvvhxw.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="introduction">Introduction</h2> <p>Reinforcement learning (RL) is a paradigm of AI methodologies in which an agent learns to interact with its environment in order to maximize the expectation of reward signals received from its environment. Unlike supervised learning, in which the agent is given labeled examples and learns to predict an output based on input, RL involves the agent actively taking actions in its environment and receiving feedback in the form of rewards or punishments. This feedback is used to adjust the agent’s behavior and improve its performance over time.</p> <p>RL has been applied to a wide range of domains, including robotics, natural language processing, and finance. In the gaming industry, RL has been used to develop advanced game-playing agents, such as the <strong>AlphaGo [1]</strong> algorithm that defeated a human champion in the board game Go. In the healthcare industry, RL has been used to optimize treatment plans for patients with chronic diseases, such as diabetes. RL has also been used in the field of robotics, allowing robots to learn and adapt to new environments and tasks.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2038/1*Y6mxK5VOgXkOlwlqv3TJNQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>One of the most iconic recent breakthroughs in RL is the development of <a href="https://openai.com/blog/chatgpt/">chatGPT</a> [2] by OpenAI, a natural language processing system that can hold intelligent conversations with humans. chatGPT was trained on a large dataset of human conversations and can generate coherent and contextually appropriate responses to user inputs. This system demonstrates the potential for RL to be used to improve natural language processing systems and create more human-like AI assistants.</p> <p>As RL continues to advance and make an impact in various fields, it has become increasingly important for professionals and researchers to have a strong understanding of this technique. If you’re interested in learning about RL, you’re in luck! There are a variety of resources available online that can help you get started and become proficient in this exciting field. In this blog post, we’ll highlight some of the best, mostly free, resources for learning about RL, including tutorials, courses, books, and more. Whether you’re a beginner looking to get your feet wet or an experienced practitioner looking to deepen your understanding, these resources will have something for you.</p> <p>In this post, we are going to first start by introducing the best **online courses, lectures, and tutorials **available for RL on the internet. Then we will introduce the best and most popular **books **and **textbooks **in the field. And at last, we will also include some useful extra resources and GitHub repositories on the topic.</p> <h2 id="online-courses">Online Courses</h2> <p>While there are numerous courses available on the subject, we’ve carefully selected a list of the most comprehensive and high-quality options that are mostly free. These courses cover a wide range of topics in RL, from the basics to advanced concepts, and are taught by experts in the field. Whether you’re a beginner looking to get your feet wet or an experienced practitioner looking to deepen your understanding, these courses will have something for you. Keep reading to discover some of the top online courses for learning about RL! Please note that this is not an exhaustive list, but rather a curated selection of the most highly recommended courses available.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/3794/1*-is1XgAQyi9d58SA0yiH_w.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The Reinforcement Learning Specialization on Coursera, offered by the University of Alberta and the Alberta Machine Intelligence Institute, is a comprehensive program designed to teach you the foundations of reinforcement learning. This specialization consists of three courses and one capstone project that cover a wide range of topics in RL, including RL fundamentals, value-based methods, policy gradient methods, model-based RL, deep RL, etc. Throughout the course, you’ll have the opportunity to apply what you’ve learned through hands-on programming assignments and a final project. The course is taught by experienced instructors and academics who are experts in the field of RL and includes a mix of lectures, readings, and interactive exercises. This specialization is suitable for students with a background in machine learning or a related field and is a great resource for anyone looking to gain a solid understanding of RL.</p> <p>Although it is not technically free, you could always apply for Coursera’s financial aid to waive the course fee if you were not to afford it. However, considering the content quality and material, it would be totally worthwhile.</p> <p>Link to the course:www.coursera.org/specializations/reinforcement-learning</p> <h3 id="2---reinforcement-learning-lecture-series-2021--by-deepmind-x-ucl"><a href="https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021">2 - Reinforcement Learning Lecture Series 2021 — by DeepMind x UCL</a></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2400/0*ywdNu229dv3PQnK8.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The “Reinforcement Learning Lecture Series” is a series of lectures on the topic of reinforcement learning, presented by DeepMind and UCL. This course covers a wide range of topics within the field of reinforcement learning, including foundational concepts such as Markov decision processes and dynamic programming, as well as more advanced techniques such as model-based and model-free learning and off-policy, value-/policy-based algorithms, function approximation, and deep RL. The lectures are offered by renowned academics and researchers from Deepmind and UCL. The lectures are aimed at researchers and practitioners interested in learning about the latest developments and applications in reinforcement learning. The course is offered online and is open to anyone who is interested in learning about this exciting and rapidly-evolving field.</p> <p>Link to the course: <a href="https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021"><strong>Reinforcement Learning Lecture Series 2021</strong></a></p> <p>There is also an older version of this series from 2018 which could be <a href="https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2018">found here</a>.</p> <h3 id="3---stanford-cs234-reinforcement-learning--winter-2019"><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u">3 - Stanford CS234: Reinforcement Learning — Winter 2019</a></h3> <p>The CS234 Reinforcement Learning course from Stanford is a comprehensive study of reinforcement learning, taught by Prof. Emma Brunskill. This course covers a wide range of topics in RL, including foundational concepts such as MDPs and Monte Carlo methods, as well as more advanced techniques like temporal difference learning and deep reinforcement learning. The course is designed for students who have a background in machine learning and are interested in learning about the latest techniques and applications in reinforcement learning. The course is offered through a series of video lectures, which are available on YouTube through the provided link.</p> <p>Link to the course: <a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u">Here</a></p> <h3 id="4---introduction-to-reinforcement-learning-with-david-silver"><a href="https://www.deepmind.com/learning-resources/introduction-to-reinforcement-learning-with-david-silver">4 - Introduction to Reinforcement Learning with David Silver</a></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2560/0*fiC_p1Qa5Q6iF6ms.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The Introduction to Reinforcement Learning with David Silver course is a comprehensive introduction to the field of reinforcement learning, taught by Professor David Silver. Silver is a leading researcher in the field of reinforcement learning and artificial intelligence, and has been a key contributor to the development of AlphaGo, the first computer program to defeat a professional human player in the game of Go. He is also among the authors of some of the key research papers in RL such as Deep Q-Learning and DDPG algorithm. The course covers the fundamental concepts and techniques of reinforcement learning, including dynamic programming, Monte Carlo methods, and temporal difference learning. It also covers more advanced topics such as exploration-exploitation trade-offs, function approximation, and deep reinforcement learning. Overall, the course provides a solid foundation in reinforcement learning and is suitable for anyone interested in learning more about this exciting and rapidly-evolving field of artificial intelligence.</p> <p>Link to the course: <a href="https://www.deepmind.com/learning-resources/introduction-to-reinforcement-learning-with-david-silver"><strong>Introduction to Reinforcement Learning with David Silver</strong></a></p> <h3 id="5---uc-berkeley-cs-285-deep-reinforcement-learning--fall-2021"><a href="https://www.youtube.com/playlist?list=PL_iWQOsE6TfXxKgI1GgyV1B_Xa0DxE5eH">5 - UC Berkeley CS 285: Deep Reinforcement Learning — Fall 2021</a></h3> <p>The UC Berkeley CS 285 Deep Reinforcement Learning course is a graduate-level course that covers the field of reinforcement learning, with a focus on deep learning techniques. The course is taught by Prof. Sergey Levine and is designed for students who have a strong background in machine learning and are interested in learning about the latest techniques and applications in reinforcement learning. The course covers a wide range of topics, including foundational concepts such as Markov decision processes and temporal difference learning, as well as advanced techniques like deep Q-learning and policy gradient methods. The course is offered through a series of video lectures, which are available on YouTube through the provided link.</p> <p>Link to the course: <a href="https://www.youtube.com/playlist?list=PL_iWQOsE6TfXxKgI1GgyV1B_Xa0DxE5eH">Here</a></p> <p>There is also an older series of the course from Fall 2020 <a href="https://www.youtube.com/playlist?list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc">here</a>.</p> <h3 id="6---deep-rl-bootcamp--uc-berkeley"><a href="https://sites.google.com/view/deep-rl-bootcamp/lectures">6 - Deep RL BootCamp — UC Berkeley</a></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*_K-ar5pvp6_MpigC_9SnnA.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The Deep RL Bootcamp is an intensive two-day course on deep reinforcement learning, taught by leading researchers in the field. The course covers a wide range of topics, including value-based methods, policy gradient algorithms, model-based reinforcement learning, exploration and uncertainty, and deep reinforcement learning in the real world. It features a mix of lectures and hands-on exercises, giving attendees the opportunity to learn about the latest techniques and apply them to real-world problems. The course is designed for researchers and practitioners with a background in machine learning and/or reinforcement learning and is suitable for those looking to gain a deeper understanding of the field and advance their research or career in this exciting area of artificial intelligence.</p> <p>Link to the course: <a href="https://sites.google.com/view/deep-rl-bootcamp/lectures"><strong>Deep RL Bootcamp</strong></a></p> <h3 id="7---deep-reinforcement-learning-course-by-huggingface"><a href="https://simoninithomas.github.io/deep-rl-course/">7 - Deep Reinforcement Learning Course by HuggingFace</a></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/3840/0*MBs-nwimt6nrzs2Z.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The Deep RL course by Hugging Face is an in-depth and interactive learning experience that covers the most important topics in deep reinforcement learning. The course is divided into units that cover various aspects of the field such as the Q-learning algorithm, policy gradients, and advanced topics like exploration, multi-agent RL, and meta-learning. Each unit includes a combination of video lectures, interactive coding tutorials, and quizzes to help learners understand and apply the concepts.</p> <p>The course also includes hands-on projects that allow learners to apply their knowledge to real-world problems. These projects include creating an RL agent to play a game, training an RL agent to navigate a virtual environment, and building an RL agent to play a game of chess. These projects provide an opportunity for learners to get hands-on experience working with RL models, and gain an understanding of the challenges and complexities of working with these models.</p> <p>The course also includes explanations of the theoretical foundations of RL, providing an understanding of the mathematical concepts and algorithms used in the field. The course is designed to be accessible to people with different backgrounds and levels of experience, from those new to the field to experienced practitioners. The course is taught by Simon Thomas, who is a researcher and expert in the field of deep reinforcement learning, and the course content is regularly updated to keep up with the latest advancements in the field.</p> <p>Links to the course: <a href="https://simoninithomas.github.io/deep-rl-course/"><strong>Deep Reinforcement Learning Course</strong> <em>Deep Reinforcement Learning Course is a free course about Deep Reinforcement Learning from beginner to expert.</em>simoninithomas.github.io</a> <a href="https://huggingface.co/deep-rl-course/unit0/introduction"><strong>Welcome to the 🤗 Deep Reinforcement Learning Course - Hugging Face Course</strong> <em>Welcome to the most fascinating topic in Artificial Intelligence: Deep Reinforcement Learning. This course will teach…</em>huggingface.co</a> <a href="https://github.com/huggingface/deep-rl-class"><strong>GitHub - huggingface/deep-rl-class: This repo contains the syllabus of the Hugging Face Deep…</strong> <em>This repository contains the Deep Reinforcement Learning Course mdx files and notebooks. The website is here…</em>github.com</a></p> <h3 id="8---lectures-by-pieter-abbeel">8 - Lectures by Pieter Abbeel</h3> <p><a href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a> is a renowned computer scientist and roboticist who is currently a professor at the University of California, Berkeley. He is known for his research in the field of robotics, particularly in the areas of reinforcement learning, learning from demonstration, and robot manipulation. He has made notable contributions to the field of robotic grasping and manipulation, developing algorithms for robots to learn to grasp and manipulate objects using trial-and-error.</p> <p>He also has been a pioneer in the field of apprenticeship learning, which allows robots to learn from human demonstrations. He has published over 150 papers, many of which can be accessed on his personal website and also has a set of video lectures available on youtube. He has also been involved in the development of open-source software for robotics and machine learning and is the co-author of the popular open-source software library <a href="https://www.gymlibrary.dev/">OpenAI Gym</a>, which is widely used in the field of reinforcement learning.</p> <p>His online lectures, which are available on YouTube are one of the high quality material available in reinforcement learning.</p> <p><strong>His “Foundations of Deep RL — lecture series” on his own YouTube channel:</strong></p> <p><strong>His Lectures from CS188 Artificial Intelligence UC Berkeley, Spring 2013:</strong></p> <h3 id="9---spinning-up-in-deep-rl-by-openai"><a href="https://spinningup.openai.com/en/latest/user/introduction.html">9 - Spinning Up in Deep RL by OpenAI</a></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/0*O9DBfKY60Af4MLau.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://spinningup.openai.com/en/latest/user/introduction.html">Spinning Up in Deep RL</a> is developed and maintained by OpenAI. It is a resource for people who want to learn about deep reinforcement learning (RL) and how to apply it. The website provides a comprehensive introduction to RL and its algorithms and includes tutorials and guides on how to implement and run RL experiments. The website also includes a set of resources such as papers, videos, and code examples to help users learn about RL.</p> <p>The website is based on the software library OpenAI Baselines, which is an implementation of RL algorithms in Python with PyTorch and TensorFlow. The library includes implementations of popular RL algorithms such as DQN, PPO, A2C, and TRPO. The website provides detailed instructions and code examples on how to use the library to train RL agents and run experiments.</p> <p>The website is designed to be accessible to people with different levels of experience and provides a step-by-step guide to getting started with RL. The website is divided into sections, including an introduction to RL, tutorials on how to use the library, and a section on advanced topics such as multi-agent RL, exploration, and meta-learning. The website also provides a set of Jupyter notebooks that users can run and modify, allowing them to experiment with different RL algorithms and environments.</p> <p>The link to the website: <a href="https://spinningup.openai.com/en/latest/index.html"><strong>Welcome to Spinning Up in Deep RL! - Spinning Up documentation</strong></a></p> <h3 id="10---phil-tabors-rl-courses">10 - Phil Tabor’s RL Courses</h3> <p>Phil Tabor is a machine learning engineer and educator who specializes in the field of reinforcement learning. He is known for his practical approach to teaching and has a special focus on the hands-on aspect of the field. He has created several courses on machine learning and artificial intelligence on Udemy, with a focus on reinforcement learning. He also has a YouTube channel <a href="https://www.youtube.com/@MachineLearningwithPhil">“Machine Learning with Phil”</a> where he uploads videos on various reinforcement learning topics such as Q-learning, policy gradients, and more advanced topics. He also uploads code-along videos to help learners understand the concept and apply them.</p> <p>His more practical approach to the field makes it rather much different than other available content. Aside from his paid courses on Udemy which are very comprehensive and well-framed, he has tons of free content on his <a href="https://www.youtube.com/@MachineLearningwithPhil">YouTube channel</a> which are not much less than his paid ones.</p> <p>**Youtube channel: <a href="https://www.youtube.com/@MachineLearningwithPhil">**https://www.youtube.com/@MachineLearningwithPhil</a></p> <p>**Udemy: <a href="https://www.udemy.com/user/phil-tabor/">**https://www.udemy.com/user/phil-tabor/</a></p> <h2 id="books">Books</h2> <p>There are tons of great books published about reinforcement learning however 5 of the most popular and comprehensive ones are listed below:</p> <h3 id="1-richard-sutton-and-andrew-barto-reinforcement-learning-an-introduction-2nd-edition--most-recommended"><a href="http://incompleteideas.net/book/RLbook2020.pdf">1. Richard Sutton and Andrew Barto, “Reinforcement Learning: An Introduction” (2nd Edition)</a> — Most Recommended*</h3> <p><a href="http://incompleteideas.net/book/RLbook2020.pdf">Reinforcement Learning: An Introduction</a> (2nd Edition) by Richard Sutton and Andrew Barto is a must-have resource for anyone interested in the field of reinforcement learning. This book provides a comprehensive introduction to the fundamental concepts and algorithms of reinforcement learning, making it an essential resource for students, researchers, and practitioners. The second edition includes new chapters on recent developments in the field and updates to existing material, making it even more current and relevant.</p> <p>The book starts with an introduction to the basic concepts of RL and lays out the RL problem along with a history of the field and its relationship to other fields such as psychology, neuroscience, and control theory. It then delves into the foundational algorithms and concepts of the field, including Multiarm bandits, Markov decision processes, dynamic programming, and Monte Carlo methods.</p> <p>The book also covers advanced topics such as temporal-difference learning, planning and learning with function approximators, and exploration and exploitation in reinforcement learning. Additional chapters discuss the application of reinforcement learning in various domains, including robotics, game playing, and healthcare.</p> <p>The book also includes chapters on recent developments in the field such as deep reinforcement learning, policy gradient methods, and inverse reinforcement learning. The final chapters cover the challenges and future of the field, including safety and reliability, multi-agent reinforcement learning, and the role of reinforcement learning in artificial general intelligence.</p> <p><strong>Book Chapters:</strong></p> <ol> <li> The Reinforcement Learning Problem</li> <li> Multi-arm Bandits</li> <li> Finite Markov Decision Processes</li> <li> Dynamic Programming</li> <li> Monte Carlo Methods</li> <li> Temporal-Difference Learning</li> <li> Eligibility Traces</li> <li> Planning and Learning with Tabular Methods</li> <li> On-policy Approximation of Action Values</li> <li> Off-policy Approximation of Action Values</li> <li> Policy Approximation</li> <li> Psychology</li> <li> Neuroscience</li> <li> Applications and Case Studies</li> <li> Prospects</li> </ol> <h3 id="2-mykel-j-kochenderfer-decision-making-under-uncertainty-theory-and-application"><a href="https://mitpress.mit.edu/9780262029254/decision-making-under-uncertainty/">2. Mykel J. Kochenderfer, “Decision Making Under Uncertainty: Theory and Application”</a></h3> <p><a href="https://mitpress.mit.edu/9780262029254/decision-making-under-uncertainty/">Decision Making Under Uncertainty: Theory and Application</a>, by Mykel J. Kochenderfer, is a comprehensive guide to decision-making under uncertainty, with a focus on reinforcement learning. The book covers the fundamental concepts of decision theory, Markov decision processes, and reinforcement learning algorithms, providing the reader with a solid foundation in these areas.</p> <p>The book also delves into advanced topics such as planning under uncertainty, safe reinforcement learning, and the use of decision-making methods in real-world applications. The author explains the concepts in a clear and concise manner, with the help of examples and exercises to help the reader understand and apply the material.</p> <p>The book is intended for a broad audience, including researchers and practitioners in the fields of artificial intelligence, operations research, and control systems. It’s also suitable for advanced undergraduate and graduate students in these areas. The book provides a thorough introduction to the theory and application of decision-making under uncertainty, with a focus on reinforcement learning, making it an essential resource for anyone interested in this field.</p> <p><strong>Book Chapters:</strong></p> <ol> <li> Introduction</li> <li> Probabilistic Models</li> <li> Decision Problems</li> <li> Sequential Problems</li> <li> Model Uncertainty</li> <li> State Uncertainty</li> <li> Cooperative Decision Making</li> <li> Probabilistic Surveillance Video Search</li> <li> Dynamic Models for Speech Applications</li> <li> Optimized Airborne Collision Avoidance</li> <li> Multi-agent Planning for Persistent Surveillance</li> <li> Integrating Automation with Humans</li> </ol> <h3 id="3-phil-winder-reinforcement-learning"><a href="https://www.oreilly.com/library/view/reinforcement-learning/9781492072386/">3. Phil Winder, “Reinforcement Learning”</a></h3> <p>“Reinforcement Learning” by Phil Winder is an in-depth examination of one of the most exciting and rapidly growing areas of machine learning. The book provides a comprehensive introduction to the theory and practice of reinforcement learning, covering a wide range of topics that are essential for understanding and working with this powerful technique.</p> <p>The book starts with the fundamentals of Markov decision processes, which form the mathematical foundation of reinforcement learning. It then delves into Q-learning, a popular algorithm for finding the optimal action-value function in a given environment. The book also covers policy gradients, a class of algorithms that allow for the optimization of policies directly, rather than value functions. Additionally, it covers the recent advancements in deep reinforcement learning and how it can be applied to solve complex problems.</p> <p>The book also includes numerous practical examples and exercises that help readers apply the concepts to real-world problems. This book is ideal for machine learning practitioners, researchers, and students who are interested in understanding and working with reinforcement learning. It provides a clear and accessible introduction to the field, making it an essential resource for anyone looking to get started with reinforcement learning or deepen their understanding of this powerful technique.</p> <p><strong>Book Chapters:</strong></p> <ol> <li>Why Reinforcement Learning?</li> <li>Markov Decision Processes, Dynamic Programming, and Monte Carlo Methods</li> <li>Temporal-Difference Learning, Q-Learning, and n-Step Algorithms</li> <li>Deep Q-Networks</li> <li>Policy Gradient Methods</li> <li>Beyond Policy Gradients</li> <li>Learning All Possible Policies with Entropy Methods</li> <li>Improving How an Agent Learns</li> <li>Practical Reinforcement Learning</li> <li>Operational Reinforcement Learning</li> <li>Conclusions and the Future</li> </ol> <h3 id="4-alexander-zai-and-brandon-brown-deep-reinforcement-learning-in-action"><a href="https://www.manning.com/books/deep-reinforcement-learning-in-action">4. Alexander Zai and Brandon Brown, “Deep Reinforcement Learning in Action”</a></h3> <p>“Deep Reinforcement Learning in Action” by Alexander Zai and Brandon Brown is an in-depth guide that takes the reader through the process of building intelligent systems using deep reinforcement learning. The book starts by introducing the basic concepts and algorithms of reinforcement learning, including Q-learning and policy gradients. It then goes on to cover more advanced topics such as actor-critic methods and deep Q-networks (DQN), which are used to improve the performance of reinforcement learning algorithms.</p> <p>One of the key features of the book is its emphasis on hands-on examples and exercises. Throughout the book, the authors provide code snippets and sample projects that illustrate how to implement reinforcement learning algorithms in practice. These examples and exercises are designed to help readers understand the material and apply it to their own projects.</p> <p>In addition to covering the fundamentals of reinforcement learning, the book also covers recent advances in the field such as double DQN, prioritized replay, and A3C. These techniques are used to improve the performance of reinforcement learning algorithms and make them more efficient. The book is intended for readers with some experience in machine learning and deep learning, but no prior experience with reinforcement learning is required. The authors provide a comprehensive and accessible introduction to the field, making it an ideal choice for both beginners and experienced practitioners.</p> <p><strong>Book Chapters:</strong></p> <ol> <li>What is reinforcement learning </li> <li>Modeling reinforcement learning problems: Markov decision processes </li> <li>Predicting the best states and actions: Deep Q-networks </li> <li>Learning to pick the best policy: Policy gradient methods </li> <li>Tackling more complex problems with actor-critic methods </li> <li>Alternative optimization methods: Evolutionary algorithms </li> <li>Distributional DQN: Getting the full story </li> <li>Curiosity-driven exploration </li> <li>Multi-agent reinforcement learning </li> <li>Interpretable reinforcement learning: Attention and relational model </li> <li>conclusion: A review and roadmap </li> </ol> <h3 id="5-maxim-lapan-deep-reinforcement-learning-hands-on"><a href="https://www.packtpub.com/product/deep-reinforcement-learning-hands-on-second-edition/9781838826994">5. Maxim Lapan, “Deep Reinforcement Learning Hands-On”</a></h3> <p>Deep Reinforcement Learning Hands-On” by Maxim Lapan is an updated edition of the popular guide to understanding and implementing deep reinforcement learning (DRL) techniques. This book is designed to provide readers with a solid understanding of the key concepts and techniques behind DRL and to equip them with the practical skills needed to build and train their own DRL models.</p> <p>The book covers a wide range of topics, including the basics of reinforcement learning and its connection to neural networks, advanced DRL algorithms such as Q-Learning, SARSA, and DDPG, and the use of DRL in real-world applications such as robotics, gaming, and autonomous vehicles. Additionally, the book includes practical examples and hands-on exercises, allowing readers to apply the concepts and techniques covered in the book to real-world problems.</p> <p>With its focus on both theory and practice, “Deep Reinforcement Learning Hands-On” is the perfect guide for anyone looking to gain a deep understanding of DRL and start building their own DRL models.</p> <p><strong>Book Chapters:</strong></p> <ol> <li> What Is Reinforcement Learning?</li> <li> OpenAI Gym</li> <li> Deep Learning with PyTorch</li> <li> The Cross-Entropy Method</li> <li> Tabular Learning and the Bellman Equation</li> <li> Deep Q-Networks</li> <li> Higher-Level RL Libraries</li> <li> DQN Extensions</li> <li> Ways to Speed up RL</li> <li> Stocks Trading Using RL</li> <li> Policy Gradients — an Alternative</li> <li> The Actor-Critic Method</li> <li> Asynchronous Advantage Actor-Critic</li> <li> Training Chatbots with RL</li> <li> The TextWorld Environment</li> <li> Web Navigation</li> <li> Continuous Action Space</li> <li> RL in Robotics</li> <li> Trust Regions — PPO, TRPO, ACKTR, and SAC</li> <li> Black-Box Optimization in RL</li> <li> Advanced Exploration</li> <li> Beyond Model-Free — Imagination</li> <li> AlphaGo Zero</li> <li> RL in Discrete Optimization</li> <li> Multi-agent RL</li> </ol> <h2 id="bonus-other-useful-resources">Bonus: Other Useful Resources</h2> <h3 id="the-best-tools-for-reinforcement-learning-in-python"><a href="https://neptune.ai/blog/the-best-tools-for-reinforcement-learning-in-python"><strong>The Best Tools for Reinforcement Learning in Python</strong></a></h3> <p>This post by neptune.ai provides an overview of the popular tools and libraries used in RL with Python to help readers decide which tools are best suited for their specific use case. it covers a variety of popular RL libraries such as TensorFlow, PyTorch, and OpenAI Baselines, as well as other tools such as OpenAI Gym, and RL Toolbox. The post also covers other topics such as visualization tools, model management tools and experiment tracking tools which are useful for RL. The blog post is well-organized and easy to follow. It includes code examples and links to the relevant documentation for each tool, making it a useful resource for anyone interested in getting started with RL in Python. <a href="https://neptune.ai/blog/the-best-tools-for-reinforcement-learning-in-python"><strong>The Best Tools for Reinforcement Learning in Python You Actually Want to Try - neptune.ai</strong> <em>Nowadays, Deep Reinforcement Learning (RL) is one of the hottest topics in the Data Science community. The fast…</em>neptune.ai</a></p> <h3 id="awesome-deep-rl"><a href="https://github.com/kengz/awesome-deep-rl#tutorials"><strong>awesome-deep-rl</strong></a></h3> <p>This GitHub repository is a curated list of resources for deep reinforcement learning (RL) and contains a comprehensive list of <strong>papers</strong>, <strong>tutorials</strong>, <strong>videos</strong>, and other resources on various topics related to deep RL, such as Q-learning, policy gradients, exploration, meta-learning, and more. It also includes links to popular RL libraries and frameworks, such as TensorFlow, PyTorch, and OpenAI Baselines, as well as other tools and resources that are useful for RL. The repository is well-organized and easy to navigate, making it a useful resource for anyone interested in learning about deep RL. <a href="https://github.com/kengz/awesome-deep-rl#tutorials"><strong>GitHub - kengz/awesome-deep-rl: A curated list of awesome Deep Reinforcement Learning resources.</strong> <em>A curated list of awesome Deep Reinforcement Learning resources. - GitHub - kengz/awesome-deep-rl: A curated list of…</em>github.com</a></p> <h3 id="awesome-deep-reinforcement-learning-in-finance"><a href="https://wire.insiderfinance.io/awesome-deep-reinforcement-learning-in-finance-f319f4302897"><strong>Awesome Deep Reinforcement Learning in Finance</strong></a></h3> <p>this article provides an overview of the use of deep reinforcement learning (RL) in the field of finance. The article includes a curated list of resources for learning more about RL in finance, including papers, videos, and tutorials. The article discusses the potential applications of RL in finance such as portfolio management, algorithmic trading, and risk management. It also highlights some of the challenges and limitations of using RL in finance, such as the lack of data and the difficulty of evaluating the performance of RL models. <a href="https://wire.insiderfinance.io/awesome-deep-reinforcement-learning-in-finance-f319f4302897"><strong>Awesome Deep Reinforcement Learning in Finance</strong></a></p> <h2 id="refernces">Refernces</h2> <p>[1] — Silver, D., Huang, A., Maddison, C. <em>et al.</em> Mastering the game of Go with deep neural networks and tree search. <em>Nature</em> <strong>529</strong>, 484–489 (2016). <a href="https://doi.org/10.1038/nature16961">https://doi.org/10.1038/nature16961</a></p> <p>[2] — <a href="https://openai.com/blog/chatgpt/">https://openai.com/blog/chatgpt/</a></p>]]></content><author><name></name></author><category term="reinforcement-learning"/><category term="RL"/><category term="resources"/><summary type="html"><![CDATA[Some of the best (mostly free) tutorials, courses, books, and more on this ever-evolving field]]></summary></entry></feed>