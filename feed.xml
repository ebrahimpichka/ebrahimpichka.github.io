<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://epichka.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://epichka.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-12T23:47:47+00:00</updated><id>https://epichka.com/feed.xml</id><title type="html">Ebrahim Pichka</title><subtitle>Ebrahim Pichka&apos;s personal webiste. </subtitle><entry><title type="html">Automatic Differentiation (AutoDiff): A Brief Intro with Examples</title><link href="https://epichka.com/blog/2024/autodiff/" rel="alternate" type="text/html" title="Automatic Differentiation (AutoDiff): A Brief Intro with Examples"/><published>2024-10-10T12:00:00+00:00</published><updated>2024-10-10T12:00:00+00:00</updated><id>https://epichka.com/blog/2024/autodiff</id><content type="html" xml:base="https://epichka.com/blog/2024/autodiff/"><![CDATA[<p>An introduction to the mechanics of AutoDiff, exploring its mathematical principles, implementation strategies, and applications in currently most-used frameworks</p> <p><img src="https://cdn-images-1.medium.com/max/12032/0*2zoQV7HydfU2dV8c" alt="Photo by [Bozhin Karaivanov](https://unsplash.com/@bkaraivanov?utm_source=medium&amp;utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&amp;utm_medium=referral)" style="margin:auto; display:block;" class="img-fluid rounded z-depth-1"/></p> <hr/> <h1 id="1-the-role-of-differentiation-in-modern-ml-optimization">1. The Role of Differentiation in Modern ML Optimization</h1> <p>At the heart of machine learning lies the optimization of loss/objective functions. This optimization process heavily relies on computing gradients of these functions with respect to model parameters. As Baydin et al. (2018) elucidate in their comprehensive survey [1], these gradients guide the iterative updates in optimization algorithms such as stochastic gradient descent (SGD):</p> \[\theta_{t+1} = \theta_{t} - \alpha \nabla_{\theta} \mathbb{L}_{\theta_{t}}(x)\] <p>Where:</p> <ul> <li> <p>\(\theta_{t}\) represents the model parameters at step t</p> </li> <li> <p>\(\alpha\) is the learning rate</p> </li> <li> <p>\(\nabla_{\theta}\mathbb{L}_{\theta_{t}}(x)\) denotes the gradient of the loss function \(\mathbb{L}\) with respect to the parameters \(\theta\)</p> </li> </ul> <p>This simple update rule belies the complexity of computing gradients in deep neural networks with millions or even billions of parameters.</p> <hr/> <h1 id="2-the-differentiation-triad">2. The Differentiation Triad</h1> <p>Differentiation can generally be performed in three main manners. <strong>Symbolic</strong>, <strong>Numeric</strong>, and <strong>Automatic</strong> Differentiation. We will now briefly discuss the differences between them.</p> <h2 id="21-symbolic-differentiation">2.1 Symbolic Differentiation</h2> <p><em>Symbolic</em> differentiation involves the manipulation of mathematical expressions to produce exact derivatives. If you have ever taken any introductory courses in calculus, this method must’ve been your first exposure to differentiation. While it provides precise results, it often leads to expression swell, making it impractical for the complex, nested functions typical in machine learning [1].</p> <p>Consider the function \(f(x) = x^4 + 3x^2 + 2x\). Symbolic differentiation would yield:</p> \[f'(x) = 4x^3 + 6x + 2\] <p>While this is manageable for simple functions with clear analytical clsoed forms, imagine the complexity for a neural network with thousands of nonlinearities and multiple skip connections, branches, heads!</p> <h2 id="22-numeric-differentiation">2.2 Numeric Differentiation</h2> <p>Numeric differentiation approximates derivatives using finite differences following thw formal definition of derivatives, namely:</p> \[f'(x) ≈ \frac{f(x + h) - f(x)}{h}\] <p>This method simply tries to compute an approximate value for \(f'\) by <strong>assigning a</strong> <strong>very small value to \(h\)</strong> and computing the change it causes in the output of \(f\). While straightforward to implement, it’s realy susceptible to truncation errors (for large h) and round-off errors (for small h) [2]. Moreover, its computational cost scales poorly with the number of input variables as each input \(x_i\) would require calling of the function separately.</p> <h2 id="23-automatic-differentiation">2.3 Automatic Differentiation</h2> <p>In contrast with the two previous methods, Automatic Differentiation, <strong><em>AutoDiff</em></strong> for short, strikes a balance between symbolic and numeric methods, computing exact derivatives (up to machine precision) efficiently by systematically applying <a href="https://tutorial.math.lamar.edu/classes/calci/chainrule.aspx"><strong>the chain rule</strong></a> to elementary operations and functions [1]. In short, the chain rule basically says that the <strong>derivative of a composite function</strong> is the <strong>product of the derivatives of its component</strong> functions. Mathematically, if we have two functions \(y = f(u)\) and \(u = f(x)\), we have:</p> \[\frac{dy}{dx} = \frac{dy}{du} \times \frac{du}{dx}\] <p>where:</p> <ul> <li> <p><em>dy/dx</em> is the derivative of <em>y</em> with respect to <em>x</em> (the overall derivative we’re trying to find — in case of deep learning models, <em>y</em> is usually the finall loss and <em>x</em> is the doels weights)</p> </li> <li> <p><em>dy/du</em> is the derivative of <em>y</em> with respect to an intermediate variable <em>u</em></p> </li> <li> <p><em>du/dx</em> is the derivative of the intermediate variable <em>u</em> with respect to <em>x</em></p> </li> </ul> <p>Leveraging the chain rule, along with some implementation details that we are going to discuss next, allows us to compute gradients in a very optimal manner.</p> <hr/> <h1 id="3-autodiff-modes-forward-and-reverse">3. AutoDiff Modes: Forward and Reverse</h1> <p>AutoDiff can be practically done in two ways, namely <strong>Forward</strong> mode and <strong>Reverse</strong> mode differentiation, each having some computational advantages and disadvantages based on the use case.</p> <h2 id="31-forward-mode">3.1 Forward Mode</h2> <p>Forward mode — also known as left-to-right — AutoDiff computes <strong>directional</strong> derivatives alongside the function evaluation. It’s particularly efficient for functions with few inputs and many outputs [3].</p> <p>Mathematically, for a function \(y = f(x)\) where \(x \in \mathrm{R}^n\) and $$y \in \mathrm{R}^m$, forward mode computes the Jacobian-vector product on the side:</p> <p><img src="https://cdn-images-1.medium.com/max/2000/1*wZ21Lgor0RqG1dHOOrSVHQ.png" alt="" style="margin:auto; display:block;" class="img-fluid rounded z-depth-1"/></p> <p>Where \(J\) is the Jacobian matrix and \(\dot{x}\) is the seed vector. For a detailed explanation of Jacobian-vector product see <a href="https://maximerobeyns.com/of_vjps_and_jvps">here</a>.</p> <p>Let’s implement a simple forward mode AD:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Dual</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">derivative</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>
        <span class="n">self</span><span class="p">.</span><span class="n">derivative</span> <span class="o">=</span> <span class="n">derivative</span> <span class="c1"># works as \dot{x}
</span>
    <span class="k">def</span> <span class="nf">__add__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="nc">Dual</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">+</span> <span class="n">other</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">derivative</span> <span class="o">+</span> <span class="n">other</span><span class="p">.</span><span class="n">derivative</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__mul__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="nc">Dual</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">*</span> <span class="n">other</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> 
                    <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">*</span> <span class="n">other</span><span class="p">.</span><span class="n">derivative</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">derivative</span> <span class="o">*</span> <span class="n">other</span><span class="p">.</span><span class="n">value</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__pow__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="k">return</span> <span class="nc">Dual</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">**</span> <span class="n">n</span><span class="p">,</span> 
                    <span class="n">n</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">**</span> <span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">derivative</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span>

<span class="n">x</span> <span class="o">=</span> <span class="nc">Dual</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>  <span class="c1"># x = 2, dx/dx = 1 (~&gt; \dot{x})
</span><span class="n">result</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">f(2) = </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">value</span><span class="si">}</span><span class="s">, f</span><span class="sh">'</span><span class="s">(2) = </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">derivative</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># &gt;&gt; Output: f(2) = 42.0, f'(2) = 58.0
</span></code></pre></div></div> <p>As demonstrated, Forward autodiff augments each intermediate variable during evaluation of a function <strong>with its derivative</strong>. It involves replacing individual floating point values flowing through a function with <strong>tuples of the original intermediate values also called primals paired with their derivatives</strong>.</p> <p>To compute the partial derivative of a function with respect to an input variable, we have to run <strong>a separate forward pass</strong> for each input variable of interest with corresponding seed set to 1. The forward mode autodiff produces one column of the corresponding <strong>Jacobian</strong> \(J\).</p> <p>For a two dimensional input \(x \in \mathrm{R}^2\), setting \(\dot{x}\) to [1, 0] yields the first column of \(J\) which is the partial derivative w.r.t \(x_1\) and setting it to [0, 1] results in the second column which is the partial derivative w.r.t \(x_2\).</p> <p>Ari Seff does a great job explaining it in his <a href="https://youtu.be/wG_nF1awSSY?t=305"><strong>AutoDiff video here</strong></a>.</p> <h2 id="32-reverse-mode">3.2 Reverse Mode</h2> <p>Reverse mode AutoDiff, which is the main AD method used in current major deep learning frameworks, computes gradients by propagating derivatives by <strong>going backward</strong> through the <a href="https://simple-english-machine-learning.readthedocs.io/en/latest/neural-networks/computational-graphs.html"><strong>computation graph (see [6])</strong></a> starting from the output and then applying the chain rule until it traverses the whole graph. It’s particularly efficient for functions with many inputs and few outputs, which is the typical case in neural networks [3].</p> <p>Reverse mode computes the vector-Jacobian product which is explained PyTorch’s introduction to AtuoDiff in the “<strong>Vector Calculus using autograd</strong>” section <a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html">here</a>.</p> <p>Here’s a simplified implementation of reverse mode AD:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Node</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>
        <span class="n">self</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="bp">None</span> <span class="c1"># this is defined as the forward mode is done based on the computation graph. 
</span>        <span class="n">self</span><span class="p">.</span><span class="n">_prev</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__add__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Node</span><span class="p">)</span> <span class="k">else</span> <span class="nc">Node</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nc">Node</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">+</span> <span class="n">other</span><span class="p">.</span><span class="n">value</span><span class="p">)</span>
        <span class="n">out</span><span class="p">.</span><span class="n">_prev</span> <span class="o">=</span> <span class="p">{</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">}</span>
        <span class="k">def</span> <span class="nf">_backward</span><span class="p">():</span>
            <span class="n">self</span><span class="p">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">out</span><span class="p">.</span><span class="n">grad</span>
            <span class="n">other</span><span class="p">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">out</span><span class="p">.</span><span class="n">grad</span>
        <span class="n">out</span><span class="p">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="n">_backward</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">__mul__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Node</span><span class="p">)</span> <span class="k">else</span> <span class="nc">Node</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nc">Node</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">*</span> <span class="n">other</span><span class="p">.</span><span class="n">value</span><span class="p">)</span>
        <span class="n">out</span><span class="p">.</span><span class="n">_prev</span> <span class="o">=</span> <span class="p">{</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">}</span>
        <span class="k">def</span> <span class="nf">_backward</span><span class="p">():</span>
            <span class="n">self</span><span class="p">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">other</span><span class="p">.</span><span class="n">value</span> <span class="o">*</span> <span class="n">out</span><span class="p">.</span><span class="n">grad</span>
            <span class="n">other</span><span class="p">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">*</span> <span class="n">out</span><span class="p">.</span><span class="n">grad</span>
        <span class="n">out</span><span class="p">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="n">_backward</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">__pow__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nc">Node</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">**</span> <span class="n">n</span><span class="p">)</span>
        <span class="n">out</span><span class="p">.</span><span class="n">_prev</span> <span class="o">=</span> <span class="p">{</span><span class="n">self</span><span class="p">}</span>

        <span class="k">def</span> <span class="nf">_backward</span><span class="p">():</span>
            <span class="n">self</span><span class="p">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">n</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">**</span> <span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">out</span><span class="p">.</span><span class="n">grad</span> 

        <span class="n">out</span><span class="p">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="n">_backward</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">node</span><span class="p">):</span>
    <span class="n">topo</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">visited</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">build_topo</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">v</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">visited</span><span class="p">:</span>
            <span class="n">visited</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">v</span><span class="p">.</span><span class="n">_prev</span><span class="p">:</span>
                <span class="nf">build_topo</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
            <span class="n">topo</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

    <span class="nf">build_topo</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
    <span class="n">node</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="n">topo</span><span class="p">):</span>
        <span class="n">node</span><span class="p">.</span><span class="nf">_backward</span><span class="p">()</span>


<span class="c1"># Example usage
</span><span class="n">x</span> <span class="o">=</span> <span class="nc">Node</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span>

<span class="nf">backward</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">f(2) = </span><span class="si">{</span><span class="n">y</span><span class="p">.</span><span class="n">value</span><span class="si">}</span><span class="s">, f</span><span class="sh">'</span><span class="s">(2) = </span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># &gt;&gt; Output: f(2) = 42.0, f'(2) = 58.0
</span></code></pre></div></div> <p>This implementation builds a computational graph and then traverses it backwards when the backward method is called on the output, to compute gradients, mimicking the process used in deep learning frameworks.</p> <p>The <strong>key difference</strong> in computational complexity between <strong>forward</strong> and <strong>reverse</strong> modes becomes apparent when we consider functions with many inputs (parameters) and few outputs (typically a single loss value in ML), making reverse mode the preferred choice for deep learning [1]. the reason is that, in forward mode, computing the gradient for each input element \(x_i\) requires a <strong>separate</strong> forward pass through the computational graph.</p> <hr/> <h1 id="4-implementation-strategies-operator-overloading-vs-source-transformation">4. Implementation Strategies: Operator Overloading vs. Source Transformation</h1> <h2 id="41-operator-overloading">4.1 Operator Overloading</h2> <p>Operator overloading, as demonstrated in our previous examples, redefines mathematical operations to compute both the result and its derivative. It’s the approach used by PyTorch and many Python-based AD libraries [4].</p> <h2 id="42-source-transformation">4.2 Source Transformation</h2> <p>Source transformation analyzes and modifies the source code to insert derivative computations. While more complex to implement, it can lead to more optimized code, especially for <strong>static</strong> computational graphs [1]. Tools like Tapenade use this approach.</p> <p>Here’s a conceptual example of how source transformation might work:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Original function
</span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span>

<span class="c1"># Transformed function (conceptual, not actual code)
</span><span class="k">def</span> <span class="nf">f_and_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># Forward pass
</span>    <span class="n">t1</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">t2</span> <span class="o">=</span> <span class="n">t1</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">t3</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">t1</span>
    <span class="n">t4</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">t2</span> <span class="o">+</span> <span class="n">t3</span> <span class="o">+</span> <span class="n">t4</span>

    <span class="c1"># Backward pass
</span>    <span class="n">dy</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">dt4</span> <span class="o">=</span> <span class="n">dy</span>
    <span class="n">dt3</span> <span class="o">=</span> <span class="n">dy</span>
    <span class="n">dt2</span> <span class="o">=</span> <span class="n">dy</span>
    <span class="n">dt1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="n">dt3</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">t1</span> <span class="o">*</span> <span class="n">dt2</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">3</span> <span class="o">*</span> <span class="n">dy</span> <span class="o">+</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="n">dy</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">dy</span>

    <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">dx</span>
</code></pre></div></div> <p>This transformed version computes both the function value and its gradient in a single pass through the code. As you can see, it is not as flexible and scalable for large-scale purposes such as in deep learning.</p> <hr/> <h1 id="5-autodiff-in-the-wild-pytorch-vs-jax">5. AutoDiff in the Wild: PyTorch vs. JAX</h1> <h2 id="51-pytorch">5.1 PyTorch</h2> <p>PyTorch uses a dynamic computational graph, built on-the-fly as operations are performed. This allows for flexibility in network architecture and easier debugging [5].</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">f(2) = </span><span class="si">{</span><span class="n">y</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">, f</span><span class="sh">'</span><span class="s">(2) = </span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># Output: f(2) = 42.0, f'(2) = 58.0
</span></code></pre></div></div> <p>PyTorch’s autograd engine records operations in a directed acyclic graph (DAG), where leaves are input tensors and roots are output tensors. During the backward pass, it computes gradients by traversing this graph [5].</p> <p>For very detailed explanation to get a sense of how PyTorch’s autograd works, i would extremely recommend the first to videos of Andrej Karpathy’s <a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ"><strong>Neural Networks: Zero to Hero</strong></a> playlist.</p> <h2 id="52-jax">5.2 JAX</h2> <p>JAX, developed by Google Research, on the the hand uses a static computational graph and leverages XLA (Accelerated Linear Algebra) for efficient compilation to achieve better performance. It provides function transformations like grad for automatic differentiation, vmap for vectorization, and jit for compilation [4].</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>

<span class="nd">@jax.jit</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="mf">2.0</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">f(2) = </span><span class="si">{</span><span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s">, f</span><span class="sh">'</span><span class="s">(2) = </span><span class="si">{</span><span class="nf">df</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># Output: f(2) = 42.0, f'(2) = 58.0
</span>
<span class="c1"># Vectorized computation
</span><span class="n">vdf</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="n">x_vec</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">f</span><span class="sh">'</span><span class="s">(x) for x=[1,2,3]: </span><span class="si">{</span><span class="nf">vdf</span><span class="p">(</span><span class="n">x_vec</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># Output: f'(x) for x=[1,2,3]: [10. 58. 154.]
</span></code></pre></div></div> <p>JAX’s approach allows for efficient compilation and execution on accelerators like GPUs and TPUs [4]. Check out the <a href="https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html"><strong>“The Autodiff Cookbook”</strong></a> from JAX developers for a more technical grasp of their implementations.</p> <p>Note the difference that PyTorch’s implementation requires that first a forward pass is done with an input, then as the backwards are computed, the gradients are accessible, whereas in JAX, the jax.grad transformation can be called on any defined function without the need to calling the function itself manually.</p> <hr/> <h1 id="6-some-advanced-topics-in-autodiff">6. Some Advanced Topics in AutoDiff</h1> <h2 id="61-higher-order-derivatives">6.1 Higher-Order Derivatives</h2> <p>One thing o note is that AutoDiff isn’t limited to first-order derivatives. By applying AD to its own output, we can compute higher-order derivatives. This is crucial for optimization algorithms like Newton’s method that use second-order information (Hessians).</p> <p>In JAX particularly, computing higher-order derivatives is pretty straightforward:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>


<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span>

<span class="n">ddf</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">f</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="mf">2.0</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">f</span><span class="sh">''</span><span class="s">(2) = </span><span class="si">{</span><span class="nf">ddf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># Output: f''(2) = 102.0
</span></code></pre></div></div> <p>just call the grad function transformation twice on your function and you’re good to go.</p> <h2 id="62-vector-jacobian-products-vjps-and-jacobian-vector-products-jvps">6.2 Vector-Jacobian Products (VJPs) and Jacobian-Vector Products (JVPs)</h2> <p>VJPs and JVPs are the building blocks of reverse and forward mode AD, respectively. Understanding these operations is crucial for implementing efficient custom gradients.</p> <p>JAX provides explicit functions for these operations:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>


<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">x</span><span class="o">**</span><span class="mi">3</span><span class="p">])</span>

<span class="n">x</span> <span class="o">=</span> <span class="mf">2.0</span>
<span class="n">y</span><span class="p">,</span> <span class="n">vjp_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">vjp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">VJP: </span><span class="si">{</span><span class="nf">vjp_fn</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]))[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="n">primal</span><span class="p">,</span> <span class="n">jvp_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">jvp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">JVP: </span><span class="si">{</span><span class="n">jvp_fn</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Output:
# VJP: 16.0
# JVP: [4. 12.]
</span></code></pre></div></div> <h2 id="63-ad-through-iterative-processes">6.3 AD through Iterative Processes</h2> <p>Applying AD to iterative processes like optimization loops or recurrent neural networks requires careful handling to avoid excessive memory usage. Techniques like checkpointing and reversible computations are used to balance memory usage and computational cost [1].</p> <hr/> <h1 id="7-the-impact-of-autodiff-on-deep-learning">7. The Impact of AutoDiff on Deep Learning</h1> <p>AutoDiff, particularly <strong>reverse</strong> mode AD, has been instrumental in the deep learning revolution. It allows efficient computation of gradients for millions of parameters with respect to a loss value. This efficiency enables the training of increasingly complex models, driving advancements in areas like natural language processing, computer vision, and reinforcement learning [2].</p> <p>Some key impacts to mention:</p> <ol> <li> <p><strong>Architectural Flexibility</strong>: AD allows researchers to easily experiment with novel network architectures without manually deriving gradients.</p> </li> <li> <p><strong>Computational Efficiency</strong>: Reverse mode AD makes it feasible to train very deep networks with millions or billions of parameters.</p> </li> <li> <p><strong>Higher-Order Optimization</strong>: Easy access to higher-order derivatives enables more sophisticated optimization techniques.</p> </li> <li> <p><strong>Custom Differentiable Operations</strong>: Researchers can define custom differentiable operations, expanding the range of possible model architectures.</p> </li> </ol> <hr/> <h1 id="8-conclusion-and-future-directions">8. Conclusion and Future Directions</h1> <p>Automatic Differentiation has become an indispensable tool in machine learning, enabling the training of increasingly complex models. As we push the boundaries of AI, several exciting directions for AD research emerge:</p> <ol> <li> <p><strong>AD for Probabilistic Programming</strong>: Extending AD to handle probabilistic computations and enable more flexible Bayesian inference.</p> </li> <li> <p><strong>Differentiable Programming</strong>: Moving beyond traditional neural networks to make entire programs differentiable.</p> </li> <li> <p><strong>Hardware-Specific Optimizations</strong>: Tailoring AD algorithms for specialized AI hardware.</p> </li> <li> <p><strong>AD for Sparse and Structured Computations</strong>: Developing efficient AD techniques for sparse or structured problems common in scientific computing.</p> </li> </ol> <p>As these areas develop, we can expect AutoDiff to continue playing a crucial role in advancing the field of machine learning and artificial intelligence.</p> <hr/> <h1 id="references">References</h1> <p>[1] Baydin, A. G., Pearlmutter, B. A., Radul, A. A., &amp; Siskind, J. M. (2018). Automatic differentiation in machine learning: a survey. Journal of Machine Learning Research, 18, 1–43.</p> <p>[2] Grosse, R. (2019). Automatic Differentiation. CSC421/2516 Lecture Notes, University of Toronto.</p> <p>[3] Andmholm. (2023). What is Automatic Differentiation. Hugging Face Blog.</p> <p>[4] JAX Team. (2024). Automatic Differentiation and the JAX Ecosystem. JAX Documentation.</p> <p>[5] PyTorch Team. (2024). Autograd: Automatic Differentiation. PyTorch Tutorials.</p> <p>[6] <a href="https://simple-english-machine-learning.readthedocs.io/en/latest/neural-networks/computational-graphs.html">https://simple-english-machine-learning.readthedocs.io/en/latest/neural-networks/computational-graphs.html</a></p>]]></content><author><name>Ebrahim Pichka</name></author><category term="Deep"/><category term="Learning"/><category term="deep-learning"/><category term="optimization"/><summary type="html"><![CDATA[An introduction to the mechanics of AutoDiff, exploring its mathematical principles, implementation strategies, and applications in currently most-used frameworks.]]></summary></entry><entry><title type="html">Solve Optimization Problems on Google Cloud Platform using Google’s OR API and OR-tools MathOpt</title><link href="https://epichka.com/blog/2024/ortools-mathopt/" rel="alternate" type="text/html" title="Solve Optimization Problems on Google Cloud Platform using Google’s OR API and OR-tools MathOpt"/><published>2024-09-29T12:00:00+00:00</published><updated>2024-09-29T12:00:00+00:00</updated><id>https://epichka.com/blog/2024/ortools-mathopt</id><content type="html" xml:base="https://epichka.com/blog/2024/ortools-mathopt/"><![CDATA[<p>A quick guide to implementing and solving mathematical optimization models using Google Cloud Platform (GCP) with OR-tools MathOpt interface and OR API endpoints in Python.</p> <p><img src="https://cdn-images-1.medium.com/max/2000/1*FbEMA7eGy_ZEjZb6Y06c-w.jpeg" alt="" style="margin:auto; display:block;" class="img-fluid rounded z-depth-1"/></p> <hr/> <h2 id="introduction">Introduction</h2> <p>Operations Research and Mathematical Optimization practitioners and developers often face challenges when dealing with large-scale or even medium-sized optimization problems on their local machines, such as hardware and resource constraints, dependency management, etc.</p> <p>Google’s new <a href="https://developers.google.com/optimization/service"><strong>OR API</strong></a>, part of the OR-tools suite, offers a solution by allowing users to solve <strong>linear</strong>, <strong>mixed-integer</strong>, and <strong>quadratic</strong> programming problems on remote cloud services of GCP. This way we can use Google’s computing infrastructure to handle solving complex models that might be impractical to solve on a single machine due to memory or time constraints.</p> <p><img src="https://cdn-images-1.medium.com/max/2000/1*sK1UjX9ppF0UsQw_QbA1xw.jpeg" alt="" style="margin:auto; display:block;" class="img-fluid rounded z-depth-1"/></p> <p>In this post, we’ll explore how to use this API to formulate and solve optimization problems, focusing on its integration with OR-Tools, Google’s open-source software suite for optimization. We’ll cover the basics of problem formulation, discuss the supported solvers like GLOP, PDLP, and SCIP, and walk through the process of sending models to the cloud service for resolution.</p> <hr/> <h2 id="supported-problems">Supported Problems</h2> <p>Google’s OR API offers two problem-specific endpoints as well as a general endpoint to solve any custom modeled mathematical program.</p> <p>The problem-specific endpoints will not be the main focus of this post. Feel free to dig into their documentation if the suit your needs. These endpoints cover:</p> <ul> <li> <p><a href="https://developers.google.com/optimization/service/scheduling/workforce_scheduling"><strong>Workforce Scheduling</strong></a>: offers two solvers using the <a href="https://developers.google.com/optimization/service/reference/rest/v1/scheduling/solveShiftGeneration">SolveShiftGeneration</a> and <a href="https://developers.google.com/optimization/service/reference/rest/v1/scheduling/solveShiftScheduling">SolveShiftScheduling</a> methods.</p> </li> <li> <p><a href="https://developers.google.com/optimization/service/shipping/network_design"><strong>Shipping Network Design</strong></a>:Solves the Liner Shipping Network Design and Scheduling Problem (LSNDSP). The problem involves the design and scheduling of a liner shipping network that minimizes operational costs, while maximizing revenue from the shipping of commodity demand between ports.</p> </li> </ul> <p>The general endpoint, namely <strong>The MathOpt Service</strong>, uses <a href="https://developers.google.com/optimization/math_opt">MathOpt</a> as input format. <a href="https://developers.google.com/optimization/math_opt">MathOpt</a> is a new feature from the Google OR-tools suite that provides a unified <strong>modelling interface</strong>, that separates modelling and solving stages in math programming. After modelling is done in MathOpt, the model can be coupled up with any different Solvers and methods by just easily changing an ENUM for the solver parameter, for faster and better experiments. We’ll see how it’s done below in examples.</p> <p>In terms of generality, MathOpt models can contain:</p> <ul> <li> <p>Integer or continuous variables</p> </li> <li> <p>Linear or quadratic constraints</p> </li> <li> <p>Linear or quadratic objectives</p> </li> </ul> <p>Models are defined independently of any solver and solvers can be swapped interchangeably. Currently following solvers are supported in MathOpt Service (the cloud endpoint):</p> <ul> <li> <p><a href="https://developers.google.com/optimization/lp/lp_advanced">GLOP</a>: Google’s Glop linear solver. Supports LP with primal and dual simplex methods.</p> </li> <li> <p><a href="https://developers.google.com/optimization/lp/pdlp_math">PDLP</a>: Google’s PDLP solver. Supports LP and convex diagonal quadratic objectives. Uses first order methods rather than simplex. Can solve very large problems.</p> </li> <li> <p><a href="https://developers.google.com/optimization/cp/cp_solver">CP-SAT</a>: Google’s CP-SAT solver. Supports problems where all variables are integer and bounded (or implied to be after presolve).</p> </li> <li> <p><a href="https://www.scipopt.org/">SCIP</a>: Solving Constraint Integer Programs (SCIP) solver (third party). Supports LP, MIP, and nonconvex integer quadratic problems. No dual data for LPs is returned though. Prefer GLOP for LPs.</p> </li> <li> <p><a href="https://www.gnu.org/software/glpk/">GLPK</a>: GNU Linear Programming Kit (GLPK) (third party). Supports MIP and LP.</p> </li> <li> <p><a href="https://osqp.org/">OSQP</a>: The Operator Splitting Quadratic Program (OSQP) solver (third party). Supports continuous problems with linear constraints and linear or convex quadratic objectives. Uses a first-order method.</p> </li> <li> <p><a href="https://highs.dev/">HiGHS</a>: The HiGHS Solver (third party). Supports LP and MIP problems (convex QPs are unimplemented).</p> </li> </ul> <p>Note that MathOpt itself supports a wider range of solver, the above were the one currently supported in the OR API.</p> <hr/> <h2 id="how-to-use-it--examples">How to use it + Examples</h2> <p>In general, the process of solving a model using MathOpt Service has three steps:</p> <ol> <li> <p>Modelling</p> </li> <li> <p>Solver Parameter Setting</p> </li> <li> <p>Making a request to OR API.</p> </li> </ol> <p>But first, you have to follow <a href="https://developers.google.com/optimization/service/setup"><strong>THESE INITIAL STEPS</strong></a> once, to obtain your OR <strong>api_key</strong> from GCP. You also need to have OR-tools installed locally. See how to <a href="https://developers.google.com/optimization/install">install OR-tools</a>.</p> <h3 id="step-1-modelling">Step 1: Modelling</h3> <p>All the modelling is done through the mathopt module. In python, it is imported as follows:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">ortools.math_opt.python</span> <span class="kn">import</span> <span class="n">mathopt</span>
</code></pre></div></div> <p><strong>Example 1:</strong></p> <p>Consider the following Linear Program (LP):</p> <p><img src="https://cdn-images-1.medium.com/max/2000/1*9njAs5wz6DCbF9Qa-j9ytQ.png" alt="" style="margin:auto; display:block;" class="img-fluid rounded z-depth-1"/></p> <p>Then modelling using the mathopt interface is as easy as:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example 1: Simple LP
</span><span class="n">model</span> <span class="o">=</span> <span class="n">mathopt</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">example_lp</span><span class="sh">"</span><span class="p">)</span>
<span class="n">x_1</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">add_variable</span><span class="p">(</span><span class="n">lb</span><span class="o">=-</span><span class="mf">1.</span><span class="p">,</span> <span class="n">ub</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">is_integer</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">x_1</span><span class="sh">"</span><span class="p">)</span>
<span class="n">x_2</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">add_variable</span><span class="p">(</span><span class="n">lb</span><span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">ub</span><span class="o">=</span><span class="mf">3.</span> <span class="p">,</span> <span class="n">is_integer</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">x_2</span><span class="sh">"</span><span class="p">)</span>
<span class="n">x_3</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">add_variable</span><span class="p">(</span><span class="n">lb</span><span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">ub</span><span class="o">=</span><span class="mf">5.</span> <span class="p">,</span> <span class="n">is_integer</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">x_3</span><span class="sh">"</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add_linear_constraint</span><span class="p">(</span><span class="n">x_1</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x_2</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">x_3</span> <span class="o">&lt;=</span> <span class="mf">2.5</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">maximize</span><span class="p">(</span><span class="n">x_1</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x_2</span> <span class="o">-</span> <span class="mi">5</span><span class="o">*</span><span class="n">x_3</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Example 2:</strong></p> <p>Consider a basic <strong><em>N by N</em></strong> <a href="https://en.wikipedia.org/wiki/Assignment_problem">assignment problem</a> with the following integer (binary) linear program:</p> <p><img src="https://cdn-images-1.medium.com/max/2000/1*IlFCaK4jypqKpu2Hd7zr1w.png" alt="" style="margin:auto; display:block;" class="img-fluid rounded z-depth-1"/></p> <p>where decision variable x_ij is 1 if source <strong><em>i</em></strong> is assigned to target <strong><em>j</em></strong>, and 0 otherwise. And the constraints make sure each source is exactly assigned to one target and vice versa. This model is implemented using mathopt as follows:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example 1: Assignment IP
</span>
<span class="c1"># n: number of sources/targets
# W: nxn numpy array of coefficients
</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">mathopt</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">example_assignment</span><span class="sh">"</span><span class="p">)</span>
<span class="n">X_mat</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">[</span><span class="n">model</span><span class="p">.</span><span class="nf">add_binary_variable</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">x_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">j</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
            <span class="c1"># equivalently can use either:
</span>            <span class="c1"># model.add_variable(lb=0., ub=1., is_integer=True, name=f"x_{i}_{j}")
</span>            <span class="c1"># model.add_integer_variable(lb=0., ub=1., name=f"x_{i}_{j}")
</span>      <span class="p">]</span>

<span class="c1"># Objective function
</span><span class="n">obj</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span>
      <span class="n">W</span><span class="p">[</span><span class="n">s</span><span class="p">][</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">X_mat</span><span class="p">[</span><span class="n">s</span><span class="p">][</span><span class="n">t</span><span class="p">]</span>
      <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1"># source indices
</span>      <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1"># target indices
</span>      <span class="p">)</span>
<span class="c1"># Consraintsfunction
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
      <span class="n">model</span><span class="p">.</span><span class="nf">add_linear_constraint</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">X_mat</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">model</span><span class="p">.</span><span class="nf">add_linear_constraint</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">X_mat</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p>That all! Now let’s see how we can solve these models.</p> <h3 id="steps-23-parameter-setting-and-solving">Steps 2–3: Parameter Setting and Solving</h3> <p>In general when we not use the GCP API service, and want to solve a model such as the above mentioned example models locally with mathopt. The models are solved as follows:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">params</span> <span class="o">=</span> <span class="n">mathopt</span><span class="p">.</span><span class="nc">SolveParameters</span><span class="p">(</span><span class="n">enable_output</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">mathopt</span><span class="p">.</span><span class="nf">solve</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mathopt</span><span class="p">.</span><span class="n">SolverType</span><span class="p">.</span><span class="n">GLOP</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">result</span><span class="p">.</span><span class="n">termination</span><span class="p">.</span><span class="n">reason</span> <span class="o">!=</span> <span class="n">mathopt</span><span class="p">.</span><span class="n">TerminationReason</span><span class="p">.</span><span class="n">OPTIMAL</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">model failed to solve: </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">termination</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>As you can see, it does not really matter which model (from example 1 or 2) we want to solve, the process is the same nonetheless. The important point is to select the most appropriate solver for our problem. The solver can be changed using the corresponding ENUM from the following list:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Solvers supported in MathOpt Service (OR API)</span>
<span class="na">GSCIP</span><span class="pi">:</span>     <span class="s">mathopt.SolverType.GSCIP</span>
<span class="na">GLOP</span><span class="pi">:</span>      <span class="s">mathopt.SolverType.GLOP</span>
<span class="na">CP_SAT</span><span class="pi">:</span>    <span class="s">mathopt.SolverType.CP_SAT</span>
<span class="na">PDLP</span><span class="pi">:</span>      <span class="s">mathopt.SolverType.PDLP</span>
<span class="na">GLPK</span><span class="pi">:</span>      <span class="s">mathopt.SolverType.GLPK</span>
<span class="na">OSQP</span><span class="pi">:</span>      <span class="s">mathopt.SolverType.OSQP</span>
<span class="na">HIGHS</span><span class="pi">:</span>     <span class="s">mathopt.SolverType.HIGHS</span>

<span class="c1"># Additional sovers for local use</span>
<span class="na">ECOS</span><span class="pi">:</span>      <span class="s">mathopt.SolverType.ECOS</span>
<span class="na">SCS</span><span class="pi">:</span>       <span class="s">mathopt.SolverType.SCS</span>
<span class="na">GUROBI</span><span class="pi">:</span>    <span class="s">mathopt.SolverType.GUROBI</span>
<span class="na">SANTORINI</span><span class="pi">:</span> <span class="s">mathopt.SolverType.SANTORINI</span>
</code></pre></div></div> <p>For parameters argument params you are able to set the following additional parameters depending on the chosen solver:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">-</span> <span class="na">time_limit</span><span class="pi">:</span> <span class="s">The maximum time a solver should spend on the problem,</span>
<span class="pi">-</span> <span class="na">iteration_limit</span><span class="pi">:</span> <span class="s">Limit on the iterations of the underlying algorithm (e.g.</span>
      <span class="s">simplex pivots).</span> 
<span class="pi">-</span> <span class="na">node_limit</span><span class="pi">:</span> <span class="s">Limit on the number of subproblems solved in enumerative search</span>
      <span class="s">(e.g. branch and bound).</span>
<span class="pi">-</span> <span class="na">cutoff_limit</span><span class="pi">:</span> <span class="s">The solver stops early if it can prove there are no primal</span>
      <span class="s">solutions at least as good as cutoff.</span>
<span class="pi">-</span> <span class="na">objective_limit</span><span class="pi">:</span> <span class="s">The solver stops early as soon as it finds a solution at</span>
      <span class="s">least this good, with TerminationReason.FEASIBLE and Limit.OBJECTIVE.</span>
<span class="pi">-</span> <span class="na">best_bound_limit</span><span class="pi">:</span> <span class="s">The solver stops early as soon as it proves the best bound</span>
      <span class="s">is at least this good, with TerminationReason of FEASIBLE or</span>
      <span class="s">NO_SOLUTION_FOUND and Limit.OBJECTIVE.</span>
<span class="pi">-</span> <span class="na">solution_limit</span><span class="pi">:</span> <span class="s">The solver stops early after finding this many feasible</span>
      <span class="s">solutions, with TerminationReason.FEASIBLE and Limit.SOLUTION. Must be</span>
      <span class="s">greater than zero if set</span>
<span class="pi">-</span> <span class="na">enable_output</span><span class="pi">:</span> <span class="s">If the solver should print out its log messages.</span>
<span class="pi">-</span> <span class="na">absolute_gap_tolerance</span><span class="pi">:</span> <span class="s">An absolute optimality tolerance (primarily) for MIP</span>
      <span class="s">solvers. The absolute GAP is the absolute value of the difference between</span>

<span class="pi">-</span> <span class="na">relative_gap_tolerance</span><span class="pi">:</span> <span class="s">A relative optimality tolerance (primarily) for MIP</span>
      <span class="s">solvers. The relative GAP is a normalized version of the absolute GAP.</span>
<span class="pi">-</span> <span class="na">solution_pool_size</span><span class="pi">:</span> <span class="s">Maintain up to `solution_pool_size` solutions while</span>
      <span class="s">searching.</span>
<span class="pi">-</span> <span class="na">lp_algorithm</span><span class="pi">:</span> <span class="s">The algorithm for solving a linear program. If UNSPECIFIED,</span>
      <span class="s">use the solver default algorithm.</span>
<span class="pi">-</span> <span class="na">presolve</span><span class="pi">:</span> <span class="s">Effort on simplifying the problem before starting the main</span>
      <span class="s">algorithm (e.g. simplex).</span>
<span class="pi">-</span> <span class="na">cuts</span><span class="pi">:</span> <span class="s">Effort on getting a stronger LP relaxation (MIP only). Note that in</span>
      <span class="s">some solvers, disabling cuts may prevent callbacks from having a chance to</span>
      <span class="s">add cuts at MIP_NODE.</span>
<span class="pi">-</span> <span class="na">heuristics</span><span class="pi">:</span> <span class="s">Effort in finding feasible solutions beyond those encountered in</span>
      <span class="s">the complete search procedure.</span>
<span class="pi">-</span> <span class="na">scaling</span><span class="pi">:</span> <span class="s">Effort in rescaling the problem to improve numerical stability.</span>
<span class="pi">-</span> <span class="na">gscip</span><span class="pi">:</span> <span class="s">GSCIP specific solve parameters.</span>
<span class="pi">-</span> <span class="na">gurobi</span><span class="pi">:</span> <span class="s">Gurobi specific solve parameters.</span>
<span class="pi">-</span> <span class="na">glop</span><span class="pi">:</span> <span class="s">Glop specific solve parameters.</span>
<span class="pi">-</span> <span class="na">cp_sat</span><span class="pi">:</span> <span class="s">CP-SAT specific solve parameters.</span>
<span class="pi">-</span> <span class="na">pdlp</span><span class="pi">:</span> <span class="s">PDLP specific solve parameters.</span>
<span class="pi">-</span> <span class="na">osqp</span><span class="pi">:</span> <span class="s">OSQP specific solve parameters.</span> 
<span class="pi">-</span> <span class="na">glpk</span><span class="pi">:</span> <span class="s">GLPK specific solve parameters.</span>
<span class="pi">-</span> <span class="na">highs</span><span class="pi">:</span> <span class="s">HiGHS specific solve parameters.</span>
</code></pre></div></div> <p>You can check out <a href="https://github.com/google/or-tools/blob/stable/ortools/math_opt/python/parameters.py">MathOpt’s source code here</a> for more details on supporting parameters.</p> <p>Now when using the OR API to solve the model remotely on GCP, the procedure differs a little bit. For this, we use the remote_http_solve module from OR-tools, as follows:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">ortools.math_opt.python</span> <span class="kn">import</span> <span class="n">mathopt</span>
<span class="kn">from</span> <span class="n">ortools.math_opt.python.ipc</span> <span class="kn">import</span> <span class="n">remote_http_solve</span>

<span class="c1"># Read the API Key from a JSON file with the format:
# {"key": "your_api_key"}
</span><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">credentials.json</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
      <span class="n">credentials</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
      <span class="n">api_key</span> <span class="o">=</span> <span class="n">credentials</span><span class="p">[</span><span class="sh">"</span><span class="s">key</span><span class="sh">"</span><span class="p">]</span>
      
<span class="n">model</span> <span class="o">=</span> <span class="p">...</span> <span class="c1"># define the model similar to examples 1,2 above
</span>
<span class="k">try</span><span class="p">:</span>
      <span class="c1"># solving remotely on GCP
</span>      <span class="n">result</span><span class="p">,</span> <span class="n">logs</span> <span class="o">=</span> <span class="n">remote_http_solve</span><span class="p">.</span><span class="nf">remote_http_solve</span><span class="p">(</span>
      <span class="n">model</span><span class="p">,</span>
      <span class="n">mathopt</span><span class="p">.</span><span class="n">SolverType</span><span class="p">.</span><span class="n">GSCIP</span><span class="p">,</span>   <span class="c1"># or any solver from the list above
</span>      <span class="n">mathopt</span><span class="p">.</span><span class="nc">SolveParameters</span><span class="p">(</span><span class="n">enable_output</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
      <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span> <span class="c1"># Other additional solving parameters can be set from the above list
</span>      <span class="p">)</span>
      <span class="n">sol_obj</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="nf">objective_value</span><span class="p">()</span>
      <span class="n">sol_var</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="nf">variable_values</span><span class="p">()</span>
      <span class="n">sol_var</span> <span class="o">=</span> <span class="p">{</span><span class="n">var</span><span class="p">.</span><span class="n">name</span><span class="p">:</span> <span class="n">val</span> <span class="k">for</span> <span class="n">var</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">sol_var</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>


      <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Objective value: </span><span class="sh">"</span><span class="p">,</span> <span class="n">sol_obj</span><span class="p">)</span>
      <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Solution: </span><span class="sh">"</span><span class="p">,</span> <span class="n">sol_var</span><span class="p">)</span>
      <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">logs</span><span class="p">))</span>

<span class="k">except</span> <span class="n">remote_http_solve</span><span class="p">.</span><span class="n">OptimizationServiceError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
      <span class="nf">print</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
</code></pre></div></div> <p>That’s it! You solved a model without depending on your machine’s resources completely remotely on GCP.</p> <p>See the complete code examples below:</p> <p><strong>Example 1:</strong></p> <script src="https://gist.github.com/ebrahimpichka/21236defd4f9f8def8f4ca70210d9f6b.js"></script> <p><strong>Example 2: (Assignment problem ILP)</strong></p> <script src="https://gist.github.com/ebrahimpichka/1c676b185151292dcde54b5610e1eea8.js"></script> <p>For more information about Google OR-tools’ new MathOpt module you can also check these resources out:</p> <ul> <li><a href="https://developers.google.com/optimization/math_opt"><strong>OR-Tools | Google for Developers</strong></a></li> <li><a href="https://youtu.be/L5b4YQowXBg"><strong>MathOpt: Solver Independent Modeling in Google’s OR-Tools | Ross Anderson | JuliaCon 2023</strong></a></li> <li><a href="https://github.com/google/or-tools/tree/stable/ortools/math_opt"><strong>Google’s Operations Research tools</strong></a></li> </ul>]]></content><author><name>Ebrahim Pichka</name></author><category term="optimization"/><category term="operations-research"/><category term="optimization"/><category term="OR"/><summary type="html"><![CDATA[A quick guide to implementing and solving mathematical optimization models using Google Cloud Platform (GCP) with OR-tools MathOpt interface and OR API endpoints in Python.]]></summary></entry><entry><title type="html">What is Query, Key, and Value (QKV) in the Transformer Architecture and Why Are They Used?</title><link href="https://epichka.com/blog/2023/qkv-transformer/" rel="alternate" type="text/html" title="What is Query, Key, and Value (QKV) in the Transformer Architecture and Why Are They Used?"/><published>2023-10-04T12:00:00+00:00</published><updated>2023-10-04T12:00:00+00:00</updated><id>https://epichka.com/blog/2023/qkv-transformer</id><content type="html" xml:base="https://epichka.com/blog/2023/qkv-transformer/"><![CDATA[<p><img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*xRE203HLy8MQj5WsOUu92w.png" alt="Image by author — generated by Midjourney" style="margin:auto; display:block;" class="img-fluid rounded z-depth-1"/></p> <hr/> <h1 id="introduction">Introduction</h1> <p>Recent years have seen the Transformer architecture make waves in the field of natural language processing (NLP), achieving state-of-the-art results in a variety of tasks including machine translation, language modeling, and text summarization, as well as other domains of AI i.e. Vision, Speech, RL, etc.</p> <p>Vaswani et al. (2017), first introduced the transformer in their paper <em>“Attention Is All You Need”</em>, in which they used the self-attention mechanism without incorporating recurrent connections while the model can focus selectively on specific portions of input sequences.</p> <p><img src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*gUxLBgWIh5btuQUIuZIr6g.png" alt="The Transformer model architecture — Image from the Vaswani et al. (2017) paper (Source: arXiv:1706.03762v7)" style="margin:auto; display:block;" class="img-fluid rounded z-depth-1"/></p> <p>In particular, previous sequence models, such as recurrent encoder-decoder models, were limited in their ability to capture long-term dependencies and parallel computations. In fact, right before the Transformers paper came out in 2017, state-of-the-art performance in most NLP tasks was obtained by using RNNs with an attention mechanism on top, so attention kind of existed before transformers. By introducing the multi-head attention mechanism on its own, and dropping the RNN part, the transformer architecture resolves these issues by allowing multiple independent attention mechanisms.</p> <p>In this post, we will go over one of the details of this architecture, namely the Query, Key, and Values, and try to make sense of the intuition used behind this part.</p> <p>Note that this post assumes you are already familiar with some basic concepts in NLP and deep learning such as embeddings, Linear (dense) layers, and in general how a simple neural network works.</p> <hr/> <h1 id="attention">Attention!</h1> <p>First, let’s start understanding what the attention mechanism is trying to achieve. And for the sake of simplicity, let’s start with a simple case of sequential data to understand what problem exactly we are going to solve, without going through all the jargon of the <em>attention mechanism</em>.</p> <h2 id="context-matters">Context Matters</h2> <p>Consider the case of smoothing time-series data. Time series are known to be one of the most basic kinds of sequential data due to the fact that it is already in a numerical and structured form, and is usually in low-dimensional space. So it would be suitable to lay out a good starting example.</p> <p>To smooth a highly variant time series, a common technique is to calculate a “weighted average” of the proximate timesteps for each timestep, as shown in image 1, the weights are usually chosen based on how close the proximate timesteps are to our desired timestep. For instance, in Gaussian Smoothing, these weights are drawn from a Gaussian function that is centered at our current step.</p> <p><img src="https://cdn-images-1.medium.com/max/2000/1*8dju9CpYYss-0SLcE4Q1jA.png" alt="time-series smoothing example — Image by author." style="margin:auto; display:block;" class="img-fluid rounded z-depth-1"/></p> <p>What we have done here, in a sense, is that:</p> <ol> <li> <p>We took a sequence of values,</p> </li> <li> <p>And for each step of this sequence, we added (a weighted) context from its proximate values, while the proportion of added context (the weight) is only related to their proximity to the target value.</p> </li> <li> <p>And finally, we attained a new contextualized sequence, which we can understand and analyze more easily.</p> </li> </ol> <p>There are two key points/issues in this example:</p> <ul> <li> <p>It only uses the proximity and ordinal position of the values to obtain the weights of the context.</p> </li> <li> <p>The weights are calculated by fixed arbitrary rules for all points.</p> </li> </ul> <h2 id="the-case-of-language">The Case of Language</h2> <p>In machine learning, textual data always have to be represented by vectors of real-valued numbers AKA Embeddings. So we assume that the primary meanings of tokens (or words) are encoded in these vectors. Now in the case of textual sequence data, if we would like to apply the same kind of technique to contextualize each token of the sequence as the above example so that each token’s new embedding would contain more information about its context, we would encounter some issues which we will discuss now:</p> <p>Firstly, in the example above, we only used the proximity of tokens to determine the importance (weights) of the context to be added, while words do not work like that. In language, the context of a word in a sentence is not based only on the ordinal distance and proximity. We can’t just blindly use proximity to incorporate context from other words.</p> <p>Secondly, adding the context only by taking the (weighted) average of the embeddings of the context tokens itself may not be entirely intuitive. A token’s embedding may contain information about different syntactical, semantical, or lexical aspects of that token. All of this information may not be relevant to the target token to be added. So it’s better not to add all the information as a whole as context.</p> <p>So if we have some (vector) representation of words in a sequence, how do we obtain the weights and the relevant context to re-weight and contextualize each token of the sequence?</p> <blockquote> <p>The answer, in a broad sense, is that we have to “search” for it, based on some specific aspects of the tokens meaning (could be semantic, syntactic, or anything). And during this search, assign the weights and the context information based on relevance or importance.</p> </blockquote> <blockquote> <p>It means that for each of the tokens in a sequence, we have to go through all other tokens in the sequence, and assign them weights and the context information, based on a similarity metric that we use to compare our target token with others. The more similar they are in terms of the desired context, the larger the weight it gets.</p> </blockquote> <p>So, in general, we could say that the attention mechanism is basically (1) assigning weights to and (2) extracting relevant context from other tokens of a sequence based on their relevance or importance to a target token (i.e. attending to them).</p> <p>And we said that in order to find this relevance/importance we need to search through our sequence and compare tokens one-to-one.</p> <p><img src="https://cdn-images-1.medium.com/max/2000/1*QNFUN1sPvBQTWl8JXPu3Qg.png" alt="Searching through the sequence for relevant context to a token for adding — Image by author." style="margin:auto; display:block;" class="img-fluid rounded z-depth-1"/></p> <p>This is where the <em>Query</em>, <em>Key</em>, and <em>Values</em> find meaning.</p> <hr/> <h1 id="query-key-and-value">Query, Key, and Value</h1> <p>To make more sense, think of when you search for something on YouTube, for example. Assume YouTube stores all its videos as a pair of “<em>video title</em>” and the “<em>video file</em>” itself. Which we call a Key-Value pair, with the Key being the video title and the Value being the video itself.</p> <p>The text you put in the search box is called a Query in search terms. So in a sense, when you search for something, YouTube compares your search Query with the Keys of all its videos, then measures the similarity between them, and ranks their Values from the highest similarity down.</p> <p><img src="https://cdn-images-1.medium.com/max/2068/1*znapogqDzbOCzVAYi3QT3g.png" alt="A basic search procedure illustrating the notion of Key, Quey, and Value — Image by author." style="margin:auto; display:block;" class="img-fluid rounded z-depth-1"/></p> <p>In our problem, we have a sequence of token vectors, and we want to search for the weights to re-weight and contextualize each token (word) embedding of the sequence, we can think in terms of:</p> <ol> <li> <p>What you want to look for is the Query.</p> </li> <li> <p>What you are searching among is Key-Value pairs.</p> </li> <li> <p>The query is compared to all the Keys to measure the relevance/importance/similarity.</p> </li> <li> <p>The Values are utilized based on the assigned similarity measure.</p> </li> </ol> <p>Another helpful relevant analogy is a dictionary (or hashmap) data structure. A dictionary stores data in key-value pairs and it maps keys to their respective value pairs. When you try to get a specific value from the dictionary, you have to provide a query to match its corresponding key, then it searches among those keys, compares them with the query, and if matched, the desired value will be returned.</p> <p>However, the difference here is that this is a “hard-matching” case, where the Query either exactly matches the Key or it doesn’t and an in-between similarity is not measured between them.</p> <p><img src="https://cdn-images-1.medium.com/max/2104/1*jZl_kJXZIXoxpcaI8Q0Cjg.png" alt="Dictionary Query matching with Key-Value pairs — Image by author" style="margin:auto; display:block;" class="img-fluid rounded z-depth-1"/></p> <p>We earlier mentioned that we are only working with real-valued vectors (token embeddings). So the Query, Key, and Value also need to be vectors. However, so far we only have one vector for each token which is its embedding vector. So, how should we obtain the Query, Key, and Value vectors?</p> <p>We Construct them using linear projections (linear transformations aka single dense layer with separate sets of weights: Wq, Wₖ, Wᵥ) of the embedding vector of each token. This means we use a learnable vector of weights for each of the Query, Key, and Value to do a linear transformation on the word embedding to obtain the corresponding Query, Key, and Value vectors.</p> <p><img src="https://cdn-images-1.medium.com/max/2000/1*moKYjUdtx-uEyYMbhPWbIw.png" alt="Linear transformation of the word embedding to obtain Query, Key, and Value vectors — Image by author" style="margin:auto; display:block;" class="img-fluid rounded z-depth-1"/></p> <p>An embedding of a token may represent different contextual, structural, and syntactical aspects of that token. By using learnable linear transformation layers to construct these vectors from the token’s embedding, we allow the network to:</p> <ol> <li> <p>Extract and pass a limited specific part of that information into the <em>Q</em>, <em>K</em>, and <em>V</em> vectors.</p> </li> <li> <p>Determine a narrower context in which the search and match is going to be done.</p> </li> <li> <p>Learn what information in an embedding is more important to attend to.</p> </li> </ol> <p>Now, having the <em>Q</em>, <em>K</em>, and <em>V</em> vectors in hand, we are able to perform the “search and compare” procedure that was discussed before, with these vectors. This results in the final derivation of the attention mechanism proposed in the proposed in (Vaswani et al 2017).</p> <p>For each token:</p> <ul> <li> <p>We compare its Query vector to all other tokens’ Key vectors.</p> </li> <li> <p>Calculate a vector similarity score between each two (i.e. the dot-product similarity in the original paper)</p> </li> <li> <p>Transform these similarity scores into weights by scaling them into [0,1] (i.e. Softmax)</p> </li> <li> <p>And add the weighted context by weighting their corresponding value vectors.</p> </li> </ul> <p><img src="https://cdn-images-1.medium.com/max/2268/1*6BEwO4jKy9UC7AzU6nIPPg.png" alt="Dot-product attention procedure —Image by author" style="margin:auto; display:block;" class="img-fluid rounded z-depth-1"/></p> <p>So the whole notion of the <em>Q</em>, <em>K</em>, and <em>V</em> vectors is like a soft dictionary to mimic a <em>search-and-match procedure</em> from which we learn how much two tokens in a sequence are relevant (the weights), and what should be added as the context (the values). Also, note that this process does not have to happen sequentially (one token at a time). This all happens in parallel by using matrix operations.</p> <p>Note that in the illustration below, the matrix dimensions are switched compared to that of the original paper (<em>n_tokens</em> by <em>dim</em> instead of <em>dim</em> by <em>n_tokens</em>). Later in this post, you will see the original and complete formulation of the attention mechanism which is the other way around.</p> <p><img src="https://cdn-images-1.medium.com/max/2064/1*JWcDjbb3V1TJxsXlpcNfcA.png" alt="Matrix form of the dot-product attention — Image by author" style="margin:auto; display:block;" class="img-fluid rounded z-depth-1"/></p> <p>This results in a more context-aware embedding of each token, where the added context is based on the relevance of the tokens to each other and it is learned through <em>Q</em>, <em>K</em>, <em>V</em> vector transformation. Hence, the dot-product attention mechanism. The original attention mechanism in (Vaswani et al, 2017) also scales the dot-product of <em>K</em> and <em>Q</em> vectors, meaning it divides the resulting vector by <em>sqrt(d)</em>, where <em>d</em> is the dimension of the Query vector. Hence the name, <em>“scaled dot-product attention”</em>. This scaling helps with reducing the variance of the dot-product before being passed to the Softmax function:</p> <p><img src="https://cdn-images-1.medium.com/max/2000/1*kxcBAMTR6bd4K4PaaTsIDA.png" alt="Attention mechanism formula — Vaswani et al. (2017) (Source: arXiv:1706.03762v7)" style="margin:auto; display:block;" class="img-fluid rounded z-depth-1"/></p> <p>Finally, we mentioned that the linear layers that transform the embedding into <em>Q</em>, <em>K</em>, <em>V</em>, may extract only a specific pattern in the embedding for finding the attention weights. To enable the model to learn different complex relations between the sequence tokens, create and use multiple different versions of these <em>Q</em>, <em>K</em>, <em>V</em>, so that each will focus on different patterns existing in our embeddings. These multiple versions are called attention heads resulting in the name “multi-head attention”. These heads can also be vectorized and computed in parallel using current popular deep learning frameworks.</p> <p><img src="https://cdn-images-1.medium.com/max/2000/1*sQP6cxjpXZ_lxDFYYe9Vdw.png" alt="Multi-head scaled dot-product attention — Image from the Vaswani et al. (2017) paper (Source: arXiv:1706.03762v7)" style="margin:auto; display:block;" class="img-fluid rounded z-depth-1"/></p> <hr/> <h1 id="conclusion">Conclusion</h1> <p>So to wrap up, in this post I tried to picture and analyze the intuition behind the use of Query, Key, and Value which are key components in the attention mechanism and may be a little difficult to make sense of, at first encounters.</p> <p>The attention mechanism discussed in this post was proposed in the transformer architecture that is introduced in the (Vaswani et al, 2017) paper “Attention is all you need” and has been one of the top-performing architectures since, in several different tasks and benchmarks in deep learning. With its vast use cases and applicability, it would be helpful to have an understanding of the intuition behind the nuts and bolts used in this architecture and know why we use it.</p> <p>I attempted to be as clear and as basic as possible while explaining this topic by laying down examples and illustrations wherever possible.</p> <h1 id="references">References</h1> <p>[1] Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “<em>Attention Is All You Need.</em>” arXiv, August 1, 2023. <a href="https://doi.org/10.48550/arXiv.1706.03762.">https://doi.org/10.48550/arXiv.1706.03762</a>.</p> <p><br/></p> <font color="gray" face="Times New Roman" size="1"> Author: <strong>Ebrahim Pichka</strong> </font>]]></content><author><name>Ebrahim Pichka</name></author><category term="transformers"/><category term="NLP"/><category term="transformers"/><summary type="html"><![CDATA[An analysis of the intuition behind the notion of Key, Query, and Value in the Transformer architecture and why is it used.]]></summary></entry><entry><title type="html">Graph Attention Networks Paper Explained With Illustration and PyTorch Implementation</title><link href="https://epichka.com/blog/2023/gat-paper-explained/" rel="alternate" type="text/html" title="Graph Attention Networks Paper Explained With Illustration and PyTorch Implementation"/><published>2023-07-26T14:57:00+00:00</published><updated>2023-07-26T14:57:00+00:00</updated><id>https://epichka.com/blog/2023/gat-paper-explained</id><content type="html" xml:base="https://epichka.com/blog/2023/gat-paper-explained/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/4240/1*JeY2ChpCHoH84dyJ-Ugu3Q.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="introduction">Introduction</h1> <p>Graph neural networks (GNNs) are a powerful class of neural networks that operate on graph-structured data. They learn node representations (embeddings) by aggregating information from a node’s local neighborhood. This concept is known as <strong><em>‘message passing’</em></strong> in the graph representation learning literature.</p> <p>Messages (embeddings) are passed between nodes in the graph through multiple layers of the GNN. Each node <strong>aggregates</strong> the messages from its <strong>neighbors</strong> to <strong>update</strong> its representation. This process is repeated across layers, allowing nodes to obtain representations that encode richer information about the graph. Some of the important variants of GNNs can are GraphSAGE [2], Graph Convolution Network [3], etc. You can explore more GNN variants <a href="https://paperswithcode.com/methods/category/graph-models">here</a>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*sJ01stdUwds-YN4-5SYiyQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Graph Attention Networks (GAT)</strong>[1] are a special class of GNNs that were proposed to improve upon this message-passing scheme. They introduced a learnable <strong>attention mechanism</strong> that enables a node to decide which neighbor nodes are more important when aggregating messages from their local neighborhood by assigning a weight between each source and target node instead of aggregating information from all neighbors with equal weights.</p> <p>Empirically, Graph Attention Networks have been shown to outperform many other GNN models on tasks such as node classification, link prediction, and graph classification. They demonstrated state-of-the-art performance on several benchmark graph datasets.</p> <p>In this post, we will walk through the crucial part of the original “Graph Attention Networks” paper by Veličković et al. [1], explain these parts, and simultaneously implement the notions proposed in the paper using PyTorch framework to better grasp the intuition of the GAT method.</p> <p>You can also access the full code used in this post, containing the training and validation code in <a href="https://github.com/ebrahimpichka/GAT-pt">this GitHub repository</a></p> <h1 id="going-through-the-paper">Going Through the Paper</h1> <h2 id="section-1---introduction">Section 1 - Introduction</h2> <p>After broadly reviewing the existing methods in the graph representation learning literature in Section 1, “<em>Introduction</em>”, the Graph Attention Network (GAT) is introduced. The authors mention:</p> <ol> <li> <p>An overall view of the incorporated attention mechanism.</p> </li> <li> <p>Three properties of GATs, namely efficient computation, general applicability to all nodes, and usability in <strong>inductive learning</strong>.</p> </li> <li> <p>Benchmarks and Datasets on which they evaluated the GAT’s performance.</p> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/3394/1*fsN-_tEJoW-3llxs34xxRQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Then After comparing their approach to some existing methods and mentioning the general similarities and differences between them, they move forward to the next section of the paper.</p> <h2 id="section-2---gat-architecture">Section 2 - GAT Architecture</h2> <p>In this section, which accounts for the main part of the paper, the Graph Attention Network architecture is laid out in detail. To move forward with the explanation, assume the proposed architecture performs on a graph with <strong><em>N nodes (V = {vᵢ}; i=1,…,N)</em></strong> and each node is represented with a <strong>vector hᵢ</strong> of <strong>F elements</strong>, With any arbitrary setting of edges existing between nodes.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*HTBen2imL_rH-j0unv-LpQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The authors first start by characterizing a single <strong>Graph Attention Layer</strong>, and how it operates, which becomes the building blocks of a Graph Attention Network. In general, a single GAT layer is supposed to take a graph with its given node embeddings (representations) as input, propagate information to local neighbor nodes, and output an updated representation of nodes.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*bZu6PYombELP47kEkF5pEg.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>As highlighted above, to do so, first, they state that all the input node feature vectors (<strong><em>hᵢ</em></strong>) to the GA-layer are linearly transformed (i.e. multiplied by <strong>a weight matrix <em>W</em></strong>), in PyTorch, it is generally done as follows:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2288/1*iUfoc0v7-Nf5wMuv25RzfQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># in_features -&gt; F and out_feature -&gt; F'
</span><span class="n">in_features</span> <span class="o">=</span> <span class="bp">...</span>
<span class="n">out_feature</span> <span class="o">=</span> <span class="bp">...</span>

<span class="c1"># instanciate the learnable weight matrix W (FxF')
</span><span class="n">W</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_feature</span><span class="p">)))</span>

<span class="c1">#  Initialize the weight matrix W
</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_normal_</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>

<span class="c1"># multiply W and h (h is input features of all the nodes -&gt; NxF matrix)
</span><span class="n">h_transformed</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mm</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
</code></pre></div></div> <p>Now having in mind that we obtained a transformed version of our input node features (embeddings), we jump a few steps forward to observe and understand what is our final objective in a GAT layer.</p> <p>As described in the paper, at the end of a graph attention layer, <strong>for each node <em>i</em></strong>, we need to obtain a new feature vector that is more structure- and context-aware from its neighborhood.</p> <p>This is done by calculating a <strong>weighted sum</strong> of neighboring node features followed by a non-linear activation function <em>σ</em>. This weighted sum is also known as the ‘Aggregation Step’ in the general GNN layer operations, according to Graph ML literature.</p> <p>These <strong>weights <em>αᵢⱼ</em></strong> ∈ [0, 1] are <strong>learned</strong> and computed by an attention mechanism that <strong>denotes the importance</strong> of the <strong>neighbor <em>j</em></strong> features for <strong>node <em>i</em></strong> during message passing and aggregation.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2204/1*1VOm2GtHtIFHk9N-mc2dEg.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2576/1*nMAaVyiVh_awOiu7iocyUA.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Now let’s see how these attention <strong>weights <em>αᵢⱼ</em></strong> are computed for each pair of node <em>i</em> and its neighbor <em>j</em>:</p> <p>In short, attention <strong>weights <em>αᵢⱼ</em></strong> are calculated as below</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*2tU3NKNVke4ytwVOCocgyQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>where the <strong><em>eᵢⱼ</em></strong> are <strong><em>attention scores</em></strong> and the Softmax function is applied so that all the weights are in the [0, 1] interval and sum to 1.</p> <p>The attention <strong>scores <em>eᵢⱼ</em></strong> are now calculated between <strong>each node <em>i</em></strong> and its <strong>neighbors <em>j</em> ∈ <em>Nᵢ</em></strong> through the attention function <strong><em>a</em>(…)</strong> as such:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/3478/1*0-A9rQ5r7zOZKHxjq0cZGA.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Where <strong>||</strong> denotes the <strong>concatenation</strong> of two transformed node embeddings, and <strong>a</strong> is a vector of <strong>learnable</strong> parameters (i.e., <strong>attention parameters</strong>) of the size <em>2 x F’</em> (twice the size of transformed embeddings).</p> <p>And the <strong>(aᵀ)</strong> is the <strong>transpose</strong> of the vector <strong>a</strong>, resulting in the whole expression <strong>aᵀ [Whᵢ || Whⱼ]</strong> being the <strong>dot (inner) product</strong> between “a” and the concatenation of transformed embeddings.</p> <p>The whole operation is illustrated below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/3640/1*MY09NqbYWf-AmemC5YlmCA.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In PyTorch, to achieve these scores, we take a slightly different approach. Because it is more efficient to compute <strong><em>eᵢⱼ</em></strong> between <strong>all pairs of nodes</strong> and then select only those which represent existing edges between nodes. To calculate all <strong><em>eᵢⱼ</em></strong>:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># instanciate the learnable attention parameter vector `a`
</span><span class="n">a</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">out_feature</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

<span class="c1"># Initialize the parameter vector `a`
</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_normal_</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="c1"># we obtained `h_transformed` in the previous code snippet
</span>
<span class="c1"># calculating the dot product of all node embeddings
# and first half the attention vector parameters (corresponding to neighbor messages)
</span><span class="n">source_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">h_transformed</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">a</span><span class="p">[:</span><span class="n">out_feature</span><span class="p">,</span> <span class="p">:])</span>

<span class="c1"># calculating the dot product of all node embeddings
# and second half the attention vector parameters (corresponding to target node)
</span><span class="n">target_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">h_transformed</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">a</span><span class="p">[</span><span class="n">out_feature</span><span class="p">:,</span> <span class="p">:])</span>

<span class="c1"># broadcast add 
</span><span class="n">e</span> <span class="o">=</span> <span class="n">source_scores</span> <span class="o">+</span> <span class="n">target_scores</span><span class="p">.</span><span class="n">T</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">leakyrelu</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</code></pre></div></div> <p>The last part of the code snippet (<em># broadcast add</em>) adds all the one-to-one source and target scores, resulting in an <em>N</em>x<em>N</em> matrix containing all the <strong><em>eᵢⱼ</em></strong> scores. (illustrated below)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2348/1*vnNQAJRpMuF9wRZB9bWnig.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>So far, it’s like we assumed the graph is fully-connected and calculated the attention scores between all possible pair of nodes. To address this, after the LeakyReLU activation is applied to the attention scores, the attention scores are masked based on existing edges in the graph, meaning we only keep the scores that correspond to existing edges.</p> <p>It can be done by assigning a <strong>large negative score</strong> (to approximate -∞) to elements in the scores matrix between nodes with non-existing edges so their corresponding attention weights <strong>become zero after softmax</strong>.</p> <p>We can achieve this by using the <strong>adjacency matrix</strong> of the graph. The adjacency matrix is an NxN matrix with 1 at row <em>i</em> and column <em>j</em> if there is an edge between node <em>i</em> and <em>j</em> and 0 elsewhere. So we create the mask by assigning -∞ to zero elements of the adjacency matrix and assigning 0 elsewhere. And then, we add the mask to our <strong>scores</strong> matrix. and apply the softmax function across its rows.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">connectivity_mask</span> <span class="o">=</span> <span class="o">-</span><span class="mf">9e16</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
<span class="c1"># adj_mat is the N by N adjacency matrix
</span><span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">adj_mat</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">connectivity_mask</span><span class="p">)</span> <span class="c1"># masked attention scores
</span>        
<span class="c1"># attention coefficients are computed as a softmax over the rows
# for each column j in the attention score matrix e
</span><span class="n">attention</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p>Finally, according to the paper, after obtaining the attention scores and masking them with the existing edges, we get the attention <strong>weights <em>αᵢⱼ</em></strong> by performing softmax over the rows of the scores matrix.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/3012/1*bQhdgq1YCn5ctD-BcJoUPg.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/6924/1*2VuuJFxLxdGKYTAqRf6D1w.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>And as discussed before, we calculate the weighted sum of the node embeddings:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># final node embeddings are computed as a weighted average of the features of its neighbors
</span><span class="n">h_prime</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">h_transformed</span><span class="p">)</span>
</code></pre></div></div> <p>Finally, the paper introduces the notion of <strong>multi-head attention</strong>, where the whole discussed operations are done through multiple parallel streams of operations, where the final result heads are either averaged or concatenated.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/3154/1*myPSu2-HL2ycDkoLYDUEng.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The multi-head attention and aggregation process is illustrated below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*UfgWhR9FVvwesBvL6Tp_ng.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>To wrap up the implementation in a cleaner modular form (as a PyTorch module) and <strong>to incorporate the multi-head attention</strong> functionality, the whole Graph Attention Layer implementation is done as follows:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="c1">################################
###  GAT LAYER DEFINITION    ###
################################
</span>
<span class="k">class</span> <span class="nc">GraphAttentionLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">concat</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.4</span><span class="p">,</span>
                    <span class="n">leaky_relu_slope</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">GraphAttentionLayer</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span> <span class="c1"># Number of attention heads
</span>        <span class="n">self</span><span class="p">.</span><span class="n">concat</span> <span class="o">=</span> <span class="n">concat</span> <span class="c1"># wether to concatenate the final attention heads
</span>        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span> <span class="c1"># Dropout rate
</span>
        <span class="k">if</span> <span class="n">concat</span><span class="p">:</span> <span class="c1"># concatenating the attention heads
</span>            <span class="n">self</span><span class="p">.</span><span class="n">out_features</span> <span class="o">=</span> <span class="n">out_features</span> <span class="c1"># Number of output features per node
</span>            <span class="k">assert</span> <span class="n">out_features</span> <span class="o">%</span> <span class="n">n_heads</span> <span class="o">==</span> <span class="mi">0</span> <span class="c1"># Ensure that out_features is a multiple of n_heads
</span>            <span class="n">self</span><span class="p">.</span><span class="n">n_hidden</span> <span class="o">=</span> <span class="n">out_features</span> <span class="o">//</span> <span class="n">n_heads</span>
        <span class="k">else</span><span class="p">:</span> <span class="c1"># averaging output over the attention heads (Used in the main paper)
</span>            <span class="n">self</span><span class="p">.</span><span class="n">n_hidden</span> <span class="o">=</span> <span class="n">out_features</span>

        <span class="c1">#  A shared linear transformation, parametrized by a weight matrix W is applied to every node
</span>        <span class="c1">#  Initialize the weight matrix W 
</span>        <span class="n">self</span><span class="p">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_hidden</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">)))</span>

        <span class="c1"># Initialize the attention weights a
</span>        <span class="n">self</span><span class="p">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_heads</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">n_hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

        <span class="n">self</span><span class="p">.</span><span class="n">leakyrelu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LeakyReLU</span><span class="p">(</span><span class="n">leaky_relu_slope</span><span class="p">)</span> <span class="c1"># LeakyReLU activation function
</span>        <span class="n">self</span><span class="p">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># softmax activation function to the attention coefficients
</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">reset_parameters</span><span class="p">()</span> <span class="c1"># Reset the parameters
</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>

        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_normal_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_normal_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">a</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_attention_scores</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">h_transformed</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
        
        <span class="n">source_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">h_transformed</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">a</span><span class="p">[:,</span> <span class="p">:</span><span class="n">self</span><span class="p">.</span><span class="n">n_hidden</span><span class="p">,</span> <span class="p">:])</span>
        <span class="n">target_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">h_transformed</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">a</span><span class="p">[:,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_hidden</span><span class="p">:,</span> <span class="p">:])</span>

        <span class="c1"># broadcast add 
</span>        <span class="c1"># (n_heads, n_nodes, 1) + (n_heads, 1, n_nodes) = (n_heads, n_nodes, n_nodes)
</span>        <span class="n">e</span> <span class="o">=</span> <span class="n">source_scores</span> <span class="o">+</span> <span class="n">target_scores</span><span class="p">.</span><span class="n">mT</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">leakyrelu</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span>  <span class="n">h</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">adj_mat</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>

        <span class="n">n_nodes</span> <span class="o">=</span> <span class="n">h</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Apply linear transformation to node feature -&gt; W h
</span>        <span class="c1"># output shape (n_nodes, n_hidden * n_heads)
</span>        <span class="n">h_transformed</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mm</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="p">)</span>
        <span class="n">h_transformed</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">h_transformed</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>

        <span class="c1"># splitting the heads by reshaping the tensor and putting heads dim first
</span>        <span class="c1"># output shape (n_heads, n_nodes, n_hidden)
</span>        <span class="n">h_transformed</span> <span class="o">=</span> <span class="n">h_transformed</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">n_nodes</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_hidden</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># getting the attention scores
</span>        <span class="c1"># output shape (n_heads, n_nodes, n_nodes)
</span>        <span class="n">e</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_get_attention_scores</span><span class="p">(</span><span class="n">h_transformed</span><span class="p">)</span>

        <span class="c1"># Set the attention score for non-existent edges to -9e15 (MASKING NON-EXISTENT EDGES)
</span>        <span class="n">connectivity_mask</span> <span class="o">=</span> <span class="o">-</span><span class="mf">9e16</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
        <span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">adj_mat</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">connectivity_mask</span><span class="p">)</span> <span class="c1"># masked attention scores
</span>        
        <span class="c1"># attention coefficients are computed as a softmax over the rows
</span>        <span class="c1"># for each column j in the attention score matrix e
</span>        <span class="n">attention</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>

        <span class="c1"># final node embeddings are computed as a weighted average of the features of its neighbors
</span>        <span class="n">h_prime</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">h_transformed</span><span class="p">)</span>

        <span class="c1"># concatenating/averaging the attention heads
</span>        <span class="c1"># output shape (n_nodes, out_features)
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">concat</span><span class="p">:</span>
            <span class="n">h_prime</span> <span class="o">=</span> <span class="n">h_prime</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="n">n_nodes</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">out_features</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">h_prime</span> <span class="o">=</span> <span class="n">h_prime</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">h_prime</span>
</code></pre></div></div> <p>Next, the authors do a comparison between GATs and some of the other existing GNN methodologies/architectures. They argue that:</p> <ol> <li> <p>GATs are computationally more efficient than some existing methods due to being able to compute attention weights and perform the local aggregation <strong>in parallel</strong>.</p> </li> <li> <p>GATs can assign <strong>different importance</strong> to neighbors of a node while aggregating messages which can enable a leap in model capacity and increase interpretability.</p> </li> <li> <p>GAT does consider the complete neighborhood of nodes (does not require sampling from neighbors) and it does not assume any ordering within nodes.</p> </li> <li> <p>GAT can be reformulated as a particular instance of MoNet (Monti et al., 2016) by setting the pseudo-coordinate function to be u(<em>x, y</em>) = f(<em>x</em>)<strong>||</strong>f(<em>y</em>), where f(<em>x</em>) represents (potentially MLP-transformed) features of node <em>x</em> and <strong>||</strong> is concatenation; and the weight function to be wj(<em>u</em>) = softmax(MLP(<em>u</em>))</p> </li> </ol> <h2 id="section-3---evaluation">Section 3 - Evaluation</h2> <p>In the third section of the paper, first, the authors describe the benchmarks, datasets, and tasks on which the GAT is evaluated. Then they present the results of their evaluation of the model.</p> <h3 id="transductive-learning-vs-inductive-learning">Transductive learning vs. Inductive learning</h3> <p>Datasets used as benchmarks in this paper are differentiated into two types of tasks: <strong>Transductive and Inductive.</strong></p> <ul> <li> <p><strong>Inductive learning:</strong> It is a type of supervised learning task in which a model is trained only on a set of labeled training examples and the trained model is evaluated and tested on examples that were completely unobserved during training. It is the type of learning which is known as common supervised learning.</p> </li> <li> <p><strong>Transductive learning:</strong> In this type of task, all the data, including training, validation, and test instances, are used during training. But in each phase, only the corresponding set of labels is accessed by the model. Meaning during training, the model is only trained using the <strong>loss</strong> that is resulted from the training instances and labels, but the test and validation features are used for the message passing. It is mostly because of the structural and contextual information existing in the examples.</p> </li> </ul> <h3 id="datasets">Datasets</h3> <p>In the paper, four benchmark datasets are used to evaluate GATs, three of which correspond to transductive learning, and one other is used as an inductive learning task.</p> <p>The transductive learning datasets, namely <strong>Cora</strong>, <strong>Citeseer</strong>, and <strong>Pubmed</strong> (Sen et al., 2008) datasets are all <strong>citation graphs</strong> in which nodes are published documents and edges (connections) are citations among them, and the node features are elements of a bag-of-words representation of a document. The inductive learning dataset is a <strong>protein-protein interaction (PPI)</strong> dataset containing graphs are different <strong>human tissues</strong> (Zitnik &amp; Leskovec, 2017). Datasets are described more below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/3124/1*Hqw73vD_hs8jF3Lil1JvvA.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="setup--results">Setup &amp; Results</h3> <p>For the three transductive tasks, the setting used for training is as follows:</p> <p>They use 2 GAT layers</p> <ul> <li>first layer uses: <ul> <li><strong>K = 8</strong> attention heads</li> <li><strong>F’ = 8</strong> output feature dim per head</li> <li><strong>ELU</strong> activation</li> </ul> </li> <li>and for the second layer [<strong>Cora &amp; Citeseer | Pubmed</strong>] <ul> <li><strong>[1 | 8] attention head</strong> with <strong>C number of classes</strong> output dim</li> <li><strong>Softmax</strong> activation for classification probability output</li> </ul> </li> <li>and for the overall network <ul> <li><strong>Dropout</strong> with <strong>p = 0.6</strong></li> <li><strong>L2</strong> regularization with <strong>λ = [0.0005 | 0.001]</strong></li> </ul> </li> </ul> <p>For the three transductive tasks, the setting used for training is:</p> <ul> <li>Three layers — <ul> <li>Layer 1 &amp; 2: <strong>K = 4</strong> | <strong>F’ = 256</strong> | <strong>ELU</strong></li> <li>Layer 3: <strong>K = 6</strong> | <strong>F’ = C classes</strong> | <strong>Sigmoid (multi-label)</strong></li> <li>with <strong>no regularization and dropout</strong></li> </ul> </li> </ul> <p>The first setting’s implementation in PyTorch is done below using the layer we defined earlier:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GAT</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span>
        <span class="n">in_features</span><span class="p">,</span>
        <span class="n">n_hidden</span><span class="p">,</span>
        <span class="n">n_heads</span><span class="p">,</span>
        <span class="n">num_classes</span><span class="p">,</span>
        <span class="n">concat</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span>
        <span class="n">leaky_relu_slope</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>

        <span class="nf">super</span><span class="p">(</span><span class="n">GAT</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="c1"># Define the Graph Attention layers
</span>        <span class="n">self</span><span class="p">.</span><span class="n">gat1</span> <span class="o">=</span> <span class="nc">GraphAttentionLayer</span><span class="p">(</span>
            <span class="n">in_features</span><span class="o">=</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="n">n_heads</span><span class="p">,</span>
            <span class="n">concat</span><span class="o">=</span><span class="n">concat</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">leaky_relu_slope</span><span class="o">=</span><span class="n">leaky_relu_slope</span>
            <span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">gat2</span> <span class="o">=</span> <span class="nc">GraphAttentionLayer</span><span class="p">(</span>
            <span class="n">in_features</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">concat</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">leaky_relu_slope</span><span class="o">=</span><span class="n">leaky_relu_slope</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span> <span class="p">,</span> <span class="n">adj_mat</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>


        <span class="c1"># Apply the first Graph Attention layer
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">gat1</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">adj_mat</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">elu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># Apply ELU activation function to the output of the first layer
</span>
        <span class="c1"># Apply the second Graph Attention layer
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">gat2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">adj_mat</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Apply softmax activation function
</span></code></pre></div></div> <p>After testing, the authors report the following performance for the four benchmarks showing the comparable results of GATs compared to existing GNN methods.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2814/1*1Z8MPnC6oTL4vEhTodnUng.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2974/1*ZoSLk01leYeh6TlGWTbjOg.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="conclusion">Conclusion</h1> <p>To conclude, in this blog post, I tried to take a detailed and easy-to-follow approach in explaining the “Graph Attention Networks” paper by Veličković et al. by using illustrations to help readers understand the main ideas behind these networks and why they are important for working with complex graph-structured data (e.g., social networks or molecules). Additionally, the post includes a practical implementation of the model using PyTorch, a popular programming framework. By going through the blog post and trying out the code, I hope readers can gain a solid understanding of how GATs work and how they can be applied in real-world scenarios. I hope this post has been helpful and encouraging to explore this exciting area of research further.</p> <p>Plus, you can access the full code used in this post, containing the training and validation code in <a href="https://github.com/ebrahimpichka/GAT-pt">this GitHub repository</a>.</p> <p>I’d be happy to hear any thoughts or any suggestions/changes on the post.</p> <h1 id="references">References</h1> <p>[1] — <strong>Graph Attention Networks (2017)</strong>, <em>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio</em>. <a href="https://arxiv.org/abs/1710.10903v3">arXiv:1710.10903v3</a></p> <p>[2] — <strong>Inductive Representation Learning on Large Graphs</strong> <strong>(2017)</strong>, <em>William L. Hamilton, Rex Ying, Jure Leskovec</em>. <a href="https://arxiv.org/abs/1706.02216v4">arXiv:1706.02216v4</a></p> <p>[3] — <strong>Semi-Supervised Classification with Graph Convolutional Networks (2016),</strong> <em>Thomas N. Kipf, Max Welling</em>. <a href="https://arxiv.org/abs/1609.02907v4">arXiv:1609.02907v4</a></p> <p><br/></p> <font color="gray" face="Times New Roman" size="1"> Author: <strong>Ebrahim Pichka</strong> </font>]]></content><author><name>Ebrahim Pichka</name></author><category term="graph-representation-learning"/><category term="graph-neural-networks"/><category term="GNN"/><category term="GAT"/><summary type="html"><![CDATA[A detailed and illustrated walkthrough of the “Graph Attention Networks” paper by Veličković et al. with the PyTorch implementation of the proposed model.]]></summary></entry><entry><title type="html">Policy Gradient Algorithm’s Mathematics Explained with PyTorch Implementation</title><link href="https://epichka.com/blog/2023/pg-math-explained/" rel="alternate" type="text/html" title="Policy Gradient Algorithm’s Mathematics Explained with PyTorch Implementation"/><published>2023-05-23T12:00:00+00:00</published><updated>2023-05-23T12:00:00+00:00</updated><id>https://epichka.com/blog/2023/pg-math-explained</id><content type="html" xml:base="https://epichka.com/blog/2023/pg-math-explained/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2048/0*VgbxFJr_l6SmL1H-.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Table of Content</strong></p> <ul> <li><strong><a href="#introduction">Introduction</a></strong></li> <li><strong><a href="#policy-gradient-method">Policy Gradient Method</a></strong> <ul> <li><a href="#derivation">Derivation</a></li> <li><a href="#optimization">Optimization</a></li> <li><a href="#the-algorithm">The Algorithm</a></li> </ul> </li> <li><strong><a href="#pyTorch-implementation">PyTorch Implementation</a></strong> <ul> <li><a href="#networks">Networks</a></li> <li><a href="#training-loop-main-algorithm">Training Loop (Main algorithm)</a></li> <li><a href="#training-Results">Training Results</a></li> </ul> </li> <li><strong><a href="#conclusion">Conclusion</a></strong></li> <li><strong><a href="#references">References</a></strong></li> </ul> <hr/> <h2 id="introduction">Introduction</h2> <p>Reinforcement Learning (RL) is a subdomain of AI that aims to enable machines to learn and improve their behavior by interacting with an environment and receiving feedback in the form of reward signals. This environment is mathematically formulated as a Markov Decision Process (MDP) where at each timestep, the agent is known to be at a certain state <strong>(s ∈ <em>S</em>)</strong> where it is able to take action <strong>(a ∈ <em>A</em>)</strong>. This action results in a transition from state <strong>s</strong> to a new state <strong>(s’ ∈ <em>S)</em></strong> with a certain probability from a dynamics function <strong><em>P(s, a, s’)</em></strong> and receiving a scalar reward <strong>r</strong> from a reward function <strong><em>R(s, a, s’)</em></strong>. That said, MDPs can be shown by a tuple of sets <strong>(<em>S, A, P, R, γ</em>)</strong> in which γ ∈ (0, 1] is a discount factor for future steps’ rewards.</p> <p>RL algorithms can be generally categorized into two groups i.e., value-based and policy-based methods. Value-based methods aim at estimating the expected return of the states and selecting an action in that state which results in the highest expected value, which is rather an indirect way of behaving optimally in an MDP environment. In contrast, policy-based methods try to learn and optimize a policy function, which is basically a mapping from states to actions. The Policy Gradient (PG) method <strong>[1][2]</strong> is a popular policy-based approach in RL, which **directly **optimizes the policy function by changing its parameters using gradient ascent.</p> <p>PG has some advantages over value-based methods, especially when dealing with environments with continuous action spaces or high stochasticity. PG can also handle non-differentiable policies, making it suitable for complex scenarios. However, PG can suffer from some issues such as high variance in the gradient estimates, which can lead to slower convergence and/or instability.</p> <p>In this post, we will lay out the Policy Gradient method in detail, while examining its strengths and limitations. We will discuss the intuition behind PG, how it works, and also provide a code implementation of the algorithm. I will try to cover various modifications and extensions that have been proposed to improve its performance in further posts.</p> <hr/> <h2 id="policy-gradient-method">Policy Gradient Method</h2> <p>As explained above, Policy Gradient (PG) methods are algorithms that aim to learn the optimal policy function directly in a Markov Decision Processes setting (*S, A, P, R, *γ). In PG, the **policy **π is represented by a parametric function (e.g., a neural network), so we can control its outputs by changing its parameters. The **policy **π maps state to actions (or probability distributions over actions).</p> <p>The goal of PG is to achieve a policy that maximizes the <strong>expected cumulative rewards</strong> over a trajectory of states and actions (A.K.A. the return). Let us go through how it achieves to do so.</p> <h3 id="derivation">Derivation</h3> <p>So, considering we have a <strong>policy function π</strong>, parameterized by a parameter set θ. Given a state as input, The <strong>policy</strong> outputs a probability distribution over actions at each state. Knowing this, first, we need to come up with an <strong>objective/goal</strong> so that we would want to optimize our policy function’s parameters with regard to this goal.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2036/1*sSnx2pL_3_VGpc3ZZXbL2Q.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Note that this policy function could be approximated with any function approximation technique. However, in the case of <strong>Deep RL</strong>, we consider this function to be approximated by a <strong>Neural Network</strong> (with parameter set θ) that takes states (observations) as input and outputs the distribution over actions. It could be a discrete distribution for discrete actions or a continuous distribution for continuous actions.</p> <p>In general, the agent’s goal would be to obtain the maximum cumulative reward over a <strong>trajectory of interactions</strong> (state-action sequences). So, let <strong><em>τ</em></strong> denote such trajectory i.e., a state-action sequence <em>s₀, a₀, s₁, a₁, …, sₕ, aₕ</em>, And <strong>R(<em>τ</em>)</strong> denote the cumulative reward over this trajectory a.k.a <strong>the return</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*0X4fXdoylb1r-IUWIyWpjA.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>It is obvious that this trajectory <strong><em>τ,</em></strong> is a random variable because of its stochasticity. This makes <strong>R(<em>τ</em>)</strong> to be a stochastic function as well. As we can not maximize this stochastic function directly, we would want to maximize its expectation, meaning to maximize it on average case, while taking actions with policy π: <strong>E[ R(<em>τ</em>); π ]</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*pcz-xb2fsky67jbs92UtOw.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>And as a refresher on probabilities, this is how to calculate a discrete expectation of a function of a random variable:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2240/1*AYQ2TmEAwFop75ErmlP4ow.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>So the final objective function that we would want to maximize is rewritten as follows. Also, we will now call this objective function, <strong>the Utility Function (U)</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*x8fp_A1URSh8yP6Qma1fyQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>where <strong>P(<em>τ</em>; θ)</strong>is the probability of trajectory <em>τ</em> taking place over policy parameters θ.</p> <p>Note that this utility function is written as a function of <strong>θ</strong>, a set of policy parameters, because <strong>it is only the policy that controls the path of the trajectories</strong> since the environment dynamic is fixed (and usually unknown), and we would want to optimize this utility by changing and optimizing <strong>θ</strong>. Also, note that the reward series <strong>R</strong> over the trajectory is not dependent on the policy. Hence <strong>it is not dependent on the parameters θ</strong> because it is based on the environment and is only an experienced scalar value.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*NMBuSdL3haA9ZgTiORZeJw.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="optimization">Optimization</h3> <p>Now that we have the Utility function <strong>U(θ)</strong>, we would want to optimize it using a stochastic gradient-based optimization algorithm, namely <strong>Stochastic Gradient Ascent:</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*GnbR_fRQh3ofJZlvVQ_7vg.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>To be able to perform the parameter update, we have to compute the gradient of the Utility over <strong>θ</strong> (<strong>∇U</strong>). By attempting to do so, we get:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*8-oK4kmbJJ0EARps3EZHiw.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Note that the gradient over the summation is equal to the sum of gradients. Now we continue by applying a trick by multiplying this expression by P(<em>τ</em> ; θ)/P(<em>τ</em>; θ), which equals <strong>1</strong>. Remember from calculus that <strong>∇f(x)/f(x) = ∇log(f(x))</strong>, so we get:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*rWKTFBQYFiw8cXqfb6-S7A.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We do this trick to create a general form of expectation <strong>‘ΣP(x)f(x)’</strong> in the gradient’s formula, which we can later replace and estimate the <strong>expectation</strong> with <strong>the average over samples</strong> of real interaction data.</p> <p>As we cannot sum over ALL possible trajectories, in a stochastic setting, we estimate this expression (∇U(θ)), by generating “<strong>m</strong>” number of random trajectories using policy parameters θ and averaging the above expectation over these <strong>m</strong> trajectories to estimate the gradient ∇U(θ):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*pNUZAWxPw9BBN789Y-9fhQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>So now we have a way of estimating the gradient with trajectory samples, but there is an issue here. this gradient causes parameters θ to change in a direction that the probability of trajectories with higher return <em>*R(</em>τ<em>)</em> ***increases the most. However, this has a disadvantage, and that is the direction of change in the trajectory probability is highly dependent on how reward function R is designed. For example, if all transitions result in a positive reward, all trajectory probabilities would be increased and no trajectory would be penalized, and vice versa for cases with negative R.</p> <p>To address this issue, it is proposed to subtract a <strong>baseline value (b)</strong> [1] from the return (i.e., change ‘<strong>R(<em>τ</em>)</strong>’ to ‘<strong>R(<em>τ</em>)-b</strong>’) where this baseline <strong>b</strong> should estimate how the return would be on average. This helps returns to be centralized around zero which makes trajectory probabilities that performed better than average are increased and those that didn’t are decreased. There are multiple proposed ways of calculating baseline b, among which using a neural network is one that we will use later.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*Sq5fzKKYBVfHuf5QmrxYPw.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Further, by decomposing the trajectory probability P(<em>τ</em>) into Temporal timesteps and writing it as the <strong>product of the probabilities of all ‘H’ timesteps</strong>, and also by knowing that the environment dynamics <strong>does NOT depend on the parameters θ</strong> so its gradient over θ would be 1, we can rewrite the Trajectory’s gradient w.r.t. Temporal timesteps and solely based on the policy function <strong>π</strong>:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*QtJ9V0uA-_u5Qw_nilItVw.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Also, we can decompose the return <strong>R(<em>τ</em>)</strong> as a sum of individual timestep rewards Σrₜ. And at each timestep t, we can <strong>disregard</strong> the rewards from <strong>previous timesteps</strong> 0, … , t-1 since they are <strong>not affected by the current action aₜ</strong> (Removing terms that don’t depend on current action can lower variance). Together with incorporating the baseline, we would have the following Temporal format:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2050/1*JQ7oFxDVOJTl09NSD8bcyw.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>So the <strong>gradient estimate ‘g’</strong> will be:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*O85g5ZKpI3vY2QAKFjcWrA.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Finally, as a proper choice for estimation of the baseline b(sₜ), we can use expected return from state sₜ onward, which is also known as the <strong>Value function</strong> of this state <strong>V(sₜ)</strong>. We will use another neural network with parameter set <strong><em>ϕ</em></strong> to approximate this value function with a bellman target using either Monte Carlo or Temporal difference learning from interaction data. the final form would be as follows:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*053-3GF2kvsC6Xkz_SoRMQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Moreover, it is good to know that the difference between <strong>R(sₜ)</strong> and the baseline <strong>b(sₜ)</strong> is called the <strong>Advantage function A(sₜ)</strong>. In some implementations, as the <strong>R(sₜ)</strong> equivalences the state-action value, also known as the <strong>Q function Q(sₜ)</strong>, the advantage is written as <strong>A(sₜ) = Q(sₜ)-V(sₜ)</strong> where both Q and V can be approximated with neural networks, and maybe even with shared weights.</p> <p>It is worth mentioning that when we incorporate value estimation in policy gradient methods, it is also called <strong>the Actor-Critic method</strong>, while the <strong>actor</strong> is the policy network and the <strong>critic</strong> is the value estimator network. These types of methods are highly studied nowadays due to their good performance in various benchmarks and tasks.</p> <h3 id="the-algorithm">The Algorithm</h3> <p>Based on the derived <strong>gradient estimator g</strong>, the <strong>Value function</strong> approximation network, and the <strong>gradient ascent update rule</strong>, the Overall algorithm to train an agent with the Vanilla (simple) Policy Gradient algorithm is as follows:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*bsYAtBUIumT2yoU_bu-2jQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Note that in this algorithm, first, a number of state-action sequences (trajectories) are done then the updates are carried away. The policy network here is updated in an on-policy or online manner, whereas the value functions can be updated off-policy (offline) from batch-sampled data using gradient descent.</p> <hr/> <h2 id="pytorch-implementation">PyTorch Implementation</h2> <p>To implement VPG, we need the following components:</p> <ul> <li><strong>Policy Network</strong> with probabilistic outputs to sample from. (with Softmax output for discrete action space, or parameter estimations such as <strong>μ, σ</strong> output for Gaussian dist. for continuous action spaces)</li> <li><strong>Value Network</strong> for Advantage estimation.</li> <li><strong>Environment</strong> class with <a href="https://gymnasium.farama.org/">gym</a> interface for the agent to interact with.</li> <li><strong>Training loop</strong></li> </ul> <h3 id="networks">Networks</h3> <p>First of all, we define **Policy **and **Value Networks **as PyTorch Module classes. We are using simple Multi-layer perceptron networks for this toy task.</p> <p><strong>Importing Dependencies:</strong></p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># import dependencies
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="n">torch.distributions</span> <span class="kn">import</span> <span class="n">Categorical</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">gym</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">deque</span>
</code></pre></div></div> <p><strong>Policy Net:</strong></p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PolicyNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">PolicyNet</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rewards</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">saved_actions</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">aprob</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Softmax for categorical probabilities
</span>        <span class="k">return</span> <span class="n">aprob</span>
</code></pre></div></div> <p><strong>Value Net:</strong></p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ValueNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ValueNet</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">V</span>
</code></pre></div></div> <h3 id="training-loop-main-algorithm">Training Loop (Main algorithm)</h3> <p>We will be using a simple Cartpole environment from the <a href="https://gymnasium.farama.org/"><strong>gym</strong></a>library. You can read more about this environment and its state and action spaces <strong><a href="https://gymnasium.farama.org/environments/classic_control/cart_pole/">here</a></strong>.</p> <p>The <strong>full algorithm</strong> is as follows:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## Vanilla Policy Gradient
</span>
<span class="c1"># create environment
</span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">"</span><span class="s">CartPole-v1</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># sample toy environment
</span>
<span class="c1"># instantiate the policy and value networks
</span><span class="n">policy</span> <span class="o">=</span> <span class="nc">PolicyNet</span><span class="p">(</span><span class="n">state_dim</span><span class="o">=</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_actions</span><span class="o">=</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span> <span class="n">n_hidden</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="n">value</span> <span class="o">=</span> <span class="nc">ValueNet</span><span class="p">(</span><span class="n">state_dim</span><span class="o">=</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_hidden</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

<span class="c1"># instantiate an optimizer
</span><span class="n">policy_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">policy</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">2e-7</span><span class="p">)</span>
<span class="n">value_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">value</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">)</span>

<span class="c1"># initialize gamma and stats
</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span>
<span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">returns_deq</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">memory_buffer_deq</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">2000</span><span class="p">)</span>

<span class="k">for</span> <span class="n">n_ep</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">actions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">states</span>  <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># reset environment
</span>    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="c1"># recieve action probabilities from policy function
</span>        <span class="n">probs</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">state</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">float</span><span class="p">())</span>

        <span class="c1"># sample an action from the policy distribution
</span>        <span class="n">policy_prob_dist</span> <span class="o">=</span> <span class="nc">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">policy_prob_dist</span><span class="p">.</span><span class="nf">sample</span><span class="p">()</span>

        <span class="c1"># take that action in the environment
</span>        <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>

        <span class="c1"># store state, action and reward
</span>        <span class="n">states</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">actions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">rewards</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>

        <span class="n">memory_buffer_deq</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">new_state</span><span class="p">))</span>

        <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>

    <span class="c1">### UPDATE POLICY NET ###
</span>    <span class="n">rewards</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
    <span class="c1"># calculate rewards-to-go
</span>    <span class="n">R</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span><span class="o">*</span><span class="p">(</span><span class="n">gamma</span><span class="o">**</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">)))))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">))])</span>

    <span class="c1"># cast states and actions to tensors
</span>    <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">states</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
    <span class="n">actions</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>

    <span class="c1"># calculate baseline V(s)
</span>    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">baseline</span> <span class="o">=</span> <span class="nf">value</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>

    <span class="c1"># calculate utility func
</span>    <span class="n">probs</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
    <span class="n">sampler</span> <span class="o">=</span> <span class="nc">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="o">-</span> <span class="n">sampler</span><span class="p">.</span><span class="nf">log_prob</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>   <span class="c1"># "-" is because we are doing gradient ascent
</span>    <span class="n">utility</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">log_probs</span> <span class="o">*</span> <span class="p">(</span><span class="n">R</span><span class="o">-</span><span class="n">baseline</span><span class="p">))</span> <span class="c1"># loss that when differentiated with autograd gives the gradient of J(θ)
</span>    
    <span class="c1"># update policy weights
</span>    <span class="n">policy_optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">utility</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">policy_optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
    
    <span class="c1">########################
</span>    <span class="c1">### UPDATE VALUE NET ###
</span>
    <span class="c1"># getting batch experience data 
</span>    <span class="n">batch_experience</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">memory_buffer_deq</span><span class="p">),</span> <span class="nf">min</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">memory_buffer_deq</span><span class="p">)))</span>
    <span class="n">state_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">exp</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">exp</span> <span class="ow">in</span> <span class="n">batch_experience</span><span class="p">])</span>
    <span class="n">reward_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">exp</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">exp</span> <span class="ow">in</span> <span class="n">batch_experience</span><span class="p">]).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">new_state_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">exp</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">exp</span> <span class="ow">in</span> <span class="n">batch_experience</span><span class="p">])</span>


    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">reward_batch</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="nf">value</span><span class="p">(</span><span class="n">new_state_batch</span><span class="p">)</span>
    <span class="n">current_state_value</span> <span class="o">=</span> <span class="nf">value</span><span class="p">(</span><span class="n">new_state_batch</span><span class="p">)</span>

    <span class="n">value_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">current_state_value</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="c1"># update value weights
</span>    <span class="n">value_optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">value_loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">value_optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

    <span class="c1">########################
</span>
    <span class="c1"># calculate average return and print it out
</span>    <span class="n">returns_deq</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Episode: {:6d}</span><span class="se">\t</span><span class="s">Avg. Return: {:6.2f}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">n_ep</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">returns_deq</span><span class="p">)))</span>

<span class="c1"># close environment
</span><span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</code></pre></div></div> <h3 id="training-results">Training Results</h3> <p>After training the agent with the VPG for <strong>4000 episodes</strong>, we get the following results:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*rQwO-I_CqpDB9_A43qUJ3A.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>It is worth mentioning that vanilla PG suffers from two major limitations:</p> <ol> <li>It is highly sensitive to hyperparameters configuration such as gamma, learning rate, memory size, etc.</li> <li>It is highly prone to overshooting in the policy parameter space, which makes the learning so noisy and fragile. Sometimes the agent might take a step in the parameter space into a very suboptimal area where it would not be able to recover again.</li> </ol> <p>The second issue is addressed in some further variants of the PG algorithm, which I will try to cover in future posts.</p> <p>An illustration of a fully trained agent controlling the Cartpole environment is shown here:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/0*E2-PXki32kc2N3rh.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The full code of this implementation is available at <strong><a href="https://github.com/ebrahimpichka/vanilla-pg">this GitHub repository</a>.</strong></p> <hr/> <h2 id="conclusion"><strong>Conclusion</strong></h2> <p>In conclusion, we have explored the Policy Gradient (PG) algorithm, a powerful approach in Reinforcement Learning (RL) that directly learns the optimal policy. Throughout this blog post, we have provided a step-by-step explanation of the PG algorithm and its implementation. We started by understanding the fundamental concept of RL and the difference between value-based and policy-based methods. We then delved into the details of PG, highlighting its objective of maximizing the expected cumulative reward by updating the policy parameters using gradient ascent. We discussed the Vanilla PG algorithm as a common implementation of PG, where we compute the gradient of the log-probability of actions and update the policy using policy gradients.</p> <p>Additionally, we explored the Actor-Critic method, which combines a policy network and a value function to improve convergence. While PG offers advantages in handling continuous action spaces and non-differentiable policies, it may suffer from high variance. Nevertheless, techniques like baselines and variance reduction methods can be employed to address these challenges. By grasping the intricacies of PG, you are now equipped with a valuable tool to tackle RL problems and design intelligent systems that learn and adapt through interactions with their environments.</p> <hr/> <h2 id="references">References</h2> <p>[1] — Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning 8:229–256.</p> <p>[2] — Sutton, R. S., McAllester, D. A., Singh, S. P., Mansour, Y., et al. Policy gradient methods for reinforcement learning with function approximation. In Proc. Advances in Neural Information Processing Systems (NIPS), volume 99, pp. 1057–1063. Citeseer, 1999.</p> <p>[3] — course lectures from UC Berkeley: Deep Reinforcement Learning Bootcamp</p> <p>[4] — <a href="http://spinningup.openai.com/en/latest/algorithms/vpg.html">spinningup.openai.com/en/latest/algorithms/vpg.html</a></p> <p>[5] — Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction (second edition), The MIT Press <strong><a href="http://incompleteideas.net/book/RLbook2020.pdf">[PDF]</a></strong></p> <p><br/></p> <font color="gray" face="Times New Roman" size="1"> Author: <strong>Ebrahim Pichka</strong> </font>]]></content><author><name>Ebrahim Pichka</name></author><category term="reinforcement-learning"/><category term="RL"/><category term="policy-gradient"/><category term="REINFORCE"/><summary type="html"><![CDATA[A step-by-step explanation of the vanilla policy gradient algorithm and its implementation.]]></summary></entry><entry><title type="html">Resources to Learn Reinforcement Learning</title><link href="https://epichka.com/blog/2022/rl-resources/" rel="alternate" type="text/html" title="Resources to Learn Reinforcement Learning"/><published>2022-01-12T12:00:00+00:00</published><updated>2022-01-12T12:00:00+00:00</updated><id>https://epichka.com/blog/2022/rl-resources</id><content type="html" xml:base="https://epichka.com/blog/2022/rl-resources/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2048/1*fC-GHp0h_8OSwYUIHvvhxw.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="introduction">Introduction</h2> <p>Reinforcement learning (RL) is a paradigm of AI methodologies in which an agent learns to interact with its environment in order to maximize the expectation of reward signals received from its environment. Unlike supervised learning, in which the agent is given labeled examples and learns to predict an output based on input, RL involves the agent actively taking actions in its environment and receiving feedback in the form of rewards or punishments. This feedback is used to adjust the agent’s behavior and improve its performance over time.</p> <p>RL has been applied to a wide range of domains, including robotics, natural language processing, and finance. In the gaming industry, RL has been used to develop advanced game-playing agents, such as the <strong>AlphaGo [1]</strong> algorithm that defeated a human champion in the board game Go. In the healthcare industry, RL has been used to optimize treatment plans for patients with chronic diseases, such as diabetes. RL has also been used in the field of robotics, allowing robots to learn and adapt to new environments and tasks.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2038/1*Y6mxK5VOgXkOlwlqv3TJNQ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>One of the most iconic recent breakthroughs in RL is the development of <a href="https://openai.com/blog/chatgpt/">chatGPT</a> [2] by OpenAI, a natural language processing system that can hold intelligent conversations with humans. chatGPT was trained on a large dataset of human conversations and can generate coherent and contextually appropriate responses to user inputs. This system demonstrates the potential for RL to be used to improve natural language processing systems and create more human-like AI assistants.</p> <p>As RL continues to advance and make an impact in various fields, it has become increasingly important for professionals and researchers to have a strong understanding of this technique. If you’re interested in learning about RL, you’re in luck! There are a variety of resources available online that can help you get started and become proficient in this exciting field. In this blog post, we’ll highlight some of the best, mostly free, resources for learning about RL, including tutorials, courses, books, and more. Whether you’re a beginner looking to get your feet wet or an experienced practitioner looking to deepen your understanding, these resources will have something for you.</p> <p>In this post, we are going to first start by introducing the best **online courses, lectures, and tutorials **available for RL on the internet. Then we will introduce the best and most popular **books **and **textbooks **in the field. And at last, we will also include some useful extra resources and GitHub repositories on the topic.</p> <h2 id="online-courses">Online Courses</h2> <p>While there are numerous courses available on the subject, we’ve carefully selected a list of the most comprehensive and high-quality options that are mostly free. These courses cover a wide range of topics in RL, from the basics to advanced concepts, and are taught by experts in the field. Whether you’re a beginner looking to get your feet wet or an experienced practitioner looking to deepen your understanding, these courses will have something for you. Keep reading to discover some of the top online courses for learning about RL! Please note that this is not an exhaustive list, but rather a curated selection of the most highly recommended courses available.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/3794/1*-is1XgAQyi9d58SA0yiH_w.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The Reinforcement Learning Specialization on Coursera, offered by the University of Alberta and the Alberta Machine Intelligence Institute, is a comprehensive program designed to teach you the foundations of reinforcement learning. This specialization consists of three courses and one capstone project that cover a wide range of topics in RL, including RL fundamentals, value-based methods, policy gradient methods, model-based RL, deep RL, etc. Throughout the course, you’ll have the opportunity to apply what you’ve learned through hands-on programming assignments and a final project. The course is taught by experienced instructors and academics who are experts in the field of RL and includes a mix of lectures, readings, and interactive exercises. This specialization is suitable for students with a background in machine learning or a related field and is a great resource for anyone looking to gain a solid understanding of RL.</p> <p>Although it is not technically free, you could always apply for Coursera’s financial aid to waive the course fee if you were not to afford it. However, considering the content quality and material, it would be totally worthwhile.</p> <p>Link to the course:www.coursera.org/specializations/reinforcement-learning</p> <h3 id="2---reinforcement-learning-lecture-series-2021--by-deepmind-x-ucl"><a href="https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021">2 - Reinforcement Learning Lecture Series 2021 — by DeepMind x UCL</a></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2400/0*ywdNu229dv3PQnK8.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The “Reinforcement Learning Lecture Series” is a series of lectures on the topic of reinforcement learning, presented by DeepMind and UCL. This course covers a wide range of topics within the field of reinforcement learning, including foundational concepts such as Markov decision processes and dynamic programming, as well as more advanced techniques such as model-based and model-free learning and off-policy, value-/policy-based algorithms, function approximation, and deep RL. The lectures are offered by renowned academics and researchers from Deepmind and UCL. The lectures are aimed at researchers and practitioners interested in learning about the latest developments and applications in reinforcement learning. The course is offered online and is open to anyone who is interested in learning about this exciting and rapidly-evolving field.</p> <p>Link to the course: <a href="https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021"><strong>Reinforcement Learning Lecture Series 2021</strong></a></p> <p>There is also an older version of this series from 2018 which could be <a href="https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2018">found here</a>.</p> <h3 id="3---stanford-cs234-reinforcement-learning--winter-2019"><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u">3 - Stanford CS234: Reinforcement Learning — Winter 2019</a></h3> <p>The CS234 Reinforcement Learning course from Stanford is a comprehensive study of reinforcement learning, taught by Prof. Emma Brunskill. This course covers a wide range of topics in RL, including foundational concepts such as MDPs and Monte Carlo methods, as well as more advanced techniques like temporal difference learning and deep reinforcement learning. The course is designed for students who have a background in machine learning and are interested in learning about the latest techniques and applications in reinforcement learning. The course is offered through a series of video lectures, which are available on YouTube through the provided link.</p> <p>Link to the course: <a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u">Here</a></p> <h3 id="4---introduction-to-reinforcement-learning-with-david-silver"><a href="https://www.deepmind.com/learning-resources/introduction-to-reinforcement-learning-with-david-silver">4 - Introduction to Reinforcement Learning with David Silver</a></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2560/0*fiC_p1Qa5Q6iF6ms.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The Introduction to Reinforcement Learning with David Silver course is a comprehensive introduction to the field of reinforcement learning, taught by Professor David Silver. Silver is a leading researcher in the field of reinforcement learning and artificial intelligence, and has been a key contributor to the development of AlphaGo, the first computer program to defeat a professional human player in the game of Go. He is also among the authors of some of the key research papers in RL such as Deep Q-Learning and DDPG algorithm. The course covers the fundamental concepts and techniques of reinforcement learning, including dynamic programming, Monte Carlo methods, and temporal difference learning. It also covers more advanced topics such as exploration-exploitation trade-offs, function approximation, and deep reinforcement learning. Overall, the course provides a solid foundation in reinforcement learning and is suitable for anyone interested in learning more about this exciting and rapidly-evolving field of artificial intelligence.</p> <p>Link to the course: <a href="https://www.deepmind.com/learning-resources/introduction-to-reinforcement-learning-with-david-silver"><strong>Introduction to Reinforcement Learning with David Silver</strong></a></p> <h3 id="5---uc-berkeley-cs-285-deep-reinforcement-learning--fall-2021"><a href="https://www.youtube.com/playlist?list=PL_iWQOsE6TfXxKgI1GgyV1B_Xa0DxE5eH">5 - UC Berkeley CS 285: Deep Reinforcement Learning — Fall 2021</a></h3> <p>The UC Berkeley CS 285 Deep Reinforcement Learning course is a graduate-level course that covers the field of reinforcement learning, with a focus on deep learning techniques. The course is taught by Prof. Sergey Levine and is designed for students who have a strong background in machine learning and are interested in learning about the latest techniques and applications in reinforcement learning. The course covers a wide range of topics, including foundational concepts such as Markov decision processes and temporal difference learning, as well as advanced techniques like deep Q-learning and policy gradient methods. The course is offered through a series of video lectures, which are available on YouTube through the provided link.</p> <p>Link to the course: <a href="https://www.youtube.com/playlist?list=PL_iWQOsE6TfXxKgI1GgyV1B_Xa0DxE5eH">Here</a></p> <p>There is also an older series of the course from Fall 2020 <a href="https://www.youtube.com/playlist?list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc">here</a>.</p> <h3 id="6---deep-rl-bootcamp--uc-berkeley"><a href="https://sites.google.com/view/deep-rl-bootcamp/lectures">6 - Deep RL BootCamp — UC Berkeley</a></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/1*_K-ar5pvp6_MpigC_9SnnA.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The Deep RL Bootcamp is an intensive two-day course on deep reinforcement learning, taught by leading researchers in the field. The course covers a wide range of topics, including value-based methods, policy gradient algorithms, model-based reinforcement learning, exploration and uncertainty, and deep reinforcement learning in the real world. It features a mix of lectures and hands-on exercises, giving attendees the opportunity to learn about the latest techniques and apply them to real-world problems. The course is designed for researchers and practitioners with a background in machine learning and/or reinforcement learning and is suitable for those looking to gain a deeper understanding of the field and advance their research or career in this exciting area of artificial intelligence.</p> <p>Link to the course: <a href="https://sites.google.com/view/deep-rl-bootcamp/lectures"><strong>Deep RL Bootcamp</strong></a></p> <h3 id="7---deep-reinforcement-learning-course-by-huggingface"><a href="https://simoninithomas.github.io/deep-rl-course/">7 - Deep Reinforcement Learning Course by HuggingFace</a></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/3840/0*MBs-nwimt6nrzs2Z.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The Deep RL course by Hugging Face is an in-depth and interactive learning experience that covers the most important topics in deep reinforcement learning. The course is divided into units that cover various aspects of the field such as the Q-learning algorithm, policy gradients, and advanced topics like exploration, multi-agent RL, and meta-learning. Each unit includes a combination of video lectures, interactive coding tutorials, and quizzes to help learners understand and apply the concepts.</p> <p>The course also includes hands-on projects that allow learners to apply their knowledge to real-world problems. These projects include creating an RL agent to play a game, training an RL agent to navigate a virtual environment, and building an RL agent to play a game of chess. These projects provide an opportunity for learners to get hands-on experience working with RL models, and gain an understanding of the challenges and complexities of working with these models.</p> <p>The course also includes explanations of the theoretical foundations of RL, providing an understanding of the mathematical concepts and algorithms used in the field. The course is designed to be accessible to people with different backgrounds and levels of experience, from those new to the field to experienced practitioners. The course is taught by Simon Thomas, who is a researcher and expert in the field of deep reinforcement learning, and the course content is regularly updated to keep up with the latest advancements in the field.</p> <p>Links to the course: <a href="https://simoninithomas.github.io/deep-rl-course/"><strong>Deep Reinforcement Learning Course</strong> <em>Deep Reinforcement Learning Course is a free course about Deep Reinforcement Learning from beginner to expert.</em>simoninithomas.github.io</a> <a href="https://huggingface.co/deep-rl-course/unit0/introduction"><strong>Welcome to the 🤗 Deep Reinforcement Learning Course - Hugging Face Course</strong> <em>Welcome to the most fascinating topic in Artificial Intelligence: Deep Reinforcement Learning. This course will teach…</em>huggingface.co</a> <a href="https://github.com/huggingface/deep-rl-class"><strong>GitHub - huggingface/deep-rl-class: This repo contains the syllabus of the Hugging Face Deep…</strong> <em>This repository contains the Deep Reinforcement Learning Course mdx files and notebooks. The website is here…</em>github.com</a></p> <h3 id="8---lectures-by-pieter-abbeel">8 - Lectures by Pieter Abbeel</h3> <p><a href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a> is a renowned computer scientist and roboticist who is currently a professor at the University of California, Berkeley. He is known for his research in the field of robotics, particularly in the areas of reinforcement learning, learning from demonstration, and robot manipulation. He has made notable contributions to the field of robotic grasping and manipulation, developing algorithms for robots to learn to grasp and manipulate objects using trial-and-error.</p> <p>He also has been a pioneer in the field of apprenticeship learning, which allows robots to learn from human demonstrations. He has published over 150 papers, many of which can be accessed on his personal website and also has a set of video lectures available on youtube. He has also been involved in the development of open-source software for robotics and machine learning and is the co-author of the popular open-source software library <a href="https://www.gymlibrary.dev/">OpenAI Gym</a>, which is widely used in the field of reinforcement learning.</p> <p>His online lectures, which are available on YouTube are one of the high quality material available in reinforcement learning.</p> <p><strong>His “Foundations of Deep RL — lecture series” on his own YouTube channel:</strong></p> <p><strong>His Lectures from CS188 Artificial Intelligence UC Berkeley, Spring 2013:</strong></p> <h3 id="9---spinning-up-in-deep-rl-by-openai"><a href="https://spinningup.openai.com/en/latest/user/introduction.html">9 - Spinning Up in Deep RL by OpenAI</a></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="https://cdn-images-1.medium.com/max/2000/0*O9DBfKY60Af4MLau.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://spinningup.openai.com/en/latest/user/introduction.html">Spinning Up in Deep RL</a> is developed and maintained by OpenAI. It is a resource for people who want to learn about deep reinforcement learning (RL) and how to apply it. The website provides a comprehensive introduction to RL and its algorithms and includes tutorials and guides on how to implement and run RL experiments. The website also includes a set of resources such as papers, videos, and code examples to help users learn about RL.</p> <p>The website is based on the software library OpenAI Baselines, which is an implementation of RL algorithms in Python with PyTorch and TensorFlow. The library includes implementations of popular RL algorithms such as DQN, PPO, A2C, and TRPO. The website provides detailed instructions and code examples on how to use the library to train RL agents and run experiments.</p> <p>The website is designed to be accessible to people with different levels of experience and provides a step-by-step guide to getting started with RL. The website is divided into sections, including an introduction to RL, tutorials on how to use the library, and a section on advanced topics such as multi-agent RL, exploration, and meta-learning. The website also provides a set of Jupyter notebooks that users can run and modify, allowing them to experiment with different RL algorithms and environments.</p> <p>The link to the website: <a href="https://spinningup.openai.com/en/latest/index.html"><strong>Welcome to Spinning Up in Deep RL! - Spinning Up documentation</strong></a></p> <h3 id="10---phil-tabors-rl-courses">10 - Phil Tabor’s RL Courses</h3> <p>Phil Tabor is a machine learning engineer and educator who specializes in the field of reinforcement learning. He is known for his practical approach to teaching and has a special focus on the hands-on aspect of the field. He has created several courses on machine learning and artificial intelligence on Udemy, with a focus on reinforcement learning. He also has a YouTube channel <a href="https://www.youtube.com/@MachineLearningwithPhil">“Machine Learning with Phil”</a> where he uploads videos on various reinforcement learning topics such as Q-learning, policy gradients, and more advanced topics. He also uploads code-along videos to help learners understand the concept and apply them.</p> <p>His more practical approach to the field makes it rather much different than other available content. Aside from his paid courses on Udemy which are very comprehensive and well-framed, he has tons of free content on his <a href="https://www.youtube.com/@MachineLearningwithPhil">YouTube channel</a> which are not much less than his paid ones.</p> <p>**Youtube channel: <a href="https://www.youtube.com/@MachineLearningwithPhil">**https://www.youtube.com/@MachineLearningwithPhil</a></p> <p>**Udemy: <a href="https://www.udemy.com/user/phil-tabor/">**https://www.udemy.com/user/phil-tabor/</a></p> <h2 id="books">Books</h2> <p>There are tons of great books published about reinforcement learning however 5 of the most popular and comprehensive ones are listed below:</p> <h3 id="1-richard-sutton-and-andrew-barto-reinforcement-learning-an-introduction-2nd-edition--most-recommended"><a href="http://incompleteideas.net/book/RLbook2020.pdf">1. Richard Sutton and Andrew Barto, “Reinforcement Learning: An Introduction” (2nd Edition)</a> — Most Recommended*</h3> <p><a href="http://incompleteideas.net/book/RLbook2020.pdf">Reinforcement Learning: An Introduction</a> (2nd Edition) by Richard Sutton and Andrew Barto is a must-have resource for anyone interested in the field of reinforcement learning. This book provides a comprehensive introduction to the fundamental concepts and algorithms of reinforcement learning, making it an essential resource for students, researchers, and practitioners. The second edition includes new chapters on recent developments in the field and updates to existing material, making it even more current and relevant.</p> <p>The book starts with an introduction to the basic concepts of RL and lays out the RL problem along with a history of the field and its relationship to other fields such as psychology, neuroscience, and control theory. It then delves into the foundational algorithms and concepts of the field, including Multiarm bandits, Markov decision processes, dynamic programming, and Monte Carlo methods.</p> <p>The book also covers advanced topics such as temporal-difference learning, planning and learning with function approximators, and exploration and exploitation in reinforcement learning. Additional chapters discuss the application of reinforcement learning in various domains, including robotics, game playing, and healthcare.</p> <p>The book also includes chapters on recent developments in the field such as deep reinforcement learning, policy gradient methods, and inverse reinforcement learning. The final chapters cover the challenges and future of the field, including safety and reliability, multi-agent reinforcement learning, and the role of reinforcement learning in artificial general intelligence.</p> <p><strong>Book Chapters:</strong></p> <ol> <li> The Reinforcement Learning Problem</li> <li> Multi-arm Bandits</li> <li> Finite Markov Decision Processes</li> <li> Dynamic Programming</li> <li> Monte Carlo Methods</li> <li> Temporal-Difference Learning</li> <li> Eligibility Traces</li> <li> Planning and Learning with Tabular Methods</li> <li> On-policy Approximation of Action Values</li> <li> Off-policy Approximation of Action Values</li> <li> Policy Approximation</li> <li> Psychology</li> <li> Neuroscience</li> <li> Applications and Case Studies</li> <li> Prospects</li> </ol> <h3 id="2-mykel-j-kochenderfer-decision-making-under-uncertainty-theory-and-application"><a href="https://mitpress.mit.edu/9780262029254/decision-making-under-uncertainty/">2. Mykel J. Kochenderfer, “Decision Making Under Uncertainty: Theory and Application”</a></h3> <p><a href="https://mitpress.mit.edu/9780262029254/decision-making-under-uncertainty/">Decision Making Under Uncertainty: Theory and Application</a>, by Mykel J. Kochenderfer, is a comprehensive guide to decision-making under uncertainty, with a focus on reinforcement learning. The book covers the fundamental concepts of decision theory, Markov decision processes, and reinforcement learning algorithms, providing the reader with a solid foundation in these areas.</p> <p>The book also delves into advanced topics such as planning under uncertainty, safe reinforcement learning, and the use of decision-making methods in real-world applications. The author explains the concepts in a clear and concise manner, with the help of examples and exercises to help the reader understand and apply the material.</p> <p>The book is intended for a broad audience, including researchers and practitioners in the fields of artificial intelligence, operations research, and control systems. It’s also suitable for advanced undergraduate and graduate students in these areas. The book provides a thorough introduction to the theory and application of decision-making under uncertainty, with a focus on reinforcement learning, making it an essential resource for anyone interested in this field.</p> <p><strong>Book Chapters:</strong></p> <ol> <li> Introduction</li> <li> Probabilistic Models</li> <li> Decision Problems</li> <li> Sequential Problems</li> <li> Model Uncertainty</li> <li> State Uncertainty</li> <li> Cooperative Decision Making</li> <li> Probabilistic Surveillance Video Search</li> <li> Dynamic Models for Speech Applications</li> <li> Optimized Airborne Collision Avoidance</li> <li> Multi-agent Planning for Persistent Surveillance</li> <li> Integrating Automation with Humans</li> </ol> <h3 id="3-phil-winder-reinforcement-learning"><a href="https://www.oreilly.com/library/view/reinforcement-learning/9781492072386/">3. Phil Winder, “Reinforcement Learning”</a></h3> <p>“Reinforcement Learning” by Phil Winder is an in-depth examination of one of the most exciting and rapidly growing areas of machine learning. The book provides a comprehensive introduction to the theory and practice of reinforcement learning, covering a wide range of topics that are essential for understanding and working with this powerful technique.</p> <p>The book starts with the fundamentals of Markov decision processes, which form the mathematical foundation of reinforcement learning. It then delves into Q-learning, a popular algorithm for finding the optimal action-value function in a given environment. The book also covers policy gradients, a class of algorithms that allow for the optimization of policies directly, rather than value functions. Additionally, it covers the recent advancements in deep reinforcement learning and how it can be applied to solve complex problems.</p> <p>The book also includes numerous practical examples and exercises that help readers apply the concepts to real-world problems. This book is ideal for machine learning practitioners, researchers, and students who are interested in understanding and working with reinforcement learning. It provides a clear and accessible introduction to the field, making it an essential resource for anyone looking to get started with reinforcement learning or deepen their understanding of this powerful technique.</p> <p><strong>Book Chapters:</strong></p> <ol> <li>Why Reinforcement Learning?</li> <li>Markov Decision Processes, Dynamic Programming, and Monte Carlo Methods</li> <li>Temporal-Difference Learning, Q-Learning, and n-Step Algorithms</li> <li>Deep Q-Networks</li> <li>Policy Gradient Methods</li> <li>Beyond Policy Gradients</li> <li>Learning All Possible Policies with Entropy Methods</li> <li>Improving How an Agent Learns</li> <li>Practical Reinforcement Learning</li> <li>Operational Reinforcement Learning</li> <li>Conclusions and the Future</li> </ol> <h3 id="4-alexander-zai-and-brandon-brown-deep-reinforcement-learning-in-action"><a href="https://www.manning.com/books/deep-reinforcement-learning-in-action">4. Alexander Zai and Brandon Brown, “Deep Reinforcement Learning in Action”</a></h3> <p>“Deep Reinforcement Learning in Action” by Alexander Zai and Brandon Brown is an in-depth guide that takes the reader through the process of building intelligent systems using deep reinforcement learning. The book starts by introducing the basic concepts and algorithms of reinforcement learning, including Q-learning and policy gradients. It then goes on to cover more advanced topics such as actor-critic methods and deep Q-networks (DQN), which are used to improve the performance of reinforcement learning algorithms.</p> <p>One of the key features of the book is its emphasis on hands-on examples and exercises. Throughout the book, the authors provide code snippets and sample projects that illustrate how to implement reinforcement learning algorithms in practice. These examples and exercises are designed to help readers understand the material and apply it to their own projects.</p> <p>In addition to covering the fundamentals of reinforcement learning, the book also covers recent advances in the field such as double DQN, prioritized replay, and A3C. These techniques are used to improve the performance of reinforcement learning algorithms and make them more efficient. The book is intended for readers with some experience in machine learning and deep learning, but no prior experience with reinforcement learning is required. The authors provide a comprehensive and accessible introduction to the field, making it an ideal choice for both beginners and experienced practitioners.</p> <p><strong>Book Chapters:</strong></p> <ol> <li>What is reinforcement learning </li> <li>Modeling reinforcement learning problems: Markov decision processes </li> <li>Predicting the best states and actions: Deep Q-networks </li> <li>Learning to pick the best policy: Policy gradient methods </li> <li>Tackling more complex problems with actor-critic methods </li> <li>Alternative optimization methods: Evolutionary algorithms </li> <li>Distributional DQN: Getting the full story </li> <li>Curiosity-driven exploration </li> <li>Multi-agent reinforcement learning </li> <li>Interpretable reinforcement learning: Attention and relational model </li> <li>conclusion: A review and roadmap </li> </ol> <h3 id="5-maxim-lapan-deep-reinforcement-learning-hands-on"><a href="https://www.packtpub.com/product/deep-reinforcement-learning-hands-on-second-edition/9781838826994">5. Maxim Lapan, “Deep Reinforcement Learning Hands-On”</a></h3> <p>Deep Reinforcement Learning Hands-On” by Maxim Lapan is an updated edition of the popular guide to understanding and implementing deep reinforcement learning (DRL) techniques. This book is designed to provide readers with a solid understanding of the key concepts and techniques behind DRL and to equip them with the practical skills needed to build and train their own DRL models.</p> <p>The book covers a wide range of topics, including the basics of reinforcement learning and its connection to neural networks, advanced DRL algorithms such as Q-Learning, SARSA, and DDPG, and the use of DRL in real-world applications such as robotics, gaming, and autonomous vehicles. Additionally, the book includes practical examples and hands-on exercises, allowing readers to apply the concepts and techniques covered in the book to real-world problems.</p> <p>With its focus on both theory and practice, “Deep Reinforcement Learning Hands-On” is the perfect guide for anyone looking to gain a deep understanding of DRL and start building their own DRL models.</p> <p><strong>Book Chapters:</strong></p> <ol> <li> What Is Reinforcement Learning?</li> <li> OpenAI Gym</li> <li> Deep Learning with PyTorch</li> <li> The Cross-Entropy Method</li> <li> Tabular Learning and the Bellman Equation</li> <li> Deep Q-Networks</li> <li> Higher-Level RL Libraries</li> <li> DQN Extensions</li> <li> Ways to Speed up RL</li> <li> Stocks Trading Using RL</li> <li> Policy Gradients — an Alternative</li> <li> The Actor-Critic Method</li> <li> Asynchronous Advantage Actor-Critic</li> <li> Training Chatbots with RL</li> <li> The TextWorld Environment</li> <li> Web Navigation</li> <li> Continuous Action Space</li> <li> RL in Robotics</li> <li> Trust Regions — PPO, TRPO, ACKTR, and SAC</li> <li> Black-Box Optimization in RL</li> <li> Advanced Exploration</li> <li> Beyond Model-Free — Imagination</li> <li> AlphaGo Zero</li> <li> RL in Discrete Optimization</li> <li> Multi-agent RL</li> </ol> <h2 id="bonus-other-useful-resources">Bonus: Other Useful Resources</h2> <h3 id="the-best-tools-for-reinforcement-learning-in-python"><a href="https://neptune.ai/blog/the-best-tools-for-reinforcement-learning-in-python"><strong>The Best Tools for Reinforcement Learning in Python</strong></a></h3> <p>This post by neptune.ai provides an overview of the popular tools and libraries used in RL with Python to help readers decide which tools are best suited for their specific use case. it covers a variety of popular RL libraries such as TensorFlow, PyTorch, and OpenAI Baselines, as well as other tools such as OpenAI Gym, and RL Toolbox. The post also covers other topics such as visualization tools, model management tools and experiment tracking tools which are useful for RL. The blog post is well-organized and easy to follow. It includes code examples and links to the relevant documentation for each tool, making it a useful resource for anyone interested in getting started with RL in Python. <a href="https://neptune.ai/blog/the-best-tools-for-reinforcement-learning-in-python"><strong>The Best Tools for Reinforcement Learning in Python You Actually Want to Try - neptune.ai</strong> <em>Nowadays, Deep Reinforcement Learning (RL) is one of the hottest topics in the Data Science community. The fast…</em>neptune.ai</a></p> <h3 id="awesome-deep-rl"><a href="https://github.com/kengz/awesome-deep-rl#tutorials"><strong>awesome-deep-rl</strong></a></h3> <p>This GitHub repository is a curated list of resources for deep reinforcement learning (RL) and contains a comprehensive list of <strong>papers</strong>, <strong>tutorials</strong>, <strong>videos</strong>, and other resources on various topics related to deep RL, such as Q-learning, policy gradients, exploration, meta-learning, and more. It also includes links to popular RL libraries and frameworks, such as TensorFlow, PyTorch, and OpenAI Baselines, as well as other tools and resources that are useful for RL. The repository is well-organized and easy to navigate, making it a useful resource for anyone interested in learning about deep RL. <a href="https://github.com/kengz/awesome-deep-rl#tutorials"><strong>GitHub - kengz/awesome-deep-rl: A curated list of awesome Deep Reinforcement Learning resources.</strong> <em>A curated list of awesome Deep Reinforcement Learning resources. - GitHub - kengz/awesome-deep-rl: A curated list of…</em>github.com</a></p> <h3 id="awesome-deep-reinforcement-learning-in-finance"><a href="https://wire.insiderfinance.io/awesome-deep-reinforcement-learning-in-finance-f319f4302897"><strong>Awesome Deep Reinforcement Learning in Finance</strong></a></h3> <p>this article provides an overview of the use of deep reinforcement learning (RL) in the field of finance. The article includes a curated list of resources for learning more about RL in finance, including papers, videos, and tutorials. The article discusses the potential applications of RL in finance such as portfolio management, algorithmic trading, and risk management. It also highlights some of the challenges and limitations of using RL in finance, such as the lack of data and the difficulty of evaluating the performance of RL models. <a href="https://wire.insiderfinance.io/awesome-deep-reinforcement-learning-in-finance-f319f4302897"><strong>Awesome Deep Reinforcement Learning in Finance</strong></a></p> <h2 id="refernces">Refernces</h2> <p>[1] — Silver, D., Huang, A., Maddison, C. <em>et al.</em> Mastering the game of Go with deep neural networks and tree search. <em>Nature</em> <strong>529</strong>, 484–489 (2016). <a href="https://doi.org/10.1038/nature16961">https://doi.org/10.1038/nature16961</a></p> <p>[2] — <a href="https://openai.com/blog/chatgpt/">https://openai.com/blog/chatgpt/</a></p> <p><br/></p> <font color="gray" face="Times New Roman" size="1"> Author: <strong>Ebrahim Pichka</strong> </font>]]></content><author><name></name></author><category term="reinforcement-learning"/><category term="RL"/><category term="resources"/><summary type="html"><![CDATA[Some of the best (mostly free) tutorials, courses, books, and more on this ever-evolving field]]></summary></entry></feed>